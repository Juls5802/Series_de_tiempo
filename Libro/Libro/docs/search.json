[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de series de tiempo",
    "section": "",
    "text": "Introducción"
  },
  {
    "objectID": "index.html#esquina",
    "href": "index.html#esquina",
    "title": "Análisis de series de tiempo",
    "section": "Esquina",
    "text": "Esquina\nEste trabajo realiza un análisis de dos bases de datos: Recaudo de impuestos internos por la DIAN para los años 2000 a 2023 y consumo de energía por horas de la organización regional de transmisión PJM Interconnection para los años 2004 a 2018. El análisis se desarrolla de forma estadística, realizando la parte descriptiva con su correspondiente interpretación."
  },
  {
    "objectID": "index.html#análisis-del-recaudo-de-impuestos-internos-por-la-dian",
    "href": "index.html#análisis-del-recaudo-de-impuestos-internos-por-la-dian",
    "title": "Análisis de series de tiempo",
    "section": "Análisis del recaudo de impuestos internos por la DIAN",
    "text": "Análisis del recaudo de impuestos internos por la DIAN\nLa DIAN es la entidad encargada de administrar y recaudar los impuestos internos y aduaneros en el país. El recaudo de impuestos internos que realiza la DIAN cada mes se refiere a la suma total de los impuestos nacionales recaudados dentro del territorio colombiano durante ese período mensual. Los impuestos internos son aquellos que se aplican a las actividades económicas y transacciones que ocurren dentro del país, los cuales pueden incluir: IVA, impuesto de renta y complementarios, impuesto de timbre, impuesto de consumo, impuesto a la riqueza, impuesto predial, ICA, entre otros.\nCon el proyecto se busca estudiar esta serie de tiempo para ver como es el comportamiento de los impuestos internos de Colombia a lo largo de los años, por ejemplo, encontrar patrones y observar qué tanto han aumentado dichos impuestos.\nA continuación se presenta la manera en que se realiza la carga de los datos y un vistazo preliminar de la serie de tiempo en la Figura 1 .\n\n# Carga de la base de datos\ndian&lt;-read_excel(\"dian.xlsx\", range=\"A7:C313\", sheet = \"Rec mensual a junio 2023\" )\naños&lt;-2000:2023\ndian&lt;-dplyr::filter(dian,Año %in% años)\ncolnames(dian)&lt;-c(\"Año\",\"Mes\",\"Impuestos\")\ndian$fecha&lt;-as.Date(paste(dian$Año, dian$Mes, \"1\", sep = \"-\"), format = \"%Y-%B-%d\")\ndian&lt;-dian[,3:4]\n\n# Gráfico de la serie de tiempo\ndian2&lt;-ts(dian$Impuestos,start=c(2000,01),frequency=12)\nplot(dian2, main=\"Serie de tiempo del recaudo mensual interno\",\n     cex.main=1.3,\n     xlab=\"Tiempo\",\n     ylab=\"Recaudo interno\",\n     cex.lab=0.4)\n\n\n\n\nFigura 1: Gráfico de la serie de tiempo de la DIAN."
  },
  {
    "objectID": "Descriptivo Dian.html",
    "href": "Descriptivo Dian.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Descriptivo Energia.html",
    "href": "Descriptivo Energia.html",
    "title": "2  Análisis descriptivo Energía",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Descriptivo Dian.html#estabilización-de-la-varianza-marginal",
    "href": "Descriptivo Dian.html#estabilización-de-la-varianza-marginal",
    "title": "2  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "2.1 Estabilización de la varianza marginal",
    "text": "2.1 Estabilización de la varianza marginal\nTeniendo en cuenta lo observado utilizando el gráfico de la serie de tiempo, se puede evidenciar una heterocedasticidad marginal la cual debemos corregir, para esto utilizaremos la transformación de Box-cox, esto teniendo en cuenta que no se tienen valors negativos, de este modo se tiene que\n\nMASS::boxcox(lm(dian2 ~ 1),seq(-5, 5, length = 50)) ##Notese que no acputra al 1\n\n\n\nforecast::BoxCox.lambda(dian2, method =\"loglik\",\n                        lower = -1, upper = 3)#Entrega el valor de lambda (0.1).\n\n[1] 0.1\n\n\nCon el anterior gráfico y salida podemos observar que el valor que maximiza la log-verosimilitud es 0.1, pero este valor es bastante cercano a 0, por lo tanto lo aproximaremos para asi poder utilizar la transformación logaritmo, notese que el intervalo de confianza no captura al 1 por lo tanto puede ser conveniente realizar la transformación. En los gráficos posteriores analizaremos si dicha transformación logra estabilizar la varianza marginal de nuestros datos.\n\nplot(forecast::BoxCox(dian2,lambda=0.1))\n\n\n\npar(mar = c(1,1,1,1))\nldian2=log(dian2)\nMASS::boxcox(lm(ldian2 ~ 1),seq(-5, 5, length = 50)) #Si captura al 1\n\n\n\n\nNotese que el intervalo de confianza de la transformación Box-cox logra capturar el valor de 1, lo cual nos indica que no es necesario transformar los datos nuevamente y que la transformación ayudo de buena manera a estabilizar la varianza marginal de nuestra serie de tiempo.\nEn el siguiente gráfico mostramos la serie de recaudo de la Dian con y sin la transformación logaritmo y es posible observar que la escala disminuye pero a su vez se observan algunos cambios considerables en la forma de la serie, lo cual es un buen indicativo de la relevancia de realizar la transformación.\n\n#par(mfrow=c(2,1))\nplot(dian2,main=\"Serie Dian sin Transformar\")\n\n\n\nplot(ldian2,main=\"Series Dian con Transformación BoxCox\")\n\n\n\n\nTeniendo lo anterior en cuenta, presentaremos un gráfico de la serie sin tendencia un poco más interactivo e informativo, con el fin de lograr conocer los distintos valores en cada una de las fechas.\n\nclass(ldian2)\n\n[1] \"ts\"\n\ndian3<-window(ldian2, start = c(2000,1))\nts_plot(dian3,title=\"Serie de tiempo del recaudo mensual interno\",\n        Ytitle=\"Recaudo interno\",\n        Xtitle=\"Tiempo\",\n        Xgrid=TRUE,\n        Ygrid=TRUE)"
  },
  {
    "objectID": "index.html#análisis-del-consumo-de-energía-de-la-empresa-pjm",
    "href": "index.html#análisis-del-consumo-de-energía-de-la-empresa-pjm",
    "title": "Análisis de series de tiempo",
    "section": "Análisis del consumo de energía de la empresa PJM",
    "text": "Análisis del consumo de energía de la empresa PJM\nLa empresa PJM es una organización de transmisión regional que coordina el movimiento de electricidad mayorista en la totalidad, o parte, de 13 estados y el Distrito de Columbia.\nEl análisis del consumo de energía es esencial para mejorar la eficiencia operativa, reducir costos, cumplir con regulaciones y promover la sostenibilidad, por lo cual, este proyecto analiza la serie de tiempo con el fin de encontrar variaciones en el consumo de energía de los 13 estados y el Distrito de Columbia a lo largo del tiempo, así como también descubrir posibles patrones.\nA continuación se presenta la manera en que se realiza la carga de los datos y un vistazo preliminar de la serie de tiempo en la Figura 2 .\n\n# Carga de la base de datos\nAEP_hourly&lt;-read.csv(\"AEP_hourly.csv\")\nAEP_hourly$Datetime&lt;-as.POSIXct(AEP_hourly$Datetime, format = \"%Y-%m-%d %H:%M:%S\")\nAEP_hourly$fecha&lt;-as.Date(AEP_hourly$Datetime)\n\nenergia &lt;- AEP_hourly %&gt;%\n  group_by(fecha) %&gt;%\n  summarise(Energia = sum(AEP_MW))\nenergia&lt;-energia[-5055,]\n\n# Gráfico de la serie de tiempo\nenergia2&lt;-ts(energia$Energia,start=c(2004,10,01),frequency=365.25)\nplot(energia2, main=\"Serie de tiempo de la energía diaria de una empresa estadounidense\",\n     cex.main=1,\n     xlab=\"Tiempo \",\n     ylab=\"Energía consumida\",\n     cex.lab=0.4)\n\n\n\n\nFigura 2: Gráfico de la serie de tiempo de la energía."
  },
  {
    "objectID": "index.html#sobre-el-trabajo",
    "href": "index.html#sobre-el-trabajo",
    "title": "Análisis de series de tiempo",
    "section": "Sobre el trabajo",
    "text": "Sobre el trabajo\nEste trabajo realiza un análisis de dos bases de datos: Recaudo de impuestos internos por la DIAN para los años 2000 a 2023 y consumo de energía por horas de la organización regional de transmisión PJM Interconnection para los años 2004 a 2018. El análisis se desarrolla de forma estadística, realizando la parte descriptiva con su correspondiente interpretación."
  },
  {
    "objectID": "Descriptivo Energia.html#estabilización-de-la-varianza-marginal",
    "href": "Descriptivo Energia.html#estabilización-de-la-varianza-marginal",
    "title": "2  Análisis del consumo de energía de la empresa PJM",
    "section": "2.1 Estabilización de la varianza marginal",
    "text": "2.1 Estabilización de la varianza marginal\nComo se observa en la gráfica de la serie de tiempo no es necesario realizar una estabilización de la varianza para continuar con el análisis descriptivo, sin embargo, para comprobar esto, se hace una transformación de Box-cox para ver que tanto se estabiliza la varianza. En la Figura 2.2 se observa que se sugiere una transformación dado que 1 no está contenido en el intervalo.\n\nMASS::boxcox(lm(energia2 ~ 1),seq(-5, 5, length = 50))\nabline(v = 1, col = \"red\", lty = 2)\n\n\n\n\nFigura 2.2: Gráfico de la verosimilitud en función del hiperparámetro lambda.\n\n\n\n\nEn la siguiente salida se puede ver que el \\(\\lambda\\) sugerido es \\(-0.25\\), como es un número negativo, se procede a hacer la transformación Box-Cox usando logaritmo natural.\n\nforecast::BoxCox.lambda(energia2, method =\"loglik\",lower = -1, upper = 3)\n\n[1] -0.25\n\n\nEn la Figura 2.3 a continuación se muestra que la serie en escala logarítmica nuevamente no tiene la varianza estabilizada, dado que no se contiene al 1.\n\nlenergia2=log(energia2)\nMASS::boxcox(lm(lenergia2 ~ 1),seq(-5, 5, length =  50))\nabline(v = 1, col = \"red\", lty = 2)\n\n\n\n\nFigura 2.3: Gráfico de la verosimilitud para la serie en escala logarítmica, en función del hiperparámetro lambda.\n\n\n\n\nAdemás, se puede notar en la Figura 2.4 , que no hay una diferencia significativa entre la serie transformada y no transformada. Por lo que el análisis descriptivo se continúa usando los datos originales.\n\npar(mar = c(1,1,1,1))\npar(mfrow=c(2,1),mar=c(3,3,3,3))\nplot(energia2,main=\"Serie energía sin Transformar\",cex.main=1)\nplot(lenergia2,main=\"Serie energía con Transformación BoxCox\",cex.main=1)\n\n\n\n\nFigura 2.4: Serie original y serie con transformación logarítmica."
  },
  {
    "objectID": "Descriptivo Energia.html#estimación-preliminar-de-la-tendencia",
    "href": "Descriptivo Energia.html#estimación-preliminar-de-la-tendencia",
    "title": "2  Análisis del consumo de energía de la empresa PJM",
    "section": "2.2 Estimación preliminar de la tendencia",
    "text": "2.2 Estimación preliminar de la tendencia\nComo se observa en la gráfica de la serie de tiempo no es necesario realizar una estimación de la tendencia para continuar con el análisis descriptivo, sin embargo, se hace una estimación preliminar usando varios métodos.\n\n2.2.1 Tendencia lineal\n\n# Creación del objeto tibble\nenergia_1=energia %&gt;% map_df(rev)\nFechas=as.Date(energia_1$fecha)\nenergia_xts=xts(x = energia_1$Energia,frequency = 365.25,order.by = Fechas)\n\n# Creación objeto tssible a partir del objeto tibble\ndf_energia=data.frame(Energia=energia_1$Energia,fecha=energia_1$fecha)\ntbl_energia=tibble(df_energia)\ntbl_energia_format_fecha=tbl_energia\ntsbl_energia=as_tsibble(tbl_energia_format_fecha,index=fecha)\n\nEn la siguiente salida se presenta el ajuste de una regresión lineal para estimar la tendencia. Como el \\(R^2\\) es \\(0.058\\), se sugiere que no hay tendencia.\n\n# Análisis de tendencia con regresion simple\nsummary(fit_e&lt;-lm(energia2~time(energia2),na.action=NULL))\n\n\nCall:\nlm(formula = energia2 ~ time(energia2), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-137653  -34064   -5533   31129  179275 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6128528.2   325172.5   18.85   &lt;2e-16 ***\ntime(energia2)   -2862.7      161.7  -17.70   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45920 on 5052 degrees of freedom\nMultiple R-squared:  0.05841,   Adjusted R-squared:  0.05823 \nF-statistic: 313.4 on 1 and 5052 DF,  p-value: &lt; 2.2e-16\n\n\nEn la Figura 2.4 se presenta la serie de tiempo de la energía con la estimación lineal de la tendencia.\n\nplot(energia2, main=\"Serie de tiempo de la energía diaria de una empresa estadounidense\",\n     cex.main=1,\n     xlab=\"Tiempo \",\n     ylab=\"Energía consumida\",\n     cex.lab=0.4)\nabline(fit_e,col=\"darkcyan\",lwd=2)\n\n\n\n\nFigura 2.4: Gráfico de la serie de tiempo de la energía con la estimación lineal de la tendencia.\n\n\n\n\nPosteriormente, se procede a eliminar la tendencia lineal, como se puede ver en la Figura 2.5 .\n\n# Eliminación de la tendencia con la predicción la recta\nElimiTendenerg&lt;-energia2-predict(fit_e)\nplot(ElimiTendenerg,main=\"Serie energía sin tendencia\",\n     cex.main=1.3,\n     xlab=\"Tiempo\",\n     ylab=\"Consumo de energía\",\n     cex.lab=0.4)\n\n\n\n\nFigura 2.5: Gráfico de la serie de tiempo de la energía sin tendencia estimada con regresión lineal.\n\n\n\n\n\n\n2.2.2 Tendencia con promedios móviles\nEn la Figura 2.6 , se muestra un ajuste de la tendencia con promedios móviles, como se puede ver, aparentemente hay una sobrestimación de la tendencia, ya que muestra comportamientos que no son tan visibles en la serie de tiempo original.\n\n# Descomposición filtro de promedios móviles\nenergia_decompo=decompose(energia2)\nplot(energia_decompo)\n\n\n\n\nFigura 2.6: Gráfico de la serie de tiempo de la energía sin tendencia estimada con promedios móviles.\n\n\n\n\n\n\n2.2.3 Tendencia con diferenciación\nEn la Figura 2.7 se presentan los gráficos de las series sin tendencia, estimada con regresión lineal y con diferenciación respectivamente, se puede notar que la serie sin tendencia estimada con diferenciación impide ver los ciclos que se ven en la serie original.\n\ntsibble_energia&lt;-as_tsibble(energia2)\npar(mar = c(2,2,2,2))\npar(mfrow=c(2,1))\n\nplot(resid(fit_e), type=\"l\", main=\"Sin tendencia\") \nplot(diff(energia2), type=\"l\", main=\"Primera Diferencia\") \n\n\n\n\nFigura 2.7: Gráfico de la serie de tiempo de la energía sin tendencia estimada con regresión lineal y diferenciación.\n\n\n\n\n\n\n2.2.4 Comparación de los ACF\nEn la Figura 2.8 se puede notar un descenso rápido hacia 0 para las series original y sin tendencia estimada con regresión lineal, mientras que para la serie sin tendencia estimada con diferenciación, se puede apreciar mejor el ciclo estacional de aproximadamente 7 días.\n\n# Gráficos de los ACF\npar(mar = c(3,2,3,2))\npar(mfrow=c(3,1))\nacf(energia2, 60, main=\"ACF energia\")\nacf(resid(fit_e), 60, main=\"ACF Sin tendencia\") \nacf(diff(energia2), 60, main=\"ACF Primera Diferencia\")\n\n\n\n\nFigura 2.8: Gráficos de autocorrelación para la serie original, sin tendencia estimada con regresión lineal y con la primera diferencia."
  },
  {
    "objectID": "Descriptivo Energia.html#estimación-de-la-estacionalidad",
    "href": "Descriptivo Energia.html#estimación-de-la-estacionalidad",
    "title": "2  Análisis del consumo de energía de la empresa PJM",
    "section": "2.4 Estimación de la estacionalidad",
    "text": "2.4 Estimación de la estacionalidad\n\n2.4.1 Detección de estacionalidad\n\nlineal_1&lt;-cbind(as.matrix(ElimiTendenerg),as.character(energia$fecha))\nlineal_1&lt;-as.data.frame(lineal_1)\nnames(lineal_1)&lt;-c(\"Energia\",\"fecha\")\n\nlineal_1$Energia&lt;-as.numeric(lineal_1$Energia)\nlineal_1$fecha&lt;-as.Date(lineal_1$fecha)\n\ndf_lineal=data.frame(Energia=lineal_1$Energia,fecha=lineal_1$fecha)\ntbl_lineal=tibble(df_lineal)\ntbl_lineal_format_fecha=tbl_lineal\ntsbl_lineal=as_tsibble(tbl_lineal_format_fecha,index=fecha)\n\nEn la Figura 2.10 se presenta el gráfico de subseries diarias para la serie original, se puede ver que hay estacionalidad ya que el valor medio del día domingo por ejemplo, es menor al del resto de días.\n\n# Gráfica de subseries semanal con datos originales\ngg_subseries(tsbl_lineal,y=Energia,period=7)\n\n\n\n\nFigura 2.10: Gráfica de subseries diarias.\n\n\n\n\nEn la Figura 2.11 se presenta el gráfico de subseries mensuales para la serie original, se puede ver que no hay ciclos estacionales mensuales, ya que todos tienen la misma media. Sin embargo, esto puede deberse a la presencia de la múlriple estacionalidad.\n\n# Gráfica de subseries anual con datos originales\ngg_subseries(tsbl_lineal,y=Energia,period=12)\n\n\n\n\nFigura 2.11: Gráfica de subseries anuales.\n\n\n\n\n\nenergia_df&lt;-cbind(as.matrix(ElimiTendenerg),as.character(energia$fecha))\nenergia_df&lt;-as.data.frame(energia_df)\nnames(energia_df)&lt;-c(\"Energia\",\"Fecha\")\n\nenergia_df$Fecha&lt;-as.Date(energia_df$Fecha)\nenergia_df$time = as.POSIXct(energia_df$Fecha, \"%Y-%m-%d\")\nenergia_df$weekday &lt;- wday(energia_df$time, label = TRUE, abbr = TRUE)\nenergia_df$month &lt;- factor(month.abb[month(energia_df$time)], levels =   month.abb)\n\n# Agrupamos por mes y día\nenergia_df$Energia&lt;-as.numeric(energia_df$Energia)\nenergia_mensual &lt;- energia_df %&gt;%\n  dplyr::filter(weekday == \"dom\\\\.\" | weekday == \"mar\\\\.\" ) %&gt;% # martes se parece al comportamiento de lunes-viernes, domingo se parece a sabado\n  dplyr::group_by(weekday, month) %&gt;%\n  dplyr::summarise(mean = mean(Energia, na.rm = TRUE),\n                   sd = sd(Energia, na.rm = TRUE))\n\n# Grafico consumo (diferenciado) de energia mensual por dia\nplot_ly(data = energia_mensual, x = ~ month, y = ~ mean, type =\n          \"bar\",color = ~ weekday) %&gt;%\n  layout(title = \"Promedio diario de energía por día de la semana\",\n         yaxis = list(title = \"Media\"),\n         xaxis = list(title = \"Mes\"))\n\n\n\n\n\n\n\n2.4.2 Periodograma\nEn la Figura 2.12 se presenta el periodograma para la serie sin tendencia lineal, el valor del periodo donde se maximiza el periodograma nuevamente es \\(182.86\\), es decir, aproximadamente, el ciclo es de medio año.\n\n# Periodograma sin tendencia lineal\nspectrum(as.numeric(ElimiTendenerg),log='no')\n\nPeriodogramaEnergia2_lineal=spectrum(as.numeric(ElimiTendenerg),log='no')\nubicacionlogenergia=which.max(PeriodogramaEnergia2_lineal$spec)\n\nsprintf(\"El valor de la frecuencia donde se maximiza el periodograma para la serie es: %s\",PeriodogramaEnergia2_lineal$freq[ubicacionlogenergia])\n\n[1] \"El valor de la frecuencia donde se maximiza el periodograma para la serie es: 0.00546875\"\n\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/PeriodogramaEnergia2_lineal$freq[ubicacionlogenergia])\n\n[1] \"El periodo correspondiente es aproximadamente: 182.857142857143\"\n\n\n\n\n\nFigura 2.12: Periodograma para la serie sin tendencia lineal.\n\n\n\n\n\n2.4.2.1 Para la serie sin tendencia usando diferenciación\nEn la Figura 2.13 se presenta el periodograma para la serie sin tendencia estimada usando diferenciación, el valor del periodo donde se maximiza el periodograma es \\(3.5\\), es decir, aproximadamente, el ciclo es de tres días.\n\n# Periodograma diferenciación\nspectrum(as.numeric(diff(energia2)),log='no')\n\nPeriodogramaEnergia2_dif=spectrum(as.numeric(diff(energia2)),log='no')\nubicacionlogenergia=which.max(PeriodogramaEnergia2_dif$spec)\n\nsprintf(\"El valor de la frecuencia donde se maximiza el periodograma para la serie es: %s\",PeriodogramaEnergia2_dif$freq[ubicacionlogenergia])\n\n[1] \"El valor de la frecuencia donde se maximiza el periodograma para la serie es: 0.2857421875\"\n\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/PeriodogramaEnergia2_dif$freq[ubicacionlogenergia])\n\n[1] \"El periodo correspondiente es aproximadamente: 3.49965823650034\"\n\n\n\n\n\nFigura 2.13: Periodograma para la serie sin tendencia usando diferenciación.\n\n\n\n\n\n\n\n2.4.3 Estimación\nAhora procederemos a estimar el ciclo estacional que se observa en esta serie de tiempo, es importante resaltar que con ayuda de los graficos exploratorios y el periodograma se observo que el periodo de la componente estacional es \\(s=12\\), por lo tanto utilizaremos en primer lugar componentes de fourier, esto teniendo en cuenta que se aprecia que la componente estacional sigue un comportamiento deterministico y posiblemente sinosoidal. Teniendo lo anterior en cuenta el modelo viene dado por: \\[\\begin{align*} x_t&= ∑_{i=1}^k a_icos(k𝜔t)+b_isen(k𝜔t) + w_t \\\\ \\end{align*}\\] Donde \\(k\\) corresponderá al orden de la expansión en series de Fourier y los coeficientes \\(a_i\\) y \\(b_i\\) con \\(i=1,...,k\\) serán estimados a través del método de mínimos cuadrados. El cálculo de esta componente se muestra a continuación considerando un orden \\(k=3\\).\n\n# Frecuencia angular w=2*pi/s\nfrec_ang=(2*pi/182)\nfrec_ang2=(2*pi/7)\n\nenergia_copia&lt;-cbind(as.matrix(ElimiTendenerg),as.character(energia$fecha))\nenergia_copia&lt;-as.data.frame(energia_copia)\nnames(energia_copia)&lt;-c(\"Energia\",\"fecha\")\n\nenergia_copia$fecha&lt;-as.Date(energia_copia$fecha)\n\n#Fourier k=1 \nenergia_copia$sin = sin(c(1:5054)*(1*frec_ang))\nenergia_copia$cos = cos(c(1:5054)*(1*frec_ang))\n\n#Fourier k=2 \nenergia_copia$sin2 = sin(c(1:5054)*(2*frec_ang))\nenergia_copia$cos2 = cos(c(1:5054)*(2*frec_ang))\n\n#Fourier k=3 \nenergia_copia$sin3 = sin(c(1:5054)*(3*frec_ang))\nenergia_copia$cos3 = cos(c(1:5054)*(3*frec_ang))\n\n\n#Fourier k=1 \nenergia_copia$sin4 = sin(c(1:5054)*(1*frec_ang2))\nenergia_copia$cos4 = cos(c(1:5054)*(1*frec_ang2))\n\n#Fourier k=2 \nenergia_copia$sin5 = sin(c(1:5054)*(2*frec_ang2))\nenergia_copia$cos5 = cos(c(1:5054)*(2*frec_ang2))\n\n#Fourier k=3 \nenergia_copia$sin6 = sin(c(1:5054)*(3*frec_ang2))\nenergia_copia$cos6 = cos(c(1:5054)*(3*frec_ang2))\n\nlinmodel_ciclo&lt;-lm(Energia~1+sin+cos+sin2+cos2+sin3+cos3+sin4+cos4+sin5+cos5+sin6+cos6,data=energia_copia)\n\nresults_ciclo=linmodel_ciclo$fitted.values\nresults_ciclo&lt;-as.data.frame(results_ciclo)\nresults_ciclo_ts&lt;-ts(results_ciclo,start=c(2004,10,01),frequency=365.25)\n\nplot(ElimiTendenerg, main=\"Serie de tiempo de la energía diaria de una empresa estadounidense\",\n     cex.main=1.3,\n     xlab=\"Tiempo \",\n     ylab=\"Energía consumida\",\n     cex.lab=0.4)\nlines(results_ciclo_ts,col=\"red\")\n\n\n\nplot(ElimiTendenerg, main=\"Serie de tiempo de la energía diaria de una empresa estadounidense\",\n     cex.main=1.3,\n     xlab=\"Tiempo \",\n     ylab=\"Energía consumida\",\n     cex.lab=0.4,\n     xlim=c(2004,2007))\nlines(results_ciclo_ts,col=\"red\",xlim=c(2004,2007))\n\n\n\n\nEn la ?fig-energestacionalidad se presenta la serie original con la estimación de la componente estacional via componentes de Fourier y la serie sin estacionalidad. Se puede notar que las componentes de Fourier logran captar bien es cíclo estacional que tiene la serie. Se toma una componente de Fourier, ya que no hay una diferencia visible al utilizar 2 y 3.\n\nenergia_estacionarios&lt;-ElimiTendenerg-results_ciclo_ts\nsaveRDS(energia_estacionarios, file=\"energia_estacionarios.RDS\")\nplot(ElimiTendenerg-results_ciclo_ts)\nplot(ElimiTendenerg-results_ciclo_ts,xlim=c(2004,2007))\n\n\n\n\nFigura 2.14: Serie original con estamación de la componente estacional y serie sin estacionalidad.\n\n\n\n\n\n\n\nFigura 2.15: Serie original con estamación de la componente estacional y serie sin estacionalidad.\n\n\n\n\nA continuación, presentaremos el modelo de árboles y ell modelo de redes neuronales multicapa paraa esta serie de tiempo, disponible aquí."
  },
  {
    "objectID": "Descriptivo Energia.html#estimación-de-la-tendencia",
    "href": "Descriptivo Energia.html#estimación-de-la-tendencia",
    "title": "2  Análisis del consumo de energía de la empresa PJM",
    "section": "2.2 Estimación de la tendencia",
    "text": "2.2 Estimación de la tendencia\nComo se observa en la gráfica de la serie de tiempo no es necesario realizar una estimación de la tendencia para continuar con el análisis descriptivo, sin embargo, se hace una estimación preliminar usando varios métodos, con el fin de comprobar que la serie sin tendencia no varía mucho.\n\n2.2.1 Tendencia lineal\n\n# Creación del objeto tibble\nenergia_1=energia %&gt;% map_df(rev)\nFechas=as.Date(energia_1$fecha)\nenergia_xts=xts(x = energia_1$Energia,frequency = 365.25,order.by = Fechas)\n\n# Creación objeto tssible a partir del objeto tibble\ndf_energia=data.frame(Energia=energia_1$Energia,fecha=energia_1$fecha)\ntbl_energia=tibble(df_energia)\ntbl_energia_format_fecha=tbl_energia\ntsbl_energia=as_tsibble(tbl_energia_format_fecha,index=fecha)\n\nEn la siguiente salida se presenta el ajuste de una regresión lineal para estimar la tendencia. El \\(R^2\\) indíca qué tan bien se ajusta la recta a los datos, en este caso tiene un valor de \\(0.058\\), por lo que sugiere que no hay tendencia lineal.\n\n# Análisis de tendencia con regresion simple\nsummary(fit_e&lt;-lm(energia2~time(energia2),na.action=NULL))\n\n\nCall:\nlm(formula = energia2 ~ time(energia2), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-137653  -34064   -5533   31129  179275 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6128528.2   325172.5   18.85   &lt;2e-16 ***\ntime(energia2)   -2862.7      161.7  -17.70   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45920 on 5052 degrees of freedom\nMultiple R-squared:  0.05841,   Adjusted R-squared:  0.05823 \nF-statistic: 313.4 on 1 and 5052 DF,  p-value: &lt; 2.2e-16\n\n\nEn la Figura 2.5 se presenta la serie de tiempo de la energía con la estimación lineal de la tendencia.\n\nplot(energia2, main=\"Serie de tiempo de la energía diaria de una empresa estadounidense\",\n     cex.main=1,\n     xlab=\"Tiempo \",\n     ylab=\"Energía consumida\",\n     cex.lab=0.4)\nabline(fit_e,col=\"darkcyan\",lwd=2)\n\n\n\n\nFigura 2.5: Gráfico de la serie de tiempo de la energía con la estimación lineal de la tendencia.\n\n\n\n\nPosteriormente, se procede a eliminar la tendencia lineal, como se puede ver en la Figura 2.6 .\n\n# Eliminación de la tendencia con la predicción la recta\nElimiTendenerg&lt;-energia2-predict(fit_e)\nplot(ElimiTendenerg,main=\"Serie energía sin tendencia\",\n     cex.main=1.3,\n     xlab=\"Tiempo\",\n     ylab=\"Consumo de energía\",\n     cex.lab=0.4)\n\n\n\n\nFigura 2.6: Gráfico de la serie de tiempo de la energía sin tendencia estimada con regresión lineal.\n\n\n\n\n\n\n2.2.2 Tendencia con promedios móviles\nEn la Figura 2.7 , se muestra un ajuste de la tendencia con promedios móviles, como se puede ver, aparentemente hay una sobrestimación de la tendencia, ya que muestra comportamientos que no son tan visibles en la serie de tiempo original.\n\n# Descomposición filtro de promedios móviles\nenergia_decompo=decompose(energia2)\nplot(energia_decompo)\n\n\n\n\nFigura 2.7: Gráfico de la serie de tiempo de la energía sin tendencia estimada con promedios móviles.\n\n\n\n\n\n\n2.2.3 Tendencia con diferenciación\nEn la Figura 2.8 se presentan los gráficos de las series sin tendencia, estimada con regresión lineal y con diferenciación respectivamente, se puede notar que la serie sin tendencia estimada con diferenciación impide ver los ciclos que se ven en la serie original.\n\ntsibble_energia&lt;-as_tsibble(energia2)\npar(mar = c(2,2,2,2))\npar(mfrow=c(2,1))\n\nplot(resid(fit_e), type=\"l\", main=\"Sin tendencia lineal\") \nplot(diff(energia2), type=\"l\", main=\"Primera Diferencia\") \n\n\n\n\nFigura 2.8: Gráfico de la serie de tiempo de la energía sin tendencia estimada con regresión lineal y diferenciación.\n\n\n\n\n\n\n2.2.4 Comparación de los ACF\nEn la Figura 2.9 se puede notar un descenso rápido hacia 0 para las series original y sin tendencia estimada con regresión lineal, mientras que para la serie sin tendencia estimada con diferenciación, se puede apreciar mejor el ciclo estacional de aproximadamente 7 días.\n\n# Gráficos de los ACF\npar(mar = c(3,2,3,2))\npar(mfrow=c(3,1))\nacf(energia2, 60, main=\"ACF energia\")\nacf(resid(fit_e), 60, main=\"ACF Sin tendencia\") \nacf(diff(energia2), 60, main=\"ACF Primera Diferencia\")\n\n\n\n\nFigura 2.9: Gráficos de autocorrelación para la serie original, sin tendencia estimada con regresión lineal y con la primera diferencia.\n\n\n\n\nSi bien se estima la tendencia con diferentes métodos, se decide trabajar con la serie sin tendencia líneal."
  },
  {
    "objectID": "Descriptivo Dian.html#tendencia-estimación-y-eliminación",
    "href": "Descriptivo Dian.html#tendencia-estimación-y-eliminación",
    "title": "2  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "2.2 Tendencia Estimación y eliminación",
    "text": "2.2 Tendencia Estimación y eliminación\nLuego de estabilizar la varianza marginal de nuestra serie, procederemos a estimar la tendencia y a eliminarla. Para estimar dicha tendencia inicaremos utilizando una tendencia lineal deterministica y posteriormente restaremos dicha tendencia estimada a los datos de nuestra serie, de este modo se tiene lo siguiente:\n\nsummary(fit<-lm(ldian2~time(ldian2),na.action=NULL))\n\n\nCall:\nlm(formula = ldian2 ~ time(ldian2), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6587 -0.2338  0.0349  0.2164  0.8567 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -188.74811    5.53318  -34.11   <2e-16 ***\ntime(ldian2)    0.10150    0.00275   36.90   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3133 on 280 degrees of freedom\nMultiple R-squared:  0.8295,    Adjusted R-squared:  0.8289 \nF-statistic:  1362 on 1 and 280 DF,  p-value: < 2.2e-16\n\nplot(ldian2,ylab=\"Recaudo interno\") \nabline(fit,col=\"darkcyan\",lwd=2)\n\n\n\n\nPreliminarmente es posible ver que la recta se ajusta de un buen modo a nuestra serie de tiempo, puesto que la tendencia de nuestra serie es creciente, ahora procederemos a observar la serie de tiempo al eliminar la tendencia utilizando este metodo y tenemos lo siguiente:\n\nElimiTenddian<-ldian2-predict(fit)\nplot(ElimiTenddian,main=\"Serie Dian sin tendencia y con varianza estable\",\n     cex.main=1.3,\n     xlab=\"Tiempo\",\n     ylab=\"Recaudo interno\",\n     cex.lab=0.4)\n\n\n\n\nEs posible observar que nuestra serie de tiempo cambio considerablemente, puesto que la escala de los valores se disminujo bastante y además los datos oscilan al rededor del 0, lo cual nos indica que nuestra serie no presenta tendencia\n\nacf(ElimiTenddian,lag.max=179,main=\"Acf Serie Dian sin tendencia\")\n\n\n\n\nNotese que el gráfico de autocorrelación, nos indica preliminarmente la presencia de posibles componente estacionales, pero esto lo analizaremos un poco más en detalle más adelante.\nPero este no es el unico modo de realizar la estimación de la tendencia, tambien podemos utilizar herramientas no parametricas, pero como no hemos identificado la componente estacional, estas nos daran una estimación preliminar de la tendencia. La primera de ellas que vamos a utilizar es la descomposición via filtros de promedios moviles, que se presenta a continuación:\n\ndian_decompo=decompose(ldian2)\nplot(dian_decompo)\n\n\n\n#dian_decompo$trend\n\nNotese que la tendencia estimada via filtros de promedio moviles es aproximadamente lineal, lo cual respalda la idea de utilizar la forma deterministica de estimar la varianza, además parece estimar de mdodo correcto la componente estacional y en la componente residual se observan algunos patrones estacionales.\nAhora procederemos a utilizar la descomposción STL, para hacer un análisis similar.\n\nlibrary(feasts)\nlibrary(fable)\n### Gráfico ##\ntsibble_dian<-as_tsibble(ldian2)\nstr(tsibble_dian)\n\ntbl_ts [282 × 2] (S3: tbl_ts/tbl_df/tbl/data.frame)\n $ index: mth [1:282] 2000 ene., 2000 feb., 2000 mar., 2000 abr., 2000 may., 200...\n $ value: num [1:282] 14.2 13.9 14 14.1 14.1 ...\n - attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ .rows: list<int> [1:1] \n  .. ..$ : int [1:282] 1 2 3 4 5 6 7 8 9 10 ...\n  .. ..@ ptype: int(0) \n - attr(*, \"index\")= chr \"index\"\n  ..- attr(*, \"ordered\")= logi TRUE\n - attr(*, \"index2\")= chr \"index\"\n - attr(*, \"interval\")= interval [1:1] 1M\n  ..@ .regular: logi TRUE\n\ntsibble_dian %>%\n  model(\n    STL(value ~ trend() +\n          season(window = \"periodic\"),\n        robust = TRUE)) %>%\n  components() %>%\n  autoplot()\n\n\n\n#tsibble_dian_notendstl<-tsibble_dian$Log\n\nNotese que el gráfico anterior es bastante similar al obtenido utilizando filtro de promedios moviles y podemos realizar una interpretación bastante similar, Tambien es posible eliminar la tendencia utilizando diferenciación, pero se debe tener en cuenta que la diferenciación tambien puede eliminar la componente estacional que se tenga presente en la serie de tiempo, a continuación se muestra la manera en que se realizó la eliminación de la tendencia utilizando la diferenciación y su comparación con el primer metodo para eliminar tendencia utilizado.\n\npar(mar = c(2,2,2,2))\nfitdian = lm(ldian2~time(ldian2), na.action=NULL) \npar(mfrow=c(2,1))\nplot(resid(fitdian), type=\"l\", main=\"sin tendencia\") \nplot(diff(ldian2), type=\"l\", main=\"Primera Diferencia\") #Primera diferencia ordinaria\n\n\n\n\nNotese que cuando eliminamos la tendencia utilizando la diferenciación se observa un poco más centrada en comparación a cuando utilizamos la regresión lineal ajustada, esto puede ser producto de la posible eliminación de la componente estacional producto de la utilización de la diferenciación Revisar bien esta interpretacion con las muchachas\nEn el siguiente grafico se compara las funciones de autocorrelación obtenidas para la series cuando eliminamos la tendencia utilizando lm y utilizando diferenciación, junto con la función de autocorrelación para la serie de tiempo con varianza marginal estable.\n\npar(mar = c(3,2,3,2))\npar(mfrow=c(3,1)) # plot ACFs\nacf(ldian2, 60, main=\"ACF Dian objeto ts varianza estable por boxcox\")\nacf(resid(fitdian), 60, main=\"ACF Sin tendencia (resid(fitdian))\") \nacf(diff(ldian2), 60, main=\"ACF Primera Diferencia\")\n\n\n\n\nFalta interpretación WAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
  },
  {
    "objectID": "Descriptivo Dian.html#detección-de-estacionalidad",
    "href": "Descriptivo Dian.html#detección-de-estacionalidad",
    "title": "2  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "2.3 Detección de estacionalidad",
    "text": "2.3 Detección de estacionalidad\nLuego de estabilizar la varianza marginal y de tratar la tendencia, procedemos a observar si existe presencia de posibles ciclos o ciclos estacionales, para dicha tarea vamos a emplear multiples metodos descriptivos que nos permiten obtener información sobre esta componente. Iniciaremos por observar el grafico de retardos, el cual viene dado a continuación:\n\nts_info(ElimiTenddian)\n\n The ElimiTenddian series is a ts object with 1 variable and 282 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2023 6 \n\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimiTenddian, 12,corr=F)\n\n\n\nts_lags(ElimiTenddian,lags=1:12)\n\n\n\n\n\nEl gráfico de retardos nos indica de manera descriptiva la posible relación existente entre un tiempo y algunos de sus retardos, para este caso en particular se toman 12 retardos (esto teniendo en cuenta la frecuencia mensual de la serie de tiempo), en este caso es posible observar que existe una clara relación lineal y directa con el rezago 12, los demás rezagos no parecen ser del todo significativos. Notese que los dos graficos anteriores nos dan una información bastante similar.\nAhora observemos el gráfico de subseries, el cual toma los valores por cada mes de cada uno de los años dentro la serie, como sabemos buscamos obsevar si en el historico encontramos dieferentes valores medio mes tras mes, de este modo tenemos lo siguiente:\n\nrequire(feasts)\ndian2_tsbl_notend=as_tsibble(ElimiTenddian)\ndian2_tsbl_notend%>%gg_subseries(value)\n\n\n\n\nEs posible observar que la media en cada uno de los meses es distinta, esto es un claro indicio de la presencia de una componente ciclica estacional o ciclica, a contibuación se presentan algunas otras graficas descriptivas para observar la presencia de un ciclo estaciona.\n\ndian2sint_df <- data.frame(year = floor(time(ElimiTenddian)), month = cycle(ElimiTenddian),ElimiTenddian = as.numeric(ElimiTenddian))\ndian2sint_df$month <- factor(month.abb[dian2sint_df$month], levels = month.abb)\ndian2sint_summary <- dian2sint_df %>%group_by(month) %>%summarise(mean= mean(ElimiTenddian),sd = sd(ElimiTenddian))\ndian2sint_summary\n\n# A tibble: 12 × 3\n   month    mean     sd\n   <fct>   <dbl>  <dbl>\n 1 Jan    0.320  0.150 \n 2 Feb   -0.230  0.206 \n 3 Mar   -0.0446 0.0880\n 4 Apr    0.244  0.311 \n 5 May    0.196  0.226 \n 6 Jun    0.199  0.259 \n 7 Jul    0.0320 0.123 \n 8 Aug   -0.230  0.255 \n 9 Sep    0.240  0.187 \n10 Oct   -0.386  0.128 \n11 Nov    0.0676 0.0882\n12 Dec   -0.439  0.0985\n\nplot_ly (data = dian2sint_summary, x = ~ month, y = ~ mean, type = \"bar\", name   = \"Mean\") %>%\n  layout (title = \"dian2sint - Monthly Average\", yaxis =list(title = \"Mean\",   range = c(min(dian2sint_summary$mean), max(dian2sint_summary$mean))))\n\n\n\n\n\nEn el anterior gráfico se observa el valor medio tomado por cada uno de los meses, es posible observar que tiene un comportamiento parecido al grafico de subseries y de manera analoga nos muestra que exite una componente estacional.\nA continuación se muestran los mapas de calor para la serie Dian con varianza maeginal estable y tendencia eliminada por el metodo de diferenciacion y lineal.\n\nTSstudio::ts_heatmap(ElimiTenddian,title = \"Mapa de Calor - Impuestos Dian sin tendencia\")\n\n\n\n\nTSstudio::ts_heatmap(diff(ldian2),title = \"Mapa de Calor - Impuestos Dian sin tendencia\")\n\n\n\n\n\nAmbos mapas nos dan una información similar, en los cuales es posible observar que en los meses de noviembre, enero, junio, mayo y abril se tienen una mayor cantidad de recaudo de impuestos, mientras que en diciembre,octubre, agosto, marzo y febrero tienen un menor valor de recaudo de impuestos año tras año, nuevamente este gráfico nos ayuda a comprender la existencia de un cliclo estacional que posiblemente tenga un periodo de 12 meses.\n\n2.3.1 Periodograma\nCuando hablamos de una componente estacional dentro de nuestra serie de tiempo, tambien necesitamos hablar de su periodo y de su frecuencia, para esto utilizaremos el periodograma.\n\nspectrum(as.numeric(ElimiTenddian),log='no')\nabline(v=0.5, lty=2,col=\"red\")\n\n\n\nspectrum(as.numeric(ElimiTenddian),log='no',span=5)\n\n\n\nspectrum(as.numeric(ElimiTenddian),log='no',span=c(5,5))\n\n\n\nspectrum(as.numeric(ElimiTenddian),log='no',span=c(2,2))\n\n\n\n\nNotese que en los graficos anteriores se tienen diferentes valores de suavizamiento para nuestro periodograma, pues aunque en este caso no es dificil observar los puntos donde se tiene un pico, el suavizamiento puede ayudarnos a observar de mdood más simple los picos que son verdaderamente significativos.\n\nPeriodgramadldian2_sintendencia=spectrum(as.numeric(ElimiTenddian),log='no')\n\n\n\nubicacionlogdian=which.max(Periodgramadldian2_sintendencia$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodgramadldian2_sintendencia$freq[ubicacionlogdian])\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.5\"\n\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodgramadldian2_sintendencia$freq[ubicacionlogdian])\n\n[1] \"El periodo correspondiente es aproximadamente: 2\"\n\n\nNotese que según la salida obtenida, la frecuencia maxima se alcanza en 0.5 (es decir en 6/12=0.5) y se obtuvo que el periodo es 2, esto quiere decir que el ciclo se repite cada dos meses, pero notese que este es un multiplo no entero de 12, por lotanto en realidad se tiene que el periodo de la componente estacional serie de tipo anual (12 meses) Mirar esta interpretación con las muchachas y preguntar al profe si esta correcto."
  },
  {
    "objectID": "Descriptivo Dian.html#desestacionalizar-o-eliminación-de-la-componente-estacional",
    "href": "Descriptivo Dian.html#desestacionalizar-o-eliminación-de-la-componente-estacional",
    "title": "2  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "2.4 Desestacionalizar o eliminación de la componente estacional",
    "text": "2.4 Desestacionalizar o eliminación de la componente estacional"
  },
  {
    "objectID": "Descriptivo Dian (2).html#estabilización-de-la-varianza-marginal",
    "href": "Descriptivo Dian (2).html#estabilización-de-la-varianza-marginal",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "1.1 Estabilización de la varianza marginal",
    "text": "1.1 Estabilización de la varianza marginal\nTeniendo en cuenta lo observado utilizando el gráfico de la serie de tiempo, se puede evidenciar una heteroscedasticidad marginal la cual debemos corregir, para esto utilizaremos la transformación de Box-cox, la cual está dada por la siguiente fórmula. \\[\\begin{equation}\n    f_{\\lambda}(u_{t})= \\begin{cases}\n        \\lambda^{-1}(u^{\\lambda}_{t}-1), &  \\text{si  $u_{t} \\geq 0$, para $\\lambda&gt;0$,}\\\\\n        \\ln(u_{t}), &\\text{ si $u_{t}&gt;0$, para $\\lambda=0$}.\n    \\end{cases}\n\\end{equation}\\]\nDonde el \\(λ\\) apropiado debe ser estimado. Nótese que esta transformación es posible, puesto que no se tienen valores negativos, de este modo se tiene según la Figura 1.1 que:\n\nMASS::boxcox(lm(dian2 ~ 1),seq(-5, 5, length = 50)) ##Notese que no acputra al 1\nforecast::BoxCox.lambda(dian2, method =\"loglik\",\n                        lower = -1, upper = 3)#Entrega el valor de lambda (0.1).\n\n[1] 0.1\n\n\n\n\n\nFigura 1.1: Gráfico del valor lambda que maximiza la logverosmilitud.\n\n\n\n\nCon el anterior gráfico y salida podemos observar que el valor que maximiza la log-verosimilitud es 0.1, pero este valor es bastante cercano a 0, por lo tanto, lo aproximaremos a 0 para así poder utilizar la transformación logaritmo, nótese que el intervalo de confianza no captura al 1 en consecuencia puede ser conveniente realizar la transformación. En los gráficos de Figura 1.2 y Figura 1.3 analizaremos si dicha transformación logra estabilizar la varianza marginal de nuestros datos.\n\nplot(forecast::BoxCox(dian2,lambda=0.1))\n\n\n\n\nFigura 1.2: Grafico de la serie con varianza estable utilizando lambda=0.1.\n\n\n\n\n\npar(mar = c(1,1,1,1))\nldian2=log(dian2)\nMASS::boxcox(lm(ldian2 ~ 1),seq(-5, 5, length = 50)) #Si captura al 1\n\n\n\n\nFigura 1.3: Grafico de la serie con varianza estable utilizando el logaritmo.\n\n\n\n\nNótese que el intervalo de confianza de la transformación Box-cox logra capturar el valor de 1, lo cual nos indica que no es necesario transformar los datos nuevamente o buscar otro valor para \\(\\lambda\\), además la transformación ayudo de buena manera a estabilizar la varianza marginal de nuestra serie de tiempo.\nEn el gráfico Figura 1.4 y Figura 1.5 mostramos la serie de recaudo de la DIAN con y sin la transformación logaritmo y es posible observar que la escala disminuyey que a su vez se observan algunos cambios considerables en la forma de la serie, lo cual es un buen indicativo de la relevancia de realizar la transformación.\n\n#par(mfrow=c(2,1))\nplot(dian2,main=\"Serie Dian sin Transformar\")\n\n\n\n\nFigura 1.4: Serie DIAN sin transformar los datos utilizando el logaritmo.\n\n\n\n\n\nplot(ldian2,main=\"Series Dian con Transformación BoxCox\")\n\n\n\n\nFigura 1.5: Serie DIAN con transformación de los datos utilizando el logaritmo.\n\n\n\n\nTeniendo lo anterior en cuenta, presentaremos un gráfico (Figura 1.6) de la serie sin tendencia un poco más interactivo e informativo, con el fin de lograr conocer los distintos valores en cada una de las fechas.\n\nclass(ldian2)\n\n[1] \"ts\"\n\ndian3&lt;-window(ldian2, start = c(2000,1))\nts_plot(dian3,title=\"Serie de tiempo del recaudo mensual interno\",\n        Ytitle=\"Recaudo interno\",\n        Xtitle=\"Tiempo\",\n        Xgrid=TRUE,\n        Ygrid=TRUE)\n\n\n\n\nFigura 1.6: Gráfico de la serie DIAN con los datos transformados."
  },
  {
    "objectID": "Descriptivo Dian (2).html#tendencia-estimación-y-eliminación",
    "href": "Descriptivo Dian (2).html#tendencia-estimación-y-eliminación",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "1.2 Tendencia Estimación y eliminación",
    "text": "1.2 Tendencia Estimación y eliminación\nLuego de estabilizar la varianza marginal de nuestra serie, procederemos a estimar la tendencia y a eliminarla. Para estimar dicha tendencia iniciaremos utilizando una estimación lineal determinística de la tendencia y posteriormente restaremos la tendencia estimada a los datos de nuestra serie. EL ajuste de la recta se presenta en la Figura 1.7:\n\nsummary(fit&lt;-lm(ldian2~time(ldian2),na.action=NULL))\n\n\nCall:\nlm(formula = ldian2 ~ time(ldian2), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6587 -0.2338  0.0349  0.2164  0.8567 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -188.74811    5.53318  -34.11   &lt;2e-16 ***\ntime(ldian2)    0.10150    0.00275   36.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3133 on 280 degrees of freedom\nMultiple R-squared:  0.8295,    Adjusted R-squared:  0.8289 \nF-statistic:  1362 on 1 and 280 DF,  p-value: &lt; 2.2e-16\n\nplot(ldian2,ylab=\"Recaudo interno\") \nabline(fit,col=\"darkcyan\",lwd=2)\n\n\n\n\nFigura 1.7: Estimación de la tendencia de manera deterministica.\n\n\n\n\nPreliminarmente, es posible ver que la recta se ajusta de un buen modo a nuestra serie de tiempo, puesto que la tendencia de nuestra serie es monotona creciente, ahora procederemos a observar la serie de tiempo al eliminar la tendencia utilizando este método, esto se observa en la gráfica Figura 1.8:\n\nElimiTenddian&lt;-ldian2-predict(fit)\nplot(ElimiTenddian,main=\"Serie Dian sin tendencia y con varianza estable\",\n     cex.main=1.3,\n     xlab=\"Tiempo\",\n     ylab=\"Recaudo interno\",\n     cex.lab=0.4)\n\n\n\n\nFigura 1.8: Serie DIANsin tendencia.\n\n\n\n\nEs posible observar que nuestra serie de tiempo cambio considerablemente, puesto que la escala de los valores se disminuyó bastante y además los datos oscilan al rededor del 0, pero aun es posible observar una posible tendencia en nuestros datos, por lo tanto procedemos a analizar otras formas de eliminar la tendencia.\n\nacf(ElimiTenddian,lag.max=179,main=\"Acf Serie Dian sin tendencia\")\n\n\n\n\nFigura 1.9: Gráfico acf series DIAN sin tendencia.\n\n\n\n\nNótese que el gráfico de autocorrelación (Figura 1.9), nos indica preliminarmente la presencia de posibles componentes estacionales, pero esto lo analizaremos en detalle más adelante.\nPero este no es el único modo de realizar la estimación de la tendencia, también podemos utilizar herramientas no paramétricas, pero como no hemos identificado la componente estacional, estas nos darán una estimación preliminar de la tendencia. La primera de ellas que vamos a utilizar es la descomposición vía filtros de promedios móviles, que se presenta a continuación (Figura 1.10):\n\ndian_decompo=decompose(ldian2)\nplot(dian_decompo)\n#dian_decompo$trend\n\n\n\n\nFigura 1.10: Descomposición via filtros de promedio moviles.\n\n\n\n\nNótese que parece estimar de modo correcto la componente estacional y en la componente residual se observan algunos patrones estacionales, al análizar la tendencia de este modo, podemos ver que aunque parece bastante lineal existen algunos lugares donde se ajusta mejor a ciertos comportamientos de la serie, por lo tanto podria ser viable utilizar esta tecnica para explorar la componente de tendencia.\nAhora procederemos a utilizar la descomposición STL, para hacer un análisis similar(Figura 1.11).\n\nlibrary(feasts)\nlibrary(fable)\n### Gráfico ##\ntsibble_dian&lt;-as_tsibble(ldian2)\nstr(tsibble_dian)\n\ntbl_ts [282 × 2] (S3: tbl_ts/tbl_df/tbl/data.frame)\n $ index: mth [1:282] 2000 ene., 2000 feb., 2000 mar., 2000 abr., 2000 may., 200...\n $ value: num [1:282] 14.2 13.9 14 14.1 14.1 ...\n - attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ .rows: list&lt;int&gt; [1:1] \n  .. ..$ : int [1:282] 1 2 3 4 5 6 7 8 9 10 ...\n  .. ..@ ptype: int(0) \n - attr(*, \"index\")= chr \"index\"\n  ..- attr(*, \"ordered\")= logi TRUE\n - attr(*, \"index2\")= chr \"index\"\n - attr(*, \"interval\")= interval [1:1] 1M\n  ..@ .regular: logi TRUE\n\ntsibble_dian %&gt;%\n  model(\n    STL(value ~ trend() +\n          season(window = \"periodic\"),\n        robust = TRUE)) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\nFigura 1.11: Descomposición via STL.\n\n\n\n\n\n### Eliminando la tendencia por STL\nmodelo_stl &lt;- tsibble_dian %&gt;%\n  model(\n    STL(value ~ trend() +\n          season(window = \"periodic\"),\n        robust = TRUE)\n  ) %&gt;%\n  components()\n\n\nElimiTenddian_STL&lt;-ldian2-modelo_stl$trend\nplot(ElimiTenddian_STL,main=\"Serie Dian sin tendencia y con varianza estable (STL)\",\n     cex.main=1.3,\n     xlab=\"Tiempo\",\n     ylab=\"Recaudo interno\",\n     cex.lab=0.4)\n\n\n\n\nFigura 1.12: Desrie DIAN sin tendencia via STL.\n\n\n\n\nEs posible observar que el gráfico anterior (Figura 1.12) es bastante similar al obtenido utilizando filtro de promedios móviles (Figura 1.10) y podemos realizar una interpretación bastante similar, para continuar con nuestro análisis procederemos a continuar utilizando la eliminación de la tendencia via descomposición STL, pues es aquella que presenta un mejor desempeño,también es posible eliminar la tendencia utilizando diferenciación, pero se debe tener en cuenta que la diferenciación puede eliminar la componente estacional que se tenga presente en la serie de tiempo, a continuación se muestra la manera en que se realizó la eliminación de la tendencia utilizando la diferenciación y su comparación con el método STL para eliminar tendencia utilizado en este trabajo (Figura 1.13).\n\npar(mar = c(2,2,2,2))\nfitdian = lm(ldian2~time(ldian2), na.action=NULL) \npar(mfrow=c(2,1))\nplot(ElimiTenddian_STL, type=\"l\", main=\"Sin tendencia via STL\") \nplot(diff(ldian2), type=\"l\", main=\"Primera Diferencia\") #Primera diferencia ordinaria\n\n\n\n\nFigura 1.13: Serie DIAN sin tendencia via STL vs diferenciación.\n\n\n\n\nPodemos observar que a simple vista no hay diferencias muy importantes en cada una de las graficas, puesto que ambas se centran en 0 y parecen tener un comportamiento parecido.\nEn el siguiente gráfico (Figura 1.14) se compara las funciones de autocorrelación obtenidas para la serie cuando eliminamos la tendencia utilizando descomposición STL y utilizando diferenciación, junto con la función de autocorrelación para la serie de tiempo con tendencia.\n\npar(mar = c(3,2,3,2))\npar(mfrow=c(3,1)) # plot ACFs\nacf(ldian2, 60, main=\"ACF Dian con tendencia.\")\nacf(ElimiTenddian_STL, 60, main=\"ACF Sin tendencia STL.\") \nacf(diff(ldian2), 60, main=\"ACF Primera Diferencia.\")\n\n\n\n\nFigura 1.14: Comparacion de los gráficos acf.\n\n\n\n\nEn el anterior gráfico (Figura 1.14), podemos observar que al eliminar la tendencia, el gráfico ACF baja mucho más rápido en las series de tiempo donde se eliminó la tendencia (por el método STL, como por el método de diferenciación) en comparación con la serie a la cual solo se le ajustó la varianza marginal, pero tanto en la serie diferenciada como en la serie eliminada por STL, se observa la presencia de una alta correlación en rezagos de tamaño 12, lo cual es un indicio de la presencia de una componente estacional."
  },
  {
    "objectID": "Descriptivo Dian (2).html#detección-de-estacionalidad",
    "href": "Descriptivo Dian (2).html#detección-de-estacionalidad",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "1.3 Detección de estacionalidad",
    "text": "1.3 Detección de estacionalidad\nLuego de estabilizar la varianza marginal y de tratar la tendencia, procedemos a observar si existe presencia de posibles ciclos o ciclos estacionales, para dicha tarea vamos a emplear múltiples métodos descriptivos que nos permiten obtener información sobre esta componente. Iniciaremos por observar el gráfico de retardos, el cual viene dado a continuación:\n\nts_info(ElimiTenddian_STL)\n\n The ElimiTenddian_STL series is a ts object with 1 variable and 282 observations\n Frequency: 12 \n Start time: 2000 1 \n End time: 2023 6 \n\npar(mar = c(3,2,3,2))\nastsa::lag1.plot(ElimiTenddian_STL, 12,corr=F)\n\n\n\n\nFigura 1.15: Gráfico de retardos.\n\n\n\n\n\nts_lags(ElimiTenddian_STL,lags=1:12)\n\n\n\n\nFigura 1.16: Gráfico de retardos.\n\n\n\nEl gráfico de retardos dado en Figura 1.15 y Figura 1.16, nos indican de manera descriptiva la posible relación existente entre un tiempo y algunos de sus retardos, para este caso en particular se toman 12 retardos (esto teniendo en cuenta la frecuencia mensual de la serie de tiempo), en este caso es posible observar que existe una clara relación lineal y directa con el rezago 12, los demás rezagos no parecen ser del todo significativos. Nótese que los dos gráficos anteriores nos dan una información bastante similar.\nAhora observemos el gráfico de sub series (Figura 1.17), el cual toma los valores por cada mes de cada uno de los años dentro de la serie, como sabemos, se busca observar si en el histórico encontramos diferentes valores medios, mes tras mes, de este modo tenemos lo siguiente:\n\nrequire(feasts)\ndian2_tsbl_notend=as_tsibble(ElimiTenddian_STL)\ndian2_tsbl_notend%&gt;%gg_subseries(value)\n\n\n\n\nFigura 1.17: Gráfico de subseries.\n\n\n\n\nAl analizar el gráfico de sub series (Figura 1.17) es posible observar que la media en cada uno de los meses es distinta y no oscilan al rededor de un mismo valor, tomando su valor máximo en los meses de enero, disminuyendo luego en los meses de febrero y marzo, luego vuelve a aumentar y a mantenerse estable durante los meses de abril, mayo y junio. Esto es un claro indicio de la presencia de una componente cíclica estacional o cíclica. A continuación se presentan algunas otras gráficas descriptivas para observar la presencia de un ciclo estaciona.\n\ndian2sint_df &lt;- data.frame(year = floor(time(ElimiTenddian_STL)), month = cycle(ElimiTenddian_STL),ElimiTenddian_STL = as.numeric(ElimiTenddian_STL))\ndian2sint_df$month &lt;- factor(month.abb[dian2sint_df$month], levels = month.abb)\ndian2sint_summary &lt;- dian2sint_df %&gt;%group_by(month) %&gt;%summarise(mean= mean(ElimiTenddian_STL),sd = sd(ElimiTenddian_STL))\ndian2sint_summary\n\n# A tibble: 12 × 3\n   month    mean     sd\n   &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 Jan    0.341  0.0923\n 2 Feb   -0.211  0.177 \n 3 Mar   -0.0279 0.0540\n 4 Apr    0.259  0.248 \n 5 May    0.209  0.139 \n 6 Jun    0.210  0.175 \n 7 Jul    0.0473 0.0589\n 8 Aug   -0.216  0.226 \n 9 Sep    0.254  0.123 \n10 Oct   -0.374  0.104 \n11 Nov    0.0777 0.0591\n12 Dec   -0.431  0.102 \n\nplot_ly (data = dian2sint_summary, x = ~ month, y = ~ mean, type = \"bar\", name   = \"Mean\") %&gt;%\n  layout (title = \"dian2sint - Monthly Average\", yaxis =list(title = \"Mean\",   range = c(min(dian2sint_summary$mean), max(dian2sint_summary$mean))))\n\n\n\n\nFigura 1.18: Promedio mensual.\n\n\n\nEn el anterior gráfico (Figura 1.18) se observa el valor medio tomado por cada uno de los meses, es posible observar que tiene un comportamiento parecido al gráfico de sub series y de manera análoga nos muestra que existe una componente estacional.\nA continuación se muestran los mapas de calor para la serie DIAN con varianza marginal estable y tendencia eliminada por el método de descomposición STL y el metodo de diferenciacio STL.\n\nTSstudio::ts_heatmap(ElimiTenddian_STL,title = \"Mapa de Calor - Impuestos Dian sin tendencia (STL)\")\n\n\n\n\nFigura 1.19: Graficos de calor serie DIAN sin tendencia por STLn.\n\n\n\n\nTSstudio::ts_heatmap(diff(ldian2),title = \"Mapa de Calor - Impuestos Dian sin tendencia (dif)\")\n\n\n\n\nFigura 1.20: Graficos de calor serie DIAN sin tendencia por diferenciación.\n\n\n\nAmbos mapas (Figura 1.19 y Figura 1.20) nos dan una información similar, en los cuales es posible observar que en los meses de noviembre, enero, junio, mayo y abril se tienen una mayor cantidad de recaudo de impuestos, mientras que en diciembre, octubre, agosto, marzo y febrero tienen un menor valor de recaudo de impuestos año tras año, pero se observa un valor grande en mayo del 2007, nuevamente este gráfico nos ayuda a comprender la existencia de un ciclo estacional que posiblemente tenga un periodo de 12 meses. Es importante resaltar que el tener un valor medio alto en el mes de enero puede deberse al pago del impuesto predial en los primeros meses del año.\nCuando hablamos de una componente estacional dentro de nuestra serie de tiempo, también necesitamos hablar de su periodo y de su frecuencia, para esto utilizaremos el periodograma.\n\nspectrum(as.numeric(ElimiTenddian_STL),log='no')\nabline(v=0.5, lty=2,col=\"red\")\n\n\n\n\nFigura 1.21: Peridograma serie DIAN.\n\n\n\n\n\nspectrum(as.numeric(ElimiTenddian_STL),log='no',span=5)\n\n\n\n\nFigura 1.22: Peridograma serie DIAN.\n\n\n\n\n\nspectrum(as.numeric(ElimiTenddian_STL),log='no',span=c(5,5))\n\n\n\n\nFigura 1.23: Peridograma serie DIAN.\n\n\n\n\n\nspectrum(as.numeric(ElimiTenddian_STL),log='no',span=c(2,2))\n\n\n\n\nFigura 1.24: Peridograma serie DIAN.\n\n\n\n\nAl observar los gráficos anteriores (Figura 1.21,Figura 1.22,Figura 1.23,Figura 1.24), es posible observar que se tienen diferentes valores de suavizamiento para nuestro periodograma, pues aunque en este caso no es difícil observar los puntos donde se tiene un pico, el suavizamiento puede ayudarnos a observar de modo más simple los picos que son verdaderamente significativos.\n\nPeriodgramadldian2_sintendencia=spectrum(as.numeric(ElimiTenddian_STL),log='no')\nubicacionlogdian=which.max(Periodgramadldian2_sintendencia$spec)\nsprintf(\"El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s\",Periodgramadldian2_sintendencia$freq[ubicacionlogdian])\n\n[1] \"El valor de la frecuencia donde se máximiza el periodograma para la serie es: 0.5\"\n\nsprintf(\"El periodo correspondiente es aproximadamente: %s\",1/Periodgramadldian2_sintendencia$freq[ubicacionlogdian])\n\n[1] \"El periodo correspondiente es aproximadamente: 2\"\n\n\n\n\n\nFigura 1.25: Periodograma serie DIAN.\n\n\n\n\nNótese que según la salida obtenida, la frecuencia máxima se alcanza en 0.5 (es decir, en 6/12=0.5) y se obtuvo que el periodo es 2, esto quiere decir que el ciclo se repite cada dos meses, pero se tiene que 12 es un múltiplo de 2, por lo tanto, en realidad se tiene que el periodo de la componente estacional seria de tipo anual (12 meses)"
  },
  {
    "objectID": "Descriptivo Dian (2).html#desestacionalizar-o-eliminación-de-la-componente-estacional",
    "href": "Descriptivo Dian (2).html#desestacionalizar-o-eliminación-de-la-componente-estacional",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "1.4 Desestacionalizar o eliminación de la componente estacional",
    "text": "1.4 Desestacionalizar o eliminación de la componente estacional\nCon el fin de realizar un correcto análisis de la serie de tiempo en la cual no se tenga la interferencia de la estacionalidad, sino observar los verdaderos cambios de la serie de tiempo, procedemos a realizar la estimación de la estacionalidad utilizando componentes de Fourier. En este caso se van a utilizar 3 componentes de fourier teniendo en cuenta lo observado en el análisis realizado en el notebook de phyton.\n\n# Frecuencia angular w=2*pi/s\nfrec_ang=(2*pi/12)\n\ndian_copia=ElimiTenddian_STL\n\n#Fourier k=1 \ndian_copia$sin = sin(c(1:282)*(1*frec_ang))\n\nWarning in dian_copia$sin = sin(c(1:282) * (1 * frec_ang)): Realizando coercion\nde LHD a una lista\n\ndian_copia$cos = cos(c(1:282)*(1*frec_ang))\n\n#Fourier k=2 \ndian_copia$sin2 = sin(c(1:282)*(2*frec_ang))\ndian_copia$cos2 = cos(c(1:282)*(2*frec_ang))\n\n#Fourier k=3 \ndian_copia$sin3 = sin(c(1:282)*(3*frec_ang))\ndian_copia$cos3 = cos(c(1:282)*(3*frec_ang))\n#Fourier k=4\ndian_copia$sin4 = sin(c(1:282)*(4*frec_ang))\ndian_copia$cos4 = cos(c(1:282)*(4*frec_ang))\n\nlinmodel_ciclo_dian&lt;-lm(ElimiTenddian_STL~sin+cos+sin2+cos2\n                        +sin3+cos3+sin4+cos4,data=dian_copia)\nsummary(linmodel_ciclo_dian)\n\n\nCall:\nlm(formula = ElimiTenddian_STL ~ sin + cos + sin2 + cos2 + sin3 + \n    cos3 + sin4 + cos4, data = dian_copia)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.69466 -0.19457  0.01923  0.18382  0.59214 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.012058   0.014905   0.809 0.419221    \nsin          0.080869   0.021079   3.837 0.000155 ***\ncos         -0.134775   0.021079  -6.394 6.98e-10 ***\nsin2        -0.029576   0.021079  -1.403 0.161708    \ncos2         0.027994   0.021078   1.328 0.185257    \nsin3         0.118666   0.021079   5.630 4.48e-08 ***\ncos3        -0.001016   0.021079  -0.048 0.961610    \nsin4         0.062665   0.021076   2.973 0.003209 ** \ncos4        -0.009892   0.021077  -0.469 0.639222    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2502 on 273 degrees of freedom\nMultiple R-squared:  0.2691,    Adjusted R-squared:  0.2477 \nF-statistic: 12.56 on 8 and 273 DF,  p-value: 2.388e-15\n\nresults_ciclo_dian=linmodel_ciclo_dian$fitted.values\nresults_ciclo_dian&lt;-as.data.frame(results_ciclo_dian)\nresults_ciclo_ts&lt;-ts(results_ciclo_dian,start=c(2000,01),frequency=12)\n\n\n# Series original\nplot(ElimiTenddian_STL, main=\"Serie de tiempo de recaudo de impuestos DIAN\",\n     cex.main=1.3,\n     xlab=\"Tiempo \",\n     ylab=\"Recaudo\",\n     cex.lab=0.4)\n\n# Estimación de la estacionalidad\nlines(results_ciclo_ts,col=\"red\")\n\n\n\n# Serie sin estacionalidad\nplot(ElimiTenddian_STL-results_ciclo_ts)\n\n\n\n\n\nacf(as.numeric(ElimiTenddian_STL-results_ciclo_ts))"
  },
  {
    "objectID": "Descriptivo Energia.html#gráficas-de-retardos-e-índice-ami",
    "href": "Descriptivo Energia.html#gráficas-de-retardos-e-índice-ami",
    "title": "2  Análisis del consumo de energía de la empresa PJM",
    "section": "2.3 Gráficas de retardos e índice AMI",
    "text": "2.3 Gráficas de retardos e índice AMI\n\npar(mar = c(3, 2, 3, 2))\nastsa::lag1.plot(ElimiTendenerg, 7,corr=F)\n\n\n\n\n\ntseriesChaos::mutual(ElimiTendenerg, partitions = 50, lag.max = 10, plot=TRUE) # AMI serie sin tendencia lineal\n\n\n\n\nEs posible ver que el primer rezago reduce el estado de incertidumbre para la observación en el tiempo \\(t\\)."
  },
  {
    "objectID": "Entrega 2 bonito.html",
    "href": "Entrega 2 bonito.html",
    "title": "3  Entrega 2",
    "section": "",
    "text": "4 Ajuste de modelos para serie de Energía\nComo vimos en la sección anterior. Para la serie de energía no fue necesario estabilizar la varianza, sin embargo, esta presenta tanto tendencia como multiple estacionalidad (\\(7\\) y \\(365.25\\))\nUna vez realizado el análisis descriptivo de la serie de energía, se da inicio al modelamiento de la misma. Este se hará por medio de 3 modelos:\nEstos modelos serán contrastados con base en su capacidad predictiva para seleccionar el mejor y realizar prediciones con él acerca del consumo diario de energia de la empresa PJM para fechas poseriores al año 2018.\n# Librerías necesarias\nlibrary(TSstudio)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(astsa)\nlibrary(feasts)\nlibrary(fable)\nlibrary(timetk)\nlibrary(tsibble)\nlibrary(zoo)\nlibrary(xts)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(nonlinearTseries)\nlibrary(tseriesChaos) \nlibrary(forecast)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(parsnip)\nlibrary(rsample)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(tsibble)\nlibrary(tidymodels)\nlibrary(greybox)\nlibrary(TSA)\nlibrary(urca)\nlibrary(lmtest)\nlibrary(uroot)\nlibrary(fUnitRoots)\nlibrary(sarima)\nlibrary(TSA)\nrequire(\"PolynomF\")\nrequire(\"forecast\")"
  },
  {
    "objectID": "Entrega 2 bonito.html#suavizamiento-exponencial",
    "href": "Entrega 2 bonito.html#suavizamiento-exponencial",
    "title": "3  Ajuste de modelos para serie de Energía",
    "section": "3.1 Suavizamiento Exponencial",
    "text": "3.1 Suavizamiento Exponencial\n\n3.1.0.0.1 Carga de la base de datos\n\n# Carga de la base de datos\nAEP_hourly&lt;-read.csv(\"AEP_hourly.csv\")\nAEP_hourly$Datetime&lt;-as.POSIXct(AEP_hourly$Datetime, format = \"%Y-%m-%d %H:%M:%S\")\nAEP_hourly$fecha&lt;-as.Date(AEP_hourly$Datetime)\n\nenergia &lt;- AEP_hourly %&gt;%\n  group_by(fecha) %&gt;%\n  summarise(Energia = sum(AEP_MW))\nenergia&lt;-energia[-5055,]\nenergia2&lt;-ts(energia$Energia,start=c(2004,10,01),frequency=365.25)\n\n\n# Creación del objeto msts indicando las dos estacionalidades\nly &lt;- msts(energia$Energia,start=c(2004,10,01), seasonal.periods=c(7,365.25))\n\n#HW_ly=stats::HoltWinters(ly,seasonal=\"additive\") # parece que sí funciona\n\n# Predicciones\n#forecast::forecast(HW_ly,h=7,level =0.95)\n#plot(forecast::forecast(HW_ly,h=7,level =0.95))\n\n\n\n3.1.0.1 Separación datos de entrenamiento y prueba\nSe hizo una división de los datos originales, el \\(85\\%\\) para datos de entrenamiento y el \\(15\\%\\) restante para datoos de prueba.\n\n# Separar train y test\n\nh=1 # Haremos predicciones 1-paso hacia delante\n\n# Datos entrenamiento\nntrain=trunc(length(ly)*0.85) # 4295 datos\ntrain=window(ly,end=time(ly)[ntrain])\n\n# Datos prueba\ntest=window(ly,start=time(ly)[ntrain]+1/365.25)\nntest=length(test) # 729 datos\n\n# fchstepahe: Vector para guardar las predicciones h-pasos adelante\nfchstepahe=matrix(0,nrow=ntest,ncol=h) \n\n# verval: Vector con los verdaderos valores de la serie en el conjunto de prueba con los que se compararán los pronósticos\nverval=test[1:ntest]\n\n\n\n3.1.0.2 Ajuste del modelo\nEn el modelo por medio de suavizamiento exponencial también se considera una descomposición de la serie de forma aditiva. Las componentes de tenendecia y la estacionalidad se estiman por medio de una estadística EWMA (promedio movil ponderado exponencialmente), dándole más peso a las observaciones más cercanas en cada tiempo.\nAdemás, para este caso, se descompone la componente de tendencia en nivel y pendiente, y se estima un parámetro de la componente estacional. Las estimaciones se hallan de la siguiente manera:\n\\[\\begin{align*}\n\\text{Componente de nivel: } & a_t=α(x_t-S_{t-p})+(1−α)(a_{t−1}+b_{t−1})\\\\\n\\text{Componente de pendiente: } & b_t=β(a_t−a_{t−1})+(1−β)b_{t−1} \\\\\n\\text{Componente estacional: } & S_t=\\gamma(x_t−a_t)+(1−γ)S_{t−p} \\\\\n\\end{align*}\\]\nPara encontrar los parámetros de suavizamiento \\(\\alpha\\), \\(\\beta\\) y \\(\\gamma\\) usamos una grilla con valores desde \\(0.001\\) hasta \\(0.999\\) de a \\(0.1\\) para cada parámetro.\n\nrequire(utils)\n\n# Propuestas para cada parámetro\nsuav_inputs=cbind(seq(0.001,0.999,0.1),seq(0.001,0.999,0.1),seq(0.001,0.999,0.1))\ncolnames(suav_inputs)&lt;-c(\"alpha\",\"beta\",\"gamma\")\nsuav_inputs_tbl=tibble::as_tibble(suav_inputs)\n\n# Creación de la grilla\ngrilla_suav=expand.grid(alpha=suav_inputs_tbl$alpha,beta=suav_inputs_tbl$beta,gamma=suav_inputs_tbl$gamma)\n\n# Matriz para almacenar los errores\nerrores&lt;-matrix(NA, nrow=1000,ncol=3) \n\n# Búsqueda de alpha, beta y gamma con rolling, queremos que minimice ECM\nfor(i in 1:1000){\n  ourCallETS &lt;- \"forecast::forecast(stats::HoltWinters(x=data,alpha=grilla_suav[i,1],beta=grilla_suav[i,2],gamma=grilla_suav[i,3]),h=h,level=95)\"\n  ourValueETS &lt;- c(\"mean\",\"lower\",\"upper\")\n  origins=ntest   # Número de rolling windows\n  Valoresretornados1 &lt;- ro(ly, h=h, origins=origins, call=ourCallETS, value=ourValueETS,ci=FALSE,co=FALSE)\n  t(Valoresretornados1$holdout) # Permiten verificar los verdaderos valores h-pasos adelante. \n  t(Valoresretornados1$mean)\n  errores[i,]&lt;-sqrt(apply((Valoresretornados1$holdout -Valoresretornados1$mean)^2,1,mean,na.rm=TRUE)) # Se calcula la raíz del error cuadrático medio de predicción\n}\n\n# Error medio absoluto escalado\nerrores&lt;-na.omit(errores) \nmin(errores[,1]) # RECM= 47201.82\nwhich(errores[,1] == min(errores[,1])) # 885\ngrilla_suav[885,] # alpha=0.701, beta=0.701, gamma=0.501\n\n\n\n[1] 47201.82\n\n\nDe modo que el modelo final para suavizamiento exponencial es\n\\[\\begin{align*}\n\\text{Componente de nivel: } & a_t=0.701(x_t-S_{t-p})+(1−0.701)(a_{t−1}+b_{t−1})\\\\\n\\text{Componente de pendiente: } & b_t=0.701(a_t−a_{t−1})+(1−0.701)b_{t−1} \\\\\n\\text{Componente estacional: } & S_t=0.7501(x_t−a_t)+(1−0.501)S_{t−p} \\\\\n\\end{align*}\\]\n\n\n3.1.0.3 Evaluar los supuestos\nAunque en suavizamiento exponencial no se hacen supuestos sobre los residuales, aún así hicimos las pruebas para ver si los residuales tenían un comportamiento similar a ruido blanco.\n\n# Verificación de supuestos\nHW_train_grilla=stats::HoltWinters(train,seasonal=\"additive\",alpha=0.701,beta=0.701,gamma=0.501) # con los parámetros que dieron mejor en la grilla\n\n# Residuales\nres &lt;- ly-HW_train_grilla$fitted[,1]\nplot(res)\n\n\n\n\n\n3.1.0.3.0.1 No autocorrelación\nLuego de ser modelados con el suavizamiento exponencial, parece que aún queda correlación por explicar\n\n# ACF\nacf(as.numeric(res)) # No deberían estar fuera de las bandas\n\n\n\n#acf(res^2)\n\n\n\n3.1.0.3.0.2 No autocorrelación parcial\n\n# PACF\npacf(as.numeric(res)) # No deberían estar fuera de las bandas\n\n\n\n\n\n\n3.1.0.3.0.3 Test de normalidad\nParece que no hay normalidad en los residuales\n\n# Test de normalidad\n## NO queremos rechazar H0 pero pues no es tan grave\ntseries::jarque.bera.test(res) # Dice que no son normales\n\n\n    Jarque Bera Test\n\ndata:  res\nX-squared = 49.017, df = 2, p-value = 2.27e-11\n\n\n\n\n3.1.0.3.0.4 Test de autocorrelación\nLuego de ser modelados con el suavizamiento exponencial, parece que los residuales están correlacionados\n\n# Test de autocorrelacion \n## No quieremos rechazar H0\nBox.test(res, lag =20 , type = \"Ljung-Box\", fitdf = 2) # No puedo Rechazar la hipótesis de no autocorrelación!\n\n\n    Box-Ljung test\n\ndata:  res\nX-squared = 6590.4, df = 18, p-value &lt; 2.2e-16\n\n\n\n\n3.1.0.3.0.5 Estabilización de la varianza\nCREO QUE ESTO ES MEJOR NO PONERLO PQ NO SÉ SI SIRVA PARA SUAVIZAMIENTO\n\n# Estadisticas CUSUM\n## Mide la estabilidad en los parámetros del modelo\ncum=cumsum(res)/sd(res)\nN=length(res)\ncumq=cumsum(res^2)/sum(res^2)\nAf=0.948 ###Cuantil del 95% para la estad?stica cusum\nco=0.01717####Valor del cuantil aproximado para cusumsq para n/2\nLS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)\nLI=-LS\nLQS=co+(1:length(res))/N\nLQI=-co+(1:length(res))/N\nplot(cum,type=\"l\",ylim=c(min(LI),max(LS)),xlab=\"t\",ylab=\"\",main=\"CUSUM\")\nlines(LS,type=\"S\",col=\"red\")\nlines(LI,type=\"S\",col=\"red\")\n\n\n\n# Estadísticas CUSUMSQ\n## Mide la estabilidad en la varianza del modelo\nplot(cumq,type=\"l\",xlab=\"t\",ylab=\"\",main=\"CUSUMSQ\")                      \nlines(LQS,type=\"S\",col=\"red\")                                           \nlines(LQI,type=\"S\",col=\"red\")\n\n\n\n\n\n\n\n3.1.0.4 Predicciones sobre datos de prueba\n\nverval_ts&lt;-ts(verval,start=time(ly)[ntrain]+1/365.25,frequency=365.25)\nfchstepahe_ts&lt;-ts(fchstepahe,start=time(ly)[ntrain]+1/365.25,frequency=365.25)\n\nplot(verval_ts, col = \"blue\", ylab = \"Energía\", xlab = \"Tiempo\")\nlines(fchstepahe_ts, col = \"red\")\nlegend(\"topright\", legend = c(\"Reales\", \"Predicciones\"), col = c(\"blue\", \"red\"), lty = 1)"
  },
  {
    "objectID": "Entrega 2 bonito.html#proceso-arma",
    "href": "Entrega 2 bonito.html#proceso-arma",
    "title": "3  Ajuste de modelos para serie de Energía",
    "section": "3.2 Proceso ARMA",
    "text": "3.2 Proceso ARMA\n\n3.2.0.1 Carga de la base de datos\nPara ajustar un modelo de la familia ARMA(p,q) es necesario que los datos sean estacionarios, por lo tanto, usamos los datos que resultan luego de eliminar la tendencia línealmente y eliminar la doble estacionalidad que fue modelada usando 6 componentes de Fourier (3 para \\(s=7\\) y 3 para \\(s=182\\))\n\n# Datos estacionales\ny&lt;-readRDS(\"energia_estacionarios.RDS\") # datos estacionarios\nplot(y)\n\n\n\nplot(y,xlim=c(2004,2006))\n\n\n\n\nComprobamos que son estacionarios observando la subserie semanal y la subserie mensual\n\n# Preliminares\nest_1&lt;-cbind(as.matrix(y),as.character(energia$fecha))\nest_1&lt;-as.data.frame(est_1)\nnames(est_1)&lt;-c(\"Energia\",\"fecha\")\n\nest_1$Energia&lt;-as.numeric(est_1$Energia)\nest_1$fecha&lt;-as.Date(est_1$fecha)\n\ndf_est=data.frame(Energia=est_1$Energia,fecha=est_1$fecha)\ntbl_est=tibble(df_est)\ntbl_est_format_fecha=tbl_est\ntsbl_est=as_tsibble(tbl_est_format_fecha,index=fecha)\n\n# Subserie semanal\ngg_subseries(tsbl_est,y=Energia,period=7)\n\n\n\n# Subserie mensual\ngg_subseries(tsbl_est,y=Energia,period=12)\n\n\n\n\n\n\n3.2.0.2 Búsqueda de los hiperparámetros p y q\nLa búsqueda de los hiperparámetros p y q se hace vía ACF y PACF\n\n# Búsqueda de p,q vía acf y pacf\n\n# Búsqueda de q\nacf(as.numeric(y)) # Parece que q es gigante\n\n\n\n#acf(as.numeric(y),ci.type='ma') # En efecto, q es grande\n\n# Búsqueda de p\npacf(as.numeric(y)) # p máximo 3, posiblemente 5 o 6\n\n\n\n\nLuego de observar los gráficos, vemos que es posible que \\(p=6\\) o menos, y \\(q\\) debe ser grandísimo, por razones prácticas postulamos inicialmente \\(q=20\\), es razonable postular un modelo AR(6) y refinarlo, así como también un modelo mixto ARMA(6,20) y refinarlo. No es nada razonable pensar en un \\(MA(q)\\) puro por lo que vemos que el ACF decae excesivamente lento.\n\n\n3.2.0.3 Ajuste del modelo ARMA\nInicialmente se ajustó un modelo AR(6) y se refinó, sin embargo no se encontró un modelo autoregresivo puro que cumpliera los supuestos. Luego de una ardua búsqueda, finalmente encontramos un modelo mixto que cumpliera los supuestos, este es ARMA(5,8) que también fue refinado, de modo que el modelo final es:\n\\[X_t=\\phi_1 X_{t-1}+\\phi_2 X_{t-2}+\\phi_3 X_{t-3}+Z_t+\\theta_1 Z_{t-1}+\\theta_2 Z_{t-2}+\\theta_3 Z_{t-3}+\\theta_4 Z_{t-4}+\\theta_6 Z_{t-6}\\]\n\n# Propuesta modelo ARMA\nmodelo.propuesto2=forecast::Arima(y,order=c(5,0,8),fixed=c(NA,NA,0,NA,0,NA,NA,NA,NA,0,NA,0,0,0)) # ARMA(5,8)\nlmtest::coeftest(modelo.propuesto2)\n\n\nz test of coefficients:\n\n     Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1  0.278287   0.116425   2.3903   0.01684 *  \nar2  1.017041   0.143894   7.0680 1.572e-12 ***\nar4 -0.312920   0.034978  -8.9463 &lt; 2.2e-16 ***\nma1  0.819146   0.116858   7.0098 2.387e-12 ***\nma2 -0.567652   0.028526 -19.8992 &lt; 2.2e-16 ***\nma3 -0.828137   0.071508 -11.5811 &lt; 2.2e-16 ***\nma4 -0.244673   0.027497  -8.8982 &lt; 2.2e-16 ***\nma6  0.026696   0.011651   2.2913   0.02194 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.2.0.4 Evaluar los supuestos\nPara los modelos de la familia ARMA sí se hacen supuestos sobre los residuales que deben comportarse como ruido blanco. Por lo tanto, es necesario validar los supuestos\n\n# Verificación de supuestos ARMA\nres &lt;- modelo.propuesto2$residuals\nplot(res)\n\n\n\n\n\n3.2.0.4.0.1 No autocorrelación\nEn general, los residuales presentan un buen comportamiento. No qued nada por explicar que no haya explicado ya el modelo.\n\n# ACF\nacf(as.numeric(res)) # No deberían estar fuera de las bandas\n\n\n\n# acf(as.numeric(res^2))\n\n\n\n3.2.0.4.0.2 No autocorrelación parcial\nEn general, los residuales presentan un buen comportamiento. No qued nada por explicar que no haya explicado ya el modelo.\n\n# PACF\npacf(as.numeric(res)) # No deberían estar fuera de las bandas\n\n\n\n\n\n\n3.2.0.4.1 Test de normalidad\nParece ser que los datos NO son normales.\n\n#Test de normalidad \n## No queremos rechazar H0 pero pues no es tan grave\ntseries::jarque.bera.test(res)\n\n\n    Jarque Bera Test\n\ndata:  res\nX-squared = 2096.9, df = 2, p-value &lt; 2.2e-16\n\n\n\n\n3.2.0.4.2 Test de autocorrelación\nCon un \\(p-value=\\) no hay suficiente evidencia estadística para rechazar la hipóstesis nula, es decir, los residuales NO están correlacionados.\n\n#Test de autocorrelacion \n## No queremos rechazar H0 pq es la hipótesis de no autocorrelación\nBox.test(res, lag =20 , type = \"Ljung-Box\", fitdf = 2)\n\n\n    Box-Ljung test\n\ndata:  res\nX-squared = 19.468, df = 18, p-value = 0.3636\n\n\n\n\n3.2.0.4.3 Estabilización de la varianza\nParece que tanto los parámetros como la varianza están “estables”.\n\n# Estadisticas CUSUM\n## Mide la estabilidad en los parámetros del modelo\ncum=cumsum(res)/sd(res)\nN=length(res)\ncumq=cumsum(res^2)/sum(res^2)\nAf=0.948 ###Cuantil del 95% para la estad?stica cusum\nco=0.01717####Valor del cuantil aproximado para cusumsq para n/2\nLS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)\nLI=-LS\nLQS=co+(1:length(res))/N\nLQI=-co+(1:length(res))/N\nplot(cum,type=\"l\",ylim=c(min(LI),max(LS)),xlab=\"t\",ylab=\"\",main=\"CUSUM\")\nlines(LS,type=\"S\",col=\"red\")\nlines(LI,type=\"S\",col=\"red\")\n\n\n\n# Estadísticas CUSUMSQ\n## Mide la estabilidad en la varianza del modelo\nplot(cumq,type=\"l\",xlab=\"t\",ylab=\"\",main=\"CUSUMSQ\")                      \nlines(LQS,type=\"S\",col=\"red\")                                           \nlines(LQI,type=\"S\",col=\"red\")\n\n\n\n\n\n\n3.2.0.4.4 Rolling\nUna vez verificados los supuestos, se procede a evaluar la capacidad predictiva del modelo mixto. Para ello utilizamos rolling.\n\n# Rolling corregido\nfcmat=matrix(0,nrow=ntest,ncol=h)\nfor(i in 1:ntest){\n  x=window(y,end=time(ly)[ntrain]+(i-1)/365.25)\n  refit=Arima(x,model=modelo.propuesto2)\n  fcmat[i,]=as.numeric(forecast::forecast(refit,h=h)$mean) # Pronósticos para datos estacionarios\n}\n\n\n# Para volver a la escala original\nestacionalidad&lt;-as.vector(results_ciclo_ts)\ntendencia&lt;-as.vector(predict(fit_e))\nfchstepahe&lt;-(fcmat+estacionalidad[4296:5054])+tendencia[4296:5054] # primero sumamos la estacionalidad y luego la tendencia\n\nerrores_pred = verval-fchstepahe \nECM=apply(errores_pred^2,MARGIN = 2,mean,na.rm=TRUE)\nRECM=sqrt(ECM)\nRECM # 22840.1\n\n[1] 22840.1\n\n\n\n\n\n3.2.0.5 Predicciones sobre datos de prueba\n\nfchstepahe_ts&lt;-ts(fchstepahe,start=time(ly)[ntrain]+1/365.25,frequency=365.25)\n\nplot(verval_ts, col = \"blue\", ylab = \"Energía\", xlab = \"Tiempo\")\nlines(fchstepahe_ts, col = \"red\")\nlegend(\"topright\", legend = c(\"Reales\", \"Predicciones\"), col = c(\"blue\", \"red\"), lty = 1)"
  },
  {
    "objectID": "Entrega 2 bonito.html#proceso-arima",
    "href": "Entrega 2 bonito.html#proceso-arima",
    "title": "3  Ajuste de modelos para serie de Energía",
    "section": "3.3 Proceso ARIMA",
    "text": "3.3 Proceso ARIMA\nPara ajustar un modelo ARIMA (aunque no es lo más adecuado dado que la serie presenta componente estacional), primero se debe comprobar si la serie de tiempo presenta una raíz unitaria. Teniendo en cuenta que, para la prueba de Dicker Fuller, si el \\(p-valor\\) es menor que un nivel de significancia \\(\\alpha\\) se rechaza la hipotesis nula de que la serie de tiempo presenta una raíz unitaria, se tiene lo siguiente.\n\nstats::ar(energia2) # Selecciona un modelo AR usando el criterio de Akaike, sugiere tomar lags=36\n\n\nCall:\nstats::ar(x = energia2)\n\nCoefficients:\n      1        2        3        4        5        6        7        8  \n 1.0967  -0.4319   0.1631  -0.0202   0.0112   0.0217   0.1385  -0.1334  \n      9       10       11       12       13       14       15       16  \n 0.0302   0.0394  -0.0436  -0.0045   0.0394   0.1196  -0.1604   0.0593  \n     17       18       19       20       21       22       23       24  \n 0.0172  -0.0279   0.0320  -0.0221   0.1656  -0.1895   0.0736  -0.0536  \n     25       26       27       28       29       30       31       32  \n 0.0436  -0.0453   0.0435   0.1166  -0.1403   0.0378  -0.0228  -0.0066  \n     33       34       35       36  \n-0.0332   0.0232   0.1493  -0.1453  \n\nOrder selected 36  sigma^2 estimated as  329070266\n\ntseries::adf.test(energia2,k=36) # Prueba Dicker Fuller: No hay raíz unitaria\n\nWarning in tseries::adf.test(energia2, k = 36): p-value smaller than printed\np-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  energia2\nDickey-Fuller = -8.1586, Lag order = 36, p-value = 0.01\nalternative hypothesis: stationary\n\nsummary(ur.df(energia2,type=\"trend\",lags = 36))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-144842  -10683    -880   10058   98316 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.648e+04  3.278e+03   8.079 8.14e-16 ***\nz.lag.1      -6.722e-02  8.239e-03  -8.159 4.25e-16 ***\ntt           -5.746e-01  1.879e-01  -3.058 0.002240 ** \nz.diff.lag1   1.757e-01  1.554e-02  11.307  &lt; 2e-16 ***\nz.diff.lag2  -2.754e-01  1.569e-02 -17.549  &lt; 2e-16 ***\nz.diff.lag3  -9.995e-02  1.614e-02  -6.193 6.37e-10 ***\nz.diff.lag4  -1.297e-01  1.614e-02  -8.032 1.18e-15 ***\nz.diff.lag5  -1.096e-01  1.623e-02  -6.755 1.59e-11 ***\nz.diff.lag6  -9.557e-02  1.628e-02  -5.869 4.66e-09 ***\nz.diff.lag7   4.296e-02  1.633e-02   2.630 0.008558 ** \nz.diff.lag8  -8.916e-02  1.632e-02  -5.463 4.90e-08 ***\nz.diff.lag9  -5.914e-02  1.633e-02  -3.621 0.000297 ***\nz.diff.lag10 -1.984e-02  1.634e-02  -1.214 0.224918    \nz.diff.lag11 -6.555e-02  1.630e-02  -4.022 5.86e-05 ***\nz.diff.lag12 -6.658e-02  1.631e-02  -4.081 4.55e-05 ***\nz.diff.lag13 -2.957e-02  1.630e-02  -1.814 0.069749 .  \nz.diff.lag14  8.892e-02  1.630e-02   5.455 5.13e-08 ***\nz.diff.lag15 -7.011e-02  1.628e-02  -4.306 1.69e-05 ***\nz.diff.lag16 -1.172e-02  1.627e-02  -0.720 0.471424    \nz.diff.lag17  8.099e-03  1.623e-02   0.499 0.617737    \nz.diff.lag18 -2.567e-02  1.621e-02  -1.584 0.113336    \nz.diff.lag19  1.229e-02  1.616e-02   0.760 0.447063    \nz.diff.lag20 -1.552e-02  1.614e-02  -0.961 0.336397    \nz.diff.lag21  1.543e-01  1.610e-02   9.584  &lt; 2e-16 ***\nz.diff.lag22 -4.081e-02  1.614e-02  -2.529 0.011471 *  \nz.diff.lag23  3.898e-02  1.614e-02   2.415 0.015773 *  \nz.diff.lag24 -1.987e-02  1.609e-02  -1.235 0.217069    \nz.diff.lag25  2.709e-02  1.598e-02   1.695 0.090117 .  \nz.diff.lag26 -2.176e-02  1.588e-02  -1.371 0.170571    \nz.diff.lag27  2.413e-02  1.584e-02   1.524 0.127655    \nz.diff.lag28  1.405e-01  1.574e-02   8.923  &lt; 2e-16 ***\nz.diff.lag29 -5.265e-04  1.572e-02  -0.034 0.973277    \nz.diff.lag30  3.564e-02  1.571e-02   2.268 0.023358 *  \nz.diff.lag31  1.438e-02  1.555e-02   0.925 0.355066    \nz.diff.lag32  6.620e-03  1.535e-02   0.431 0.666276    \nz.diff.lag33 -2.859e-02  1.510e-02  -1.894 0.058303 .  \nz.diff.lag34 -4.864e-03  1.491e-02  -0.326 0.744260    \nz.diff.lag35  1.495e-01  1.411e-02  10.598  &lt; 2e-16 ***\nz.diff.lag36 -8.638e-03  1.415e-02  -0.610 0.541638    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17980 on 4978 degrees of freedom\nMultiple R-squared:  0.5122,    Adjusted R-squared:  0.5084 \nF-statistic: 137.5 on 38 and 4978 DF,  p-value: &lt; 2.2e-16\n\n\nValue of test-statistic is: -8.1586 22.1938 33.2897 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.96 -3.41 -3.12\nphi2  6.09  4.68  4.03\nphi3  8.27  6.25  5.34\n\n\nComo se puede ver, en ambos casos las pruebas muestran que la serie no presenta raíces unitarias, por lo que ajustar este modelo no es adecuado."
  },
  {
    "objectID": "Entrega 2 bonito.html#proceso-sarima",
    "href": "Entrega 2 bonito.html#proceso-sarima",
    "title": "3  Ajuste de modelos para serie de Energía",
    "section": "3.4 Proceso SARIMA",
    "text": "3.4 Proceso SARIMA\nDado que la serie presenta multiple estacionalidad, no es posible ajustar un modelo de la familia SARIMA."
  },
  {
    "objectID": "Descriptivo Dian (2).html",
    "href": "Descriptivo Dian (2).html",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "",
    "text": "2 Suavizamiento exponencial.\nEn el modelo por medio de suavizamiento exponencial también se considera una descomposición de la serie de forma aditiva. Las componentes de tenendecia y la estacionalidad se estiman por medio de una estadística EWMA (promedio movil ponderado exponencialmente), dándole más peso a las observaciones más cercanas en cada tiempo.\nAdemás, para este caso, se descompone la componente de tendencia en nivel y pendiente, y se estima un parámetro de la componente estacional por mes, como lo indica el periodo hallado de 12. Las estimaciones se hallan de la siguiente manera:\n\\[\\begin{align*}\n\\text{Componente de nivel: } & a_t=α(x_t-S_{t-p})+(1−α)(a_{t−1}+b_{t−1})\\\\\n\\text{Componente de pendiente: } & b_t=β(a_t−a_{t−1})+(1−β)b_{t−1} \\\\\n\\text{Componente estacional: } & S_t=\\gamma(x_t−a_t)+(1−γ)S_{t−p} \\\\\n\\end{align*}\\]\nPreparando los datos y tambien deividiendo el conjunto de datos para posteriormente Observar cual es el mejor modelo segun su capacidad predictiva.\nEn el siguiente aparatado se busca ajustar los modelos concernientes a la familia ARMA, esto quiere decir que se va a intentar realizar un modelamiento para un modelo AR, MA y ARMA, posteriormente se observara cual de los modelos ajustados es mejor en terminos de su capacidad predictiva comparando sus ECM.\nTeniendo en cuenta que para realizar el modelamiento utilizando ARMA es necesario que la serie de tiempo sea estacionaria, se utilizara la serie sin tendencia por medio de descomposición STL y se realizará el modelamiento simultaneamente eliminando la estacionalidad por medio de variables dummy y de componentes de Fourier.\nAntes de proceder con algun modelo ARIMA o SARIMA, realizaremos la prueba de raices unitarias de Dickey Fuller.\n##Aplicando la prueba de Dickey Fuller sobre la serie con varianza  estabilizada\npacf(ldian2)## SErie dian en escala logaritmica (Boxcox)\n\n\n\nar(ldian2)\n\n\nCall:\nar(x = ldian2)\n\nCoefficients:\n      1        2        3        4        5        6        7        8  \n 0.4535   0.4862  -0.0982   0.0570   0.0293  -0.1282   0.0188   0.1188  \n      9       10       11       12       13       14       15  \n-0.0002  -0.0061  -0.1034   0.6274  -0.3556  -0.2355   0.1044  \n\nOrder selected 15  sigma^2 estimated as  0.06139\n\nresultadodf_1&lt;-adf.test(ldian2,k=15)\nresultadodf_1\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ldian2\nDickey-Fuller = -2.1889, Lag order = 15, p-value = 0.4965\nalternative hypothesis: stationary\n\nsummary(resultadodf_1)\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \nalternative 1      -none- character\np.value     1      -none- numeric  \nmethod      1      -none- character\ndata.name   1      -none- character\nEn este caso se obtuvo un p-valor de 0.4965, el cual es mayor que un valor \\(\\alpha=0.05\\), lo que nos indica no se tiene evidencia estadistica suficiente para rechazar la hipótesis nula de que la serie temporal tiene una raíz unitaria. Ahora procederemos a realizar la diferenciación ordinaria y luego se analizará si es necesario realizar una nueva diferenciación.\n#Diferenciando la serie de tiempo \ndldian2=diff(ldian2)\nplot(dldian2)\npacf(as.numeric(dldian2),lag.max=length(dldian2)/4)\n\n\n\nacf(as.numeric(dldian2),lag.max = length(dldian2)/4)\n\n\n\nar(dldian2)\n\n\nCall:\nar(x = dldian2)\n\nCoefficients:\n      1        2        3        4        5        6        7        8  \n-0.9227  -0.4884  -0.4269  -0.3649  -0.3700  -0.4674  -0.4697  -0.4029  \n      9       10       11       12       13  \n-0.3865  -0.3352  -0.3523   0.3997   0.3078  \n\nOrder selected 13  sigma^2 estimated as  0.02836\n\nadf.test(dldian2,k=13)\n\nWarning in adf.test(dldian2, k = 13): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dldian2\nDickey-Fuller = -6.0912, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\nsummary(ur.df(dldian2,type=\"trend\",lags = 13))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82132 -0.06071  0.00072  0.05370  0.62443 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.0754375  0.0245018   3.079 0.002309 ** \nz.lag.1      -6.5128269  1.0692177  -6.091 4.18e-09 ***\ntt           -0.0000857  0.0001238  -0.692 0.489328    \nz.diff.lag1   4.5409596  1.0437749   4.351 1.98e-05 ***\nz.diff.lag2   3.8961935  0.9887973   3.940 0.000106 ***\nz.diff.lag3   3.3603737  0.9054217   3.711 0.000254 ***\nz.diff.lag4   2.8633275  0.8195086   3.494 0.000562 ***\nz.diff.lag5   2.3745493  0.7321815   3.243 0.001343 ** \nz.diff.lag6   1.8058055  0.6444828   2.802 0.005476 ** \nz.diff.lag7   1.2235736  0.5555077   2.203 0.028531 *  \nz.diff.lag8   0.7043465  0.4681285   1.505 0.133684    \nz.diff.lag9   0.2005785  0.3836188   0.523 0.601534    \nz.diff.lag10 -0.2615684  0.2993684  -0.874 0.383098    \nz.diff.lag11 -0.7261802  0.2171391  -3.344 0.000951 ***\nz.diff.lag12 -0.3736073  0.1417028  -2.637 0.008897 ** \nz.diff.lag13 -0.0720133  0.0648302  -1.111 0.267718    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1522 on 251 degrees of freedom\nMultiple R-squared:  0.9738,    Adjusted R-squared:  0.9722 \nF-statistic: 620.7 on 15 and 251 DF,  p-value: &lt; 2.2e-16\n\n\nValue of test-statistic is: -6.0912 12.5025 18.7489 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.98 -3.42 -3.13\nphi2  6.15  4.71  4.05\nphi3  8.34  6.30  5.36\n\nmonthplot(dldian2)\n\n\n\nspectrum(dldian2)\nAl utilizar ambos paquetes en este caso, se tiene que en ambas situaciones se obtiene un p-valor pequeño, el cual es menor que \\(\\alpha=0.05\\), lo que quiere decir que hay evidencia estadística para afirmar que la serie temporal no tiene una raíz unitaria, por lo tanto no sería necesario realizar más diferenciaciones ordinarias.\nresiO= residuals(modelo_SARIMA_ref)\ncoefO= coefs2poly(modelo_SARIMA_ref)\noutliersSARIMA= locate.outliers(resiO,coefO,cval=4.5)\noutliersSARIMA\n\n   type ind     coefhat      tstat\n1    AO  50 -0.01728273  -4.608871\n2    AO  52  0.01983844   5.290414\n3    AO  76  0.02344045   6.250979\n4    AO  88 -0.04910246 -13.094393\n5    AO  89  0.03305053   8.813746\n6    AO 126 -0.02203517  -5.876224\n9    TC  26 -0.01407100  -4.540132\n10   TC  87 -0.01604647  -5.177532\n13   TC 123 -0.01402550  -4.525432\n14   TC 124 -0.02073909  -6.691616\n15   TC 125 -0.01667692  -5.380926\n\nn=length(df_train4)\nxregSARIMA = outliers.effects(outliersSARIMA,n )\nxregSARIMA\n\n       AO50 AO52 AO76 AO88 AO89 AO126         TC26         TC87        TC123\n  [1,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [2,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [3,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [4,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [5,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [6,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [7,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [8,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [9,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [10,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [11,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [12,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [13,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [14,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [15,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [16,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [17,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [18,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [19,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [20,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [21,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [22,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [23,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [24,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [25,]    0    0    0    0    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [26,]    0    0    0    0    0     0 1.000000e+00 0.000000e+00 0.000000e+00\n [27,]    0    0    0    0    0     0 7.000000e-01 0.000000e+00 0.000000e+00\n [28,]    0    0    0    0    0     0 4.900000e-01 0.000000e+00 0.000000e+00\n [29,]    0    0    0    0    0     0 3.430000e-01 0.000000e+00 0.000000e+00\n [30,]    0    0    0    0    0     0 2.401000e-01 0.000000e+00 0.000000e+00\n [31,]    0    0    0    0    0     0 1.680700e-01 0.000000e+00 0.000000e+00\n [32,]    0    0    0    0    0     0 1.176490e-01 0.000000e+00 0.000000e+00\n [33,]    0    0    0    0    0     0 8.235430e-02 0.000000e+00 0.000000e+00\n [34,]    0    0    0    0    0     0 5.764801e-02 0.000000e+00 0.000000e+00\n [35,]    0    0    0    0    0     0 4.035361e-02 0.000000e+00 0.000000e+00\n [36,]    0    0    0    0    0     0 2.824752e-02 0.000000e+00 0.000000e+00\n [37,]    0    0    0    0    0     0 1.977327e-02 0.000000e+00 0.000000e+00\n [38,]    0    0    0    0    0     0 1.384129e-02 0.000000e+00 0.000000e+00\n [39,]    0    0    0    0    0     0 9.688901e-03 0.000000e+00 0.000000e+00\n [40,]    0    0    0    0    0     0 6.782231e-03 0.000000e+00 0.000000e+00\n [41,]    0    0    0    0    0     0 4.747562e-03 0.000000e+00 0.000000e+00\n [42,]    0    0    0    0    0     0 3.323293e-03 0.000000e+00 0.000000e+00\n [43,]    0    0    0    0    0     0 2.326305e-03 0.000000e+00 0.000000e+00\n [44,]    0    0    0    0    0     0 1.628414e-03 0.000000e+00 0.000000e+00\n [45,]    0    0    0    0    0     0 1.139890e-03 0.000000e+00 0.000000e+00\n [46,]    0    0    0    0    0     0 7.979227e-04 0.000000e+00 0.000000e+00\n [47,]    0    0    0    0    0     0 5.585459e-04 0.000000e+00 0.000000e+00\n [48,]    0    0    0    0    0     0 3.909821e-04 0.000000e+00 0.000000e+00\n [49,]    0    0    0    0    0     0 2.736875e-04 0.000000e+00 0.000000e+00\n [50,]    1    0    0    0    0     0 1.915812e-04 0.000000e+00 0.000000e+00\n [51,]    0    0    0    0    0     0 1.341069e-04 0.000000e+00 0.000000e+00\n [52,]    0    1    0    0    0     0 9.387480e-05 0.000000e+00 0.000000e+00\n [53,]    0    0    0    0    0     0 6.571236e-05 0.000000e+00 0.000000e+00\n [54,]    0    0    0    0    0     0 4.599865e-05 0.000000e+00 0.000000e+00\n [55,]    0    0    0    0    0     0 3.219906e-05 0.000000e+00 0.000000e+00\n [56,]    0    0    0    0    0     0 2.253934e-05 0.000000e+00 0.000000e+00\n [57,]    0    0    0    0    0     0 1.577754e-05 0.000000e+00 0.000000e+00\n [58,]    0    0    0    0    0     0 1.104428e-05 0.000000e+00 0.000000e+00\n [59,]    0    0    0    0    0     0 7.730994e-06 0.000000e+00 0.000000e+00\n [60,]    0    0    0    0    0     0 5.411696e-06 0.000000e+00 0.000000e+00\n [61,]    0    0    0    0    0     0 3.788187e-06 0.000000e+00 0.000000e+00\n [62,]    0    0    0    0    0     0 2.651731e-06 0.000000e+00 0.000000e+00\n [63,]    0    0    0    0    0     0 1.856212e-06 0.000000e+00 0.000000e+00\n [64,]    0    0    0    0    0     0 1.299348e-06 0.000000e+00 0.000000e+00\n [65,]    0    0    0    0    0     0 9.095437e-07 0.000000e+00 0.000000e+00\n [66,]    0    0    0    0    0     0 6.366806e-07 0.000000e+00 0.000000e+00\n [67,]    0    0    0    0    0     0 4.456764e-07 0.000000e+00 0.000000e+00\n [68,]    0    0    0    0    0     0 3.119735e-07 0.000000e+00 0.000000e+00\n [69,]    0    0    0    0    0     0 2.183814e-07 0.000000e+00 0.000000e+00\n [70,]    0    0    0    0    0     0 1.528670e-07 0.000000e+00 0.000000e+00\n [71,]    0    0    0    0    0     0 1.070069e-07 0.000000e+00 0.000000e+00\n [72,]    0    0    0    0    0     0 7.490483e-08 0.000000e+00 0.000000e+00\n [73,]    0    0    0    0    0     0 5.243338e-08 0.000000e+00 0.000000e+00\n [74,]    0    0    0    0    0     0 3.670337e-08 0.000000e+00 0.000000e+00\n [75,]    0    0    0    0    0     0 2.569236e-08 0.000000e+00 0.000000e+00\n [76,]    0    0    1    0    0     0 1.798465e-08 0.000000e+00 0.000000e+00\n [77,]    0    0    0    0    0     0 1.258926e-08 0.000000e+00 0.000000e+00\n [78,]    0    0    0    0    0     0 8.812479e-09 0.000000e+00 0.000000e+00\n [79,]    0    0    0    0    0     0 6.168735e-09 0.000000e+00 0.000000e+00\n [80,]    0    0    0    0    0     0 4.318115e-09 0.000000e+00 0.000000e+00\n [81,]    0    0    0    0    0     0 3.022680e-09 0.000000e+00 0.000000e+00\n [82,]    0    0    0    0    0     0 2.115876e-09 0.000000e+00 0.000000e+00\n [83,]    0    0    0    0    0     0 1.481113e-09 0.000000e+00 0.000000e+00\n [84,]    0    0    0    0    0     0 1.036779e-09 0.000000e+00 0.000000e+00\n [85,]    0    0    0    0    0     0 7.257455e-10 0.000000e+00 0.000000e+00\n [86,]    0    0    0    0    0     0 5.080219e-10 0.000000e+00 0.000000e+00\n [87,]    0    0    0    0    0     0 3.556153e-10 1.000000e+00 0.000000e+00\n [88,]    0    0    0    1    0     0 2.489307e-10 7.000000e-01 0.000000e+00\n [89,]    0    0    0    0    1     0 1.742515e-10 4.900000e-01 0.000000e+00\n [90,]    0    0    0    0    0     0 1.219760e-10 3.430000e-01 0.000000e+00\n [91,]    0    0    0    0    0     0 8.538323e-11 2.401000e-01 0.000000e+00\n [92,]    0    0    0    0    0     0 5.976826e-11 1.680700e-01 0.000000e+00\n [93,]    0    0    0    0    0     0 4.183778e-11 1.176490e-01 0.000000e+00\n [94,]    0    0    0    0    0     0 2.928645e-11 8.235430e-02 0.000000e+00\n [95,]    0    0    0    0    0     0 2.050051e-11 5.764801e-02 0.000000e+00\n [96,]    0    0    0    0    0     0 1.435036e-11 4.035361e-02 0.000000e+00\n [97,]    0    0    0    0    0     0 1.004525e-11 2.824752e-02 0.000000e+00\n [98,]    0    0    0    0    0     0 7.031676e-12 1.977327e-02 0.000000e+00\n [99,]    0    0    0    0    0     0 4.922174e-12 1.384129e-02 0.000000e+00\n[100,]    0    0    0    0    0     0 3.445521e-12 9.688901e-03 0.000000e+00\n[101,]    0    0    0    0    0     0 2.411865e-12 6.782231e-03 0.000000e+00\n[102,]    0    0    0    0    0     0 1.688306e-12 4.747562e-03 0.000000e+00\n[103,]    0    0    0    0    0     0 1.181814e-12 3.323293e-03 0.000000e+00\n[104,]    0    0    0    0    0     0 8.272697e-13 2.326305e-03 0.000000e+00\n[105,]    0    0    0    0    0     0 5.790888e-13 1.628414e-03 0.000000e+00\n[106,]    0    0    0    0    0     0 4.053622e-13 1.139890e-03 0.000000e+00\n[107,]    0    0    0    0    0     0 2.837535e-13 7.979227e-04 0.000000e+00\n[108,]    0    0    0    0    0     0 1.986275e-13 5.585459e-04 0.000000e+00\n[109,]    0    0    0    0    0     0 1.390392e-13 3.909821e-04 0.000000e+00\n[110,]    0    0    0    0    0     0 9.732745e-14 2.736875e-04 0.000000e+00\n[111,]    0    0    0    0    0     0 6.812922e-14 1.915812e-04 0.000000e+00\n[112,]    0    0    0    0    0     0 4.769045e-14 1.341069e-04 0.000000e+00\n[113,]    0    0    0    0    0     0 3.338332e-14 9.387480e-05 0.000000e+00\n[114,]    0    0    0    0    0     0 2.336832e-14 6.571236e-05 0.000000e+00\n[115,]    0    0    0    0    0     0 1.635783e-14 4.599865e-05 0.000000e+00\n[116,]    0    0    0    0    0     0 1.145048e-14 3.219906e-05 0.000000e+00\n[117,]    0    0    0    0    0     0 8.015334e-15 2.253934e-05 0.000000e+00\n[118,]    0    0    0    0    0     0 5.610734e-15 1.577754e-05 0.000000e+00\n[119,]    0    0    0    0    0     0 3.927514e-15 1.104428e-05 0.000000e+00\n[120,]    0    0    0    0    0     0 2.749260e-15 7.730994e-06 0.000000e+00\n[121,]    0    0    0    0    0     0 1.924482e-15 5.411696e-06 0.000000e+00\n[122,]    0    0    0    0    0     0 1.347137e-15 3.788187e-06 0.000000e+00\n[123,]    0    0    0    0    0     0 9.429961e-16 2.651731e-06 1.000000e+00\n[124,]    0    0    0    0    0     0 6.600972e-16 1.856212e-06 7.000000e-01\n[125,]    0    0    0    0    0     0 4.620681e-16 1.299348e-06 4.900000e-01\n[126,]    0    0    0    0    0     1 3.234477e-16 9.095437e-07 3.430000e-01\n[127,]    0    0    0    0    0     0 2.264134e-16 6.366806e-07 2.401000e-01\n[128,]    0    0    0    0    0     0 1.584893e-16 4.456764e-07 1.680700e-01\n[129,]    0    0    0    0    0     0 1.109425e-16 3.119735e-07 1.176490e-01\n[130,]    0    0    0    0    0     0 7.765978e-17 2.183814e-07 8.235430e-02\n[131,]    0    0    0    0    0     0 5.436185e-17 1.528670e-07 5.764801e-02\n[132,]    0    0    0    0    0     0 3.805329e-17 1.070069e-07 4.035361e-02\n[133,]    0    0    0    0    0     0 2.663730e-17 7.490483e-08 2.824752e-02\n[134,]    0    0    0    0    0     0 1.864611e-17 5.243338e-08 1.977327e-02\n[135,]    0    0    0    0    0     0 1.305228e-17 3.670337e-08 1.384129e-02\n[136,]    0    0    0    0    0     0 9.136596e-18 2.569236e-08 9.688901e-03\n[137,]    0    0    0    0    0     0 6.395617e-18 1.798465e-08 6.782231e-03\n[138,]    0    0    0    0    0     0 4.476932e-18 1.258926e-08 4.747562e-03\n[139,]    0    0    0    0    0     0 3.133852e-18 8.812479e-09 3.323293e-03\n[140,]    0    0    0    0    0     0 2.193697e-18 6.168735e-09 2.326305e-03\n[141,]    0    0    0    0    0     0 1.535588e-18 4.318115e-09 1.628414e-03\n[142,]    0    0    0    0    0     0 1.074911e-18 3.022680e-09 1.139890e-03\n[143,]    0    0    0    0    0     0 7.524379e-19 2.115876e-09 7.979227e-04\n[144,]    0    0    0    0    0     0 5.267066e-19 1.481113e-09 5.585459e-04\n[145,]    0    0    0    0    0     0 3.686946e-19 1.036779e-09 3.909821e-04\n[146,]    0    0    0    0    0     0 2.580862e-19 7.257455e-10 2.736875e-04\n[147,]    0    0    0    0    0     0 1.806603e-19 5.080219e-10 1.915812e-04\n[148,]    0    0    0    0    0     0 1.264622e-19 3.556153e-10 1.341069e-04\n[149,]    0    0    0    0    0     0 8.852357e-20 2.489307e-10 9.387480e-05\n[150,]    0    0    0    0    0     0 6.196650e-20 1.742515e-10 6.571236e-05\n[151,]    0    0    0    0    0     0 4.337655e-20 1.219760e-10 4.599865e-05\n[152,]    0    0    0    0    0     0 3.036358e-20 8.538323e-11 3.219906e-05\n[153,]    0    0    0    0    0     0 2.125451e-20 5.976826e-11 2.253934e-05\n[154,]    0    0    0    0    0     0 1.487816e-20 4.183778e-11 1.577754e-05\n[155,]    0    0    0    0    0     0 1.041471e-20 2.928645e-11 1.104428e-05\n[156,]    0    0    0    0    0     0 7.290297e-21 2.050051e-11 7.730994e-06\n[157,]    0    0    0    0    0     0 5.103208e-21 1.435036e-11 5.411696e-06\n[158,]    0    0    0    0    0     0 3.572245e-21 1.004525e-11 3.788187e-06\n[159,]    0    0    0    0    0     0 2.500572e-21 7.031676e-12 2.651731e-06\n[160,]    0    0    0    0    0     0 1.750400e-21 4.922174e-12 1.856212e-06\n[161,]    0    0    0    0    0     0 1.225280e-21 3.445521e-12 1.299348e-06\n[162,]    0    0    0    0    0     0 8.576961e-22 2.411865e-12 9.095437e-07\n[163,]    0    0    0    0    0     0 6.003873e-22 1.688306e-12 6.366806e-07\n[164,]    0    0    0    0    0     0 4.202711e-22 1.181814e-12 4.456764e-07\n[165,]    0    0    0    0    0     0 2.941898e-22 8.272697e-13 3.119735e-07\n[166,]    0    0    0    0    0     0 2.059328e-22 5.790888e-13 2.183814e-07\n[167,]    0    0    0    0    0     0 1.441530e-22 4.053622e-13 1.528670e-07\n[168,]    0    0    0    0    0     0 1.009071e-22 2.837535e-13 1.070069e-07\n[169,]    0    0    0    0    0     0 7.063496e-23 1.986275e-13 7.490483e-08\n[170,]    0    0    0    0    0     0 4.944447e-23 1.390392e-13 5.243338e-08\n[171,]    0    0    0    0    0     0 3.461113e-23 9.732745e-14 3.670337e-08\n[172,]    0    0    0    0    0     0 2.422779e-23 6.812922e-14 2.569236e-08\n[173,]    0    0    0    0    0     0 1.695945e-23 4.769045e-14 1.798465e-08\n[174,]    0    0    0    0    0     0 1.187162e-23 3.338332e-14 1.258926e-08\n[175,]    0    0    0    0    0     0 8.310133e-24 2.336832e-14 8.812479e-09\n[176,]    0    0    0    0    0     0 5.817093e-24 1.635783e-14 6.168735e-09\n[177,]    0    0    0    0    0     0 4.071965e-24 1.145048e-14 4.318115e-09\n[178,]    0    0    0    0    0     0 2.850376e-24 8.015334e-15 3.022680e-09\n[179,]    0    0    0    0    0     0 1.995263e-24 5.610734e-15 2.115876e-09\n[180,]    0    0    0    0    0     0 1.396684e-24 3.927514e-15 1.481113e-09\n[181,]    0    0    0    0    0     0 9.776788e-25 2.749260e-15 1.036779e-09\n[182,]    0    0    0    0    0     0 6.843752e-25 1.924482e-15 7.257455e-10\n[183,]    0    0    0    0    0     0 4.790626e-25 1.347137e-15 5.080219e-10\n[184,]    0    0    0    0    0     0 3.353438e-25 9.429961e-16 3.556153e-10\n[185,]    0    0    0    0    0     0 2.347407e-25 6.600972e-16 2.489307e-10\n[186,]    0    0    0    0    0     0 1.643185e-25 4.620681e-16 1.742515e-10\n[187,]    0    0    0    0    0     0 1.150229e-25 3.234477e-16 1.219760e-10\n[188,]    0    0    0    0    0     0 8.051605e-26 2.264134e-16 8.538323e-11\n[189,]    0    0    0    0    0     0 5.636124e-26 1.584893e-16 5.976826e-11\n[190,]    0    0    0    0    0     0 3.945287e-26 1.109425e-16 4.183778e-11\n[191,]    0    0    0    0    0     0 2.761701e-26 7.765978e-17 2.928645e-11\n[192,]    0    0    0    0    0     0 1.933190e-26 5.436185e-17 2.050051e-11\n[193,]    0    0    0    0    0     0 1.353233e-26 3.805329e-17 1.435036e-11\n[194,]    0    0    0    0    0     0 9.472633e-27 2.663730e-17 1.004525e-11\n[195,]    0    0    0    0    0     0 6.630843e-27 1.864611e-17 7.031676e-12\n[196,]    0    0    0    0    0     0 4.641590e-27 1.305228e-17 4.922174e-12\n[197,]    0    0    0    0    0     0 3.249113e-27 9.136596e-18 3.445521e-12\n[198,]    0    0    0    0    0     0 2.274379e-27 6.395617e-18 2.411865e-12\n[199,]    0    0    0    0    0     0 1.592065e-27 4.476932e-18 1.688306e-12\n[200,]    0    0    0    0    0     0 1.114446e-27 3.133852e-18 1.181814e-12\n[201,]    0    0    0    0    0     0 7.801121e-28 2.193697e-18 8.272697e-13\n[202,]    0    0    0    0    0     0 5.460785e-28 1.535588e-18 5.790888e-13\n[203,]    0    0    0    0    0     0 3.822549e-28 1.074911e-18 4.053622e-13\n[204,]    0    0    0    0    0     0 2.675784e-28 7.524379e-19 2.837535e-13\n[205,]    0    0    0    0    0     0 1.873049e-28 5.267066e-19 1.986275e-13\n[206,]    0    0    0    0    0     0 1.311134e-28 3.686946e-19 1.390392e-13\n[207,]    0    0    0    0    0     0 9.177941e-29 2.580862e-19 9.732745e-14\n[208,]    0    0    0    0    0     0 6.424558e-29 1.806603e-19 6.812922e-14\n[209,]    0    0    0    0    0     0 4.497191e-29 1.264622e-19 4.769045e-14\n[210,]    0    0    0    0    0     0 3.148034e-29 8.852357e-20 3.338332e-14\n[211,]    0    0    0    0    0     0 2.203624e-29 6.196650e-20 2.336832e-14\n[212,]    0    0    0    0    0     0 1.542536e-29 4.337655e-20 1.635783e-14\n[213,]    0    0    0    0    0     0 1.079776e-29 3.036358e-20 1.145048e-14\n[214,]    0    0    0    0    0     0 7.558429e-30 2.125451e-20 8.015334e-15\n[215,]    0    0    0    0    0     0 5.290900e-30 1.487816e-20 5.610734e-15\n[216,]    0    0    0    0    0     0 3.703630e-30 1.041471e-20 3.927514e-15\n[217,]    0    0    0    0    0     0 2.592541e-30 7.290297e-21 2.749260e-15\n[218,]    0    0    0    0    0     0 1.814779e-30 5.103208e-21 1.924482e-15\n[219,]    0    0    0    0    0     0 1.270345e-30 3.572245e-21 1.347137e-15\n[220,]    0    0    0    0    0     0 8.892416e-31 2.500572e-21 9.429961e-16\n[221,]    0    0    0    0    0     0 6.224691e-31 1.750400e-21 6.600972e-16\n[222,]    0    0    0    0    0     0 4.357284e-31 1.225280e-21 4.620681e-16\n[223,]    0    0    0    0    0     0 3.050099e-31 8.576961e-22 3.234477e-16\n[224,]    0    0    0    0    0     0 2.135069e-31 6.003873e-22 2.264134e-16\n[225,]    0    0    0    0    0     0 1.494548e-31 4.202711e-22 1.584893e-16\n              TC124        TC125\n  [1,] 0.000000e+00 0.000000e+00\n  [2,] 0.000000e+00 0.000000e+00\n  [3,] 0.000000e+00 0.000000e+00\n  [4,] 0.000000e+00 0.000000e+00\n  [5,] 0.000000e+00 0.000000e+00\n  [6,] 0.000000e+00 0.000000e+00\n  [7,] 0.000000e+00 0.000000e+00\n  [8,] 0.000000e+00 0.000000e+00\n  [9,] 0.000000e+00 0.000000e+00\n [10,] 0.000000e+00 0.000000e+00\n [11,] 0.000000e+00 0.000000e+00\n [12,] 0.000000e+00 0.000000e+00\n [13,] 0.000000e+00 0.000000e+00\n [14,] 0.000000e+00 0.000000e+00\n [15,] 0.000000e+00 0.000000e+00\n [16,] 0.000000e+00 0.000000e+00\n [17,] 0.000000e+00 0.000000e+00\n [18,] 0.000000e+00 0.000000e+00\n [19,] 0.000000e+00 0.000000e+00\n [20,] 0.000000e+00 0.000000e+00\n [21,] 0.000000e+00 0.000000e+00\n [22,] 0.000000e+00 0.000000e+00\n [23,] 0.000000e+00 0.000000e+00\n [24,] 0.000000e+00 0.000000e+00\n [25,] 0.000000e+00 0.000000e+00\n [26,] 0.000000e+00 0.000000e+00\n [27,] 0.000000e+00 0.000000e+00\n [28,] 0.000000e+00 0.000000e+00\n [29,] 0.000000e+00 0.000000e+00\n [30,] 0.000000e+00 0.000000e+00\n [31,] 0.000000e+00 0.000000e+00\n [32,] 0.000000e+00 0.000000e+00\n [33,] 0.000000e+00 0.000000e+00\n [34,] 0.000000e+00 0.000000e+00\n [35,] 0.000000e+00 0.000000e+00\n [36,] 0.000000e+00 0.000000e+00\n [37,] 0.000000e+00 0.000000e+00\n [38,] 0.000000e+00 0.000000e+00\n [39,] 0.000000e+00 0.000000e+00\n [40,] 0.000000e+00 0.000000e+00\n [41,] 0.000000e+00 0.000000e+00\n [42,] 0.000000e+00 0.000000e+00\n [43,] 0.000000e+00 0.000000e+00\n [44,] 0.000000e+00 0.000000e+00\n [45,] 0.000000e+00 0.000000e+00\n [46,] 0.000000e+00 0.000000e+00\n [47,] 0.000000e+00 0.000000e+00\n [48,] 0.000000e+00 0.000000e+00\n [49,] 0.000000e+00 0.000000e+00\n [50,] 0.000000e+00 0.000000e+00\n [51,] 0.000000e+00 0.000000e+00\n [52,] 0.000000e+00 0.000000e+00\n [53,] 0.000000e+00 0.000000e+00\n [54,] 0.000000e+00 0.000000e+00\n [55,] 0.000000e+00 0.000000e+00\n [56,] 0.000000e+00 0.000000e+00\n [57,] 0.000000e+00 0.000000e+00\n [58,] 0.000000e+00 0.000000e+00\n [59,] 0.000000e+00 0.000000e+00\n [60,] 0.000000e+00 0.000000e+00\n [61,] 0.000000e+00 0.000000e+00\n [62,] 0.000000e+00 0.000000e+00\n [63,] 0.000000e+00 0.000000e+00\n [64,] 0.000000e+00 0.000000e+00\n [65,] 0.000000e+00 0.000000e+00\n [66,] 0.000000e+00 0.000000e+00\n [67,] 0.000000e+00 0.000000e+00\n [68,] 0.000000e+00 0.000000e+00\n [69,] 0.000000e+00 0.000000e+00\n [70,] 0.000000e+00 0.000000e+00\n [71,] 0.000000e+00 0.000000e+00\n [72,] 0.000000e+00 0.000000e+00\n [73,] 0.000000e+00 0.000000e+00\n [74,] 0.000000e+00 0.000000e+00\n [75,] 0.000000e+00 0.000000e+00\n [76,] 0.000000e+00 0.000000e+00\n [77,] 0.000000e+00 0.000000e+00\n [78,] 0.000000e+00 0.000000e+00\n [79,] 0.000000e+00 0.000000e+00\n [80,] 0.000000e+00 0.000000e+00\n [81,] 0.000000e+00 0.000000e+00\n [82,] 0.000000e+00 0.000000e+00\n [83,] 0.000000e+00 0.000000e+00\n [84,] 0.000000e+00 0.000000e+00\n [85,] 0.000000e+00 0.000000e+00\n [86,] 0.000000e+00 0.000000e+00\n [87,] 0.000000e+00 0.000000e+00\n [88,] 0.000000e+00 0.000000e+00\n [89,] 0.000000e+00 0.000000e+00\n [90,] 0.000000e+00 0.000000e+00\n [91,] 0.000000e+00 0.000000e+00\n [92,] 0.000000e+00 0.000000e+00\n [93,] 0.000000e+00 0.000000e+00\n [94,] 0.000000e+00 0.000000e+00\n [95,] 0.000000e+00 0.000000e+00\n [96,] 0.000000e+00 0.000000e+00\n [97,] 0.000000e+00 0.000000e+00\n [98,] 0.000000e+00 0.000000e+00\n [99,] 0.000000e+00 0.000000e+00\n[100,] 0.000000e+00 0.000000e+00\n[101,] 0.000000e+00 0.000000e+00\n[102,] 0.000000e+00 0.000000e+00\n[103,] 0.000000e+00 0.000000e+00\n[104,] 0.000000e+00 0.000000e+00\n[105,] 0.000000e+00 0.000000e+00\n[106,] 0.000000e+00 0.000000e+00\n[107,] 0.000000e+00 0.000000e+00\n[108,] 0.000000e+00 0.000000e+00\n[109,] 0.000000e+00 0.000000e+00\n[110,] 0.000000e+00 0.000000e+00\n[111,] 0.000000e+00 0.000000e+00\n[112,] 0.000000e+00 0.000000e+00\n[113,] 0.000000e+00 0.000000e+00\n[114,] 0.000000e+00 0.000000e+00\n[115,] 0.000000e+00 0.000000e+00\n[116,] 0.000000e+00 0.000000e+00\n[117,] 0.000000e+00 0.000000e+00\n[118,] 0.000000e+00 0.000000e+00\n[119,] 0.000000e+00 0.000000e+00\n[120,] 0.000000e+00 0.000000e+00\n[121,] 0.000000e+00 0.000000e+00\n[122,] 0.000000e+00 0.000000e+00\n[123,] 0.000000e+00 0.000000e+00\n[124,] 1.000000e+00 0.000000e+00\n[125,] 7.000000e-01 1.000000e+00\n[126,] 4.900000e-01 7.000000e-01\n[127,] 3.430000e-01 4.900000e-01\n[128,] 2.401000e-01 3.430000e-01\n[129,] 1.680700e-01 2.401000e-01\n[130,] 1.176490e-01 1.680700e-01\n[131,] 8.235430e-02 1.176490e-01\n[132,] 5.764801e-02 8.235430e-02\n[133,] 4.035361e-02 5.764801e-02\n[134,] 2.824752e-02 4.035361e-02\n[135,] 1.977327e-02 2.824752e-02\n[136,] 1.384129e-02 1.977327e-02\n[137,] 9.688901e-03 1.384129e-02\n[138,] 6.782231e-03 9.688901e-03\n[139,] 4.747562e-03 6.782231e-03\n[140,] 3.323293e-03 4.747562e-03\n[141,] 2.326305e-03 3.323293e-03\n[142,] 1.628414e-03 2.326305e-03\n[143,] 1.139890e-03 1.628414e-03\n[144,] 7.979227e-04 1.139890e-03\n[145,] 5.585459e-04 7.979227e-04\n[146,] 3.909821e-04 5.585459e-04\n[147,] 2.736875e-04 3.909821e-04\n[148,] 1.915812e-04 2.736875e-04\n[149,] 1.341069e-04 1.915812e-04\n[150,] 9.387480e-05 1.341069e-04\n[151,] 6.571236e-05 9.387480e-05\n[152,] 4.599865e-05 6.571236e-05\n[153,] 3.219906e-05 4.599865e-05\n[154,] 2.253934e-05 3.219906e-05\n[155,] 1.577754e-05 2.253934e-05\n[156,] 1.104428e-05 1.577754e-05\n[157,] 7.730994e-06 1.104428e-05\n[158,] 5.411696e-06 7.730994e-06\n[159,] 3.788187e-06 5.411696e-06\n[160,] 2.651731e-06 3.788187e-06\n[161,] 1.856212e-06 2.651731e-06\n[162,] 1.299348e-06 1.856212e-06\n[163,] 9.095437e-07 1.299348e-06\n[164,] 6.366806e-07 9.095437e-07\n[165,] 4.456764e-07 6.366806e-07\n[166,] 3.119735e-07 4.456764e-07\n[167,] 2.183814e-07 3.119735e-07\n[168,] 1.528670e-07 2.183814e-07\n[169,] 1.070069e-07 1.528670e-07\n[170,] 7.490483e-08 1.070069e-07\n[171,] 5.243338e-08 7.490483e-08\n[172,] 3.670337e-08 5.243338e-08\n[173,] 2.569236e-08 3.670337e-08\n[174,] 1.798465e-08 2.569236e-08\n[175,] 1.258926e-08 1.798465e-08\n[176,] 8.812479e-09 1.258926e-08\n[177,] 6.168735e-09 8.812479e-09\n[178,] 4.318115e-09 6.168735e-09\n[179,] 3.022680e-09 4.318115e-09\n[180,] 2.115876e-09 3.022680e-09\n[181,] 1.481113e-09 2.115876e-09\n[182,] 1.036779e-09 1.481113e-09\n[183,] 7.257455e-10 1.036779e-09\n[184,] 5.080219e-10 7.257455e-10\n[185,] 3.556153e-10 5.080219e-10\n[186,] 2.489307e-10 3.556153e-10\n[187,] 1.742515e-10 2.489307e-10\n[188,] 1.219760e-10 1.742515e-10\n[189,] 8.538323e-11 1.219760e-10\n[190,] 5.976826e-11 8.538323e-11\n[191,] 4.183778e-11 5.976826e-11\n[192,] 2.928645e-11 4.183778e-11\n[193,] 2.050051e-11 2.928645e-11\n[194,] 1.435036e-11 2.050051e-11\n[195,] 1.004525e-11 1.435036e-11\n[196,] 7.031676e-12 1.004525e-11\n[197,] 4.922174e-12 7.031676e-12\n[198,] 3.445521e-12 4.922174e-12\n[199,] 2.411865e-12 3.445521e-12\n[200,] 1.688306e-12 2.411865e-12\n[201,] 1.181814e-12 1.688306e-12\n[202,] 8.272697e-13 1.181814e-12\n[203,] 5.790888e-13 8.272697e-13\n[204,] 4.053622e-13 5.790888e-13\n[205,] 2.837535e-13 4.053622e-13\n[206,] 1.986275e-13 2.837535e-13\n[207,] 1.390392e-13 1.986275e-13\n[208,] 9.732745e-14 1.390392e-13\n[209,] 6.812922e-14 9.732745e-14\n[210,] 4.769045e-14 6.812922e-14\n[211,] 3.338332e-14 4.769045e-14\n[212,] 2.336832e-14 3.338332e-14\n[213,] 1.635783e-14 2.336832e-14\n[214,] 1.145048e-14 1.635783e-14\n[215,] 8.015334e-15 1.145048e-14\n[216,] 5.610734e-15 8.015334e-15\n[217,] 3.927514e-15 5.610734e-15\n[218,] 2.749260e-15 3.927514e-15\n[219,] 1.924482e-15 2.749260e-15\n[220,] 1.347137e-15 1.924482e-15\n[221,] 9.429961e-16 1.347137e-15\n[222,] 6.600972e-16 9.429961e-16\n[223,] 4.620681e-16 6.600972e-16\n[224,] 3.234477e-16 4.620681e-16\n[225,] 2.264134e-16 3.234477e-16\nNotese que se obtuvieron 26 outliers, lo cual nos puede indicar que el modelo ajustado no es muy bueno para este modelo, por dicha razón no seria lo mejor ajustar outliers a este modelo.\nmodelo_SARIMA_ref_outliers = Arima(df_train4, c(5, 1, 1),seasonal = list(order = c(3, 1, 2), period = 12),lambda = 0,fixed=c(0,NA,0,0,0,NA,NA,NA,0,0,0,NA,NA,0,NA,NA,NA,NA,0,0,NA,0),xreg =xregSARIMA )\ncoeftest(modelo_SARIMA_ref_outliers)\n\n\nz test of coefficients:\n\n        Estimate Std. Error  z value  Pr(&gt;|z|)    \nar2    0.2341858  0.0728046   3.2166 0.0012970 ** \nma1   -0.9298058  0.0266686 -34.8652 &lt; 2.2e-16 ***\nsar1  -0.2095066  0.0700866  -2.9893 0.0027966 ** \nsar2  -0.2817612  0.0737322  -3.8214 0.0001327 ***\nAO50  -0.0130544  0.0049514  -2.6365 0.0083764 ** \nAO52   0.0106068  0.0050200   2.1129 0.0346092 *  \nAO88  -0.0479514  0.0050262  -9.5403 &lt; 2.2e-16 ***\nAO89   0.0300805  0.0048134   6.2493 4.124e-10 ***\nAO126 -0.0146734  0.0050002  -2.9346 0.0033400 ** \nTC26  -0.0145443  0.0041481  -3.5062 0.0004545 ***\nTC124 -0.0203414  0.0041051  -4.9551 7.228e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Revisando que no hay más outliers\nresiO2= residuals(modelo_SARIMA_ref_outliers)\ncoefO2= coefs2poly(modelo_SARIMA_ref_outliers)\noutliersSARIMA2= locate.outliers(resiO2,coefO2,cval=4.5)\noutliersSARIMA2\n\n  type ind     coefhat     tstat\n2   TC  33  0.01302646  5.000072\n3   TC 152 -0.01184981 -4.548306\n\nn=length(df_train4)\nxregSARIMA2 = outliers.effects(outliersSARIMA2,n )\nxregSARIMA2\n\n               TC33        TC152\n  [1,] 0.000000e+00 0.000000e+00\n  [2,] 0.000000e+00 0.000000e+00\n  [3,] 0.000000e+00 0.000000e+00\n  [4,] 0.000000e+00 0.000000e+00\n  [5,] 0.000000e+00 0.000000e+00\n  [6,] 0.000000e+00 0.000000e+00\n  [7,] 0.000000e+00 0.000000e+00\n  [8,] 0.000000e+00 0.000000e+00\n  [9,] 0.000000e+00 0.000000e+00\n [10,] 0.000000e+00 0.000000e+00\n [11,] 0.000000e+00 0.000000e+00\n [12,] 0.000000e+00 0.000000e+00\n [13,] 0.000000e+00 0.000000e+00\n [14,] 0.000000e+00 0.000000e+00\n [15,] 0.000000e+00 0.000000e+00\n [16,] 0.000000e+00 0.000000e+00\n [17,] 0.000000e+00 0.000000e+00\n [18,] 0.000000e+00 0.000000e+00\n [19,] 0.000000e+00 0.000000e+00\n [20,] 0.000000e+00 0.000000e+00\n [21,] 0.000000e+00 0.000000e+00\n [22,] 0.000000e+00 0.000000e+00\n [23,] 0.000000e+00 0.000000e+00\n [24,] 0.000000e+00 0.000000e+00\n [25,] 0.000000e+00 0.000000e+00\n [26,] 0.000000e+00 0.000000e+00\n [27,] 0.000000e+00 0.000000e+00\n [28,] 0.000000e+00 0.000000e+00\n [29,] 0.000000e+00 0.000000e+00\n [30,] 0.000000e+00 0.000000e+00\n [31,] 0.000000e+00 0.000000e+00\n [32,] 0.000000e+00 0.000000e+00\n [33,] 1.000000e+00 0.000000e+00\n [34,] 7.000000e-01 0.000000e+00\n [35,] 4.900000e-01 0.000000e+00\n [36,] 3.430000e-01 0.000000e+00\n [37,] 2.401000e-01 0.000000e+00\n [38,] 1.680700e-01 0.000000e+00\n [39,] 1.176490e-01 0.000000e+00\n [40,] 8.235430e-02 0.000000e+00\n [41,] 5.764801e-02 0.000000e+00\n [42,] 4.035361e-02 0.000000e+00\n [43,] 2.824752e-02 0.000000e+00\n [44,] 1.977327e-02 0.000000e+00\n [45,] 1.384129e-02 0.000000e+00\n [46,] 9.688901e-03 0.000000e+00\n [47,] 6.782231e-03 0.000000e+00\n [48,] 4.747562e-03 0.000000e+00\n [49,] 3.323293e-03 0.000000e+00\n [50,] 2.326305e-03 0.000000e+00\n [51,] 1.628414e-03 0.000000e+00\n [52,] 1.139890e-03 0.000000e+00\n [53,] 7.979227e-04 0.000000e+00\n [54,] 5.585459e-04 0.000000e+00\n [55,] 3.909821e-04 0.000000e+00\n [56,] 2.736875e-04 0.000000e+00\n [57,] 1.915812e-04 0.000000e+00\n [58,] 1.341069e-04 0.000000e+00\n [59,] 9.387480e-05 0.000000e+00\n [60,] 6.571236e-05 0.000000e+00\n [61,] 4.599865e-05 0.000000e+00\n [62,] 3.219906e-05 0.000000e+00\n [63,] 2.253934e-05 0.000000e+00\n [64,] 1.577754e-05 0.000000e+00\n [65,] 1.104428e-05 0.000000e+00\n [66,] 7.730994e-06 0.000000e+00\n [67,] 5.411696e-06 0.000000e+00\n [68,] 3.788187e-06 0.000000e+00\n [69,] 2.651731e-06 0.000000e+00\n [70,] 1.856212e-06 0.000000e+00\n [71,] 1.299348e-06 0.000000e+00\n [72,] 9.095437e-07 0.000000e+00\n [73,] 6.366806e-07 0.000000e+00\n [74,] 4.456764e-07 0.000000e+00\n [75,] 3.119735e-07 0.000000e+00\n [76,] 2.183814e-07 0.000000e+00\n [77,] 1.528670e-07 0.000000e+00\n [78,] 1.070069e-07 0.000000e+00\n [79,] 7.490483e-08 0.000000e+00\n [80,] 5.243338e-08 0.000000e+00\n [81,] 3.670337e-08 0.000000e+00\n [82,] 2.569236e-08 0.000000e+00\n [83,] 1.798465e-08 0.000000e+00\n [84,] 1.258926e-08 0.000000e+00\n [85,] 8.812479e-09 0.000000e+00\n [86,] 6.168735e-09 0.000000e+00\n [87,] 4.318115e-09 0.000000e+00\n [88,] 3.022680e-09 0.000000e+00\n [89,] 2.115876e-09 0.000000e+00\n [90,] 1.481113e-09 0.000000e+00\n [91,] 1.036779e-09 0.000000e+00\n [92,] 7.257455e-10 0.000000e+00\n [93,] 5.080219e-10 0.000000e+00\n [94,] 3.556153e-10 0.000000e+00\n [95,] 2.489307e-10 0.000000e+00\n [96,] 1.742515e-10 0.000000e+00\n [97,] 1.219760e-10 0.000000e+00\n [98,] 8.538323e-11 0.000000e+00\n [99,] 5.976826e-11 0.000000e+00\n[100,] 4.183778e-11 0.000000e+00\n[101,] 2.928645e-11 0.000000e+00\n[102,] 2.050051e-11 0.000000e+00\n[103,] 1.435036e-11 0.000000e+00\n[104,] 1.004525e-11 0.000000e+00\n[105,] 7.031676e-12 0.000000e+00\n[106,] 4.922174e-12 0.000000e+00\n[107,] 3.445521e-12 0.000000e+00\n[108,] 2.411865e-12 0.000000e+00\n[109,] 1.688306e-12 0.000000e+00\n[110,] 1.181814e-12 0.000000e+00\n[111,] 8.272697e-13 0.000000e+00\n[112,] 5.790888e-13 0.000000e+00\n[113,] 4.053622e-13 0.000000e+00\n[114,] 2.837535e-13 0.000000e+00\n[115,] 1.986275e-13 0.000000e+00\n[116,] 1.390392e-13 0.000000e+00\n[117,] 9.732745e-14 0.000000e+00\n[118,] 6.812922e-14 0.000000e+00\n[119,] 4.769045e-14 0.000000e+00\n[120,] 3.338332e-14 0.000000e+00\n[121,] 2.336832e-14 0.000000e+00\n[122,] 1.635783e-14 0.000000e+00\n[123,] 1.145048e-14 0.000000e+00\n[124,] 8.015334e-15 0.000000e+00\n[125,] 5.610734e-15 0.000000e+00\n[126,] 3.927514e-15 0.000000e+00\n[127,] 2.749260e-15 0.000000e+00\n[128,] 1.924482e-15 0.000000e+00\n[129,] 1.347137e-15 0.000000e+00\n[130,] 9.429961e-16 0.000000e+00\n[131,] 6.600972e-16 0.000000e+00\n[132,] 4.620681e-16 0.000000e+00\n[133,] 3.234477e-16 0.000000e+00\n[134,] 2.264134e-16 0.000000e+00\n[135,] 1.584893e-16 0.000000e+00\n[136,] 1.109425e-16 0.000000e+00\n[137,] 7.765978e-17 0.000000e+00\n[138,] 5.436185e-17 0.000000e+00\n[139,] 3.805329e-17 0.000000e+00\n[140,] 2.663730e-17 0.000000e+00\n[141,] 1.864611e-17 0.000000e+00\n[142,] 1.305228e-17 0.000000e+00\n[143,] 9.136596e-18 0.000000e+00\n[144,] 6.395617e-18 0.000000e+00\n[145,] 4.476932e-18 0.000000e+00\n[146,] 3.133852e-18 0.000000e+00\n[147,] 2.193697e-18 0.000000e+00\n[148,] 1.535588e-18 0.000000e+00\n[149,] 1.074911e-18 0.000000e+00\n[150,] 7.524379e-19 0.000000e+00\n[151,] 5.267066e-19 0.000000e+00\n[152,] 3.686946e-19 1.000000e+00\n[153,] 2.580862e-19 7.000000e-01\n[154,] 1.806603e-19 4.900000e-01\n[155,] 1.264622e-19 3.430000e-01\n[156,] 8.852357e-20 2.401000e-01\n[157,] 6.196650e-20 1.680700e-01\n[158,] 4.337655e-20 1.176490e-01\n[159,] 3.036358e-20 8.235430e-02\n[160,] 2.125451e-20 5.764801e-02\n[161,] 1.487816e-20 4.035361e-02\n[162,] 1.041471e-20 2.824752e-02\n[163,] 7.290297e-21 1.977327e-02\n[164,] 5.103208e-21 1.384129e-02\n[165,] 3.572245e-21 9.688901e-03\n[166,] 2.500572e-21 6.782231e-03\n[167,] 1.750400e-21 4.747562e-03\n[168,] 1.225280e-21 3.323293e-03\n[169,] 8.576961e-22 2.326305e-03\n[170,] 6.003873e-22 1.628414e-03\n[171,] 4.202711e-22 1.139890e-03\n[172,] 2.941898e-22 7.979227e-04\n[173,] 2.059328e-22 5.585459e-04\n[174,] 1.441530e-22 3.909821e-04\n[175,] 1.009071e-22 2.736875e-04\n[176,] 7.063496e-23 1.915812e-04\n[177,] 4.944447e-23 1.341069e-04\n[178,] 3.461113e-23 9.387480e-05\n[179,] 2.422779e-23 6.571236e-05\n[180,] 1.695945e-23 4.599865e-05\n[181,] 1.187162e-23 3.219906e-05\n[182,] 8.310133e-24 2.253934e-05\n[183,] 5.817093e-24 1.577754e-05\n[184,] 4.071965e-24 1.104428e-05\n[185,] 2.850376e-24 7.730994e-06\n[186,] 1.995263e-24 5.411696e-06\n[187,] 1.396684e-24 3.788187e-06\n[188,] 9.776788e-25 2.651731e-06\n[189,] 6.843752e-25 1.856212e-06\n[190,] 4.790626e-25 1.299348e-06\n[191,] 3.353438e-25 9.095437e-07\n[192,] 2.347407e-25 6.366806e-07\n[193,] 1.643185e-25 4.456764e-07\n[194,] 1.150229e-25 3.119735e-07\n[195,] 8.051605e-26 2.183814e-07\n[196,] 5.636124e-26 1.528670e-07\n[197,] 3.945287e-26 1.070069e-07\n[198,] 2.761701e-26 7.490483e-08\n[199,] 1.933190e-26 5.243338e-08\n[200,] 1.353233e-26 3.670337e-08\n[201,] 9.472633e-27 2.569236e-08\n[202,] 6.630843e-27 1.798465e-08\n[203,] 4.641590e-27 1.258926e-08\n[204,] 3.249113e-27 8.812479e-09\n[205,] 2.274379e-27 6.168735e-09\n[206,] 1.592065e-27 4.318115e-09\n[207,] 1.114446e-27 3.022680e-09\n[208,] 7.801121e-28 2.115876e-09\n[209,] 5.460785e-28 1.481113e-09\n[210,] 3.822549e-28 1.036779e-09\n[211,] 2.675784e-28 7.257455e-10\n[212,] 1.873049e-28 5.080219e-10\n[213,] 1.311134e-28 3.556153e-10\n[214,] 9.177941e-29 2.489307e-10\n[215,] 6.424558e-29 1.742515e-10\n[216,] 4.497191e-29 1.219760e-10\n[217,] 3.148034e-29 8.538323e-11\n[218,] 2.203624e-29 5.976826e-11\n[219,] 1.542536e-29 4.183778e-11\n[220,] 1.079776e-29 2.928645e-11\n[221,] 7.558429e-30 2.050051e-11\n[222,] 5.290900e-30 1.435036e-11\n[223,] 3.703630e-30 1.004525e-11\n[224,] 2.592541e-30 7.031676e-12\n[225,] 1.814779e-30 4.922174e-12\n\n##No hay más outliers\ntotal_outliers2&lt;-cbind(xregSARIMA,xregSARIMA2)\nmodelo_SARIMA_ref_outliers2 = Arima(df_train4, c(5, 1, 1),seasonal = list(order = c(3, 1, 2), period = 12),lambda = 0,fixed=c(0,NA,0,0,0,NA,NA,NA,0,0,0,NA,0,0,NA,0,0,0,0,0,NA,0,NA,NA),xreg =total_outliers2 )\ncoeftest(modelo_SARIMA_ref_outliers2)\n\n\nz test of coefficients:\n\n        Estimate Std. Error  z value  Pr(&gt;|z|)    \nar2    0.2067029  0.0760131   2.7193  0.006542 ** \nma1   -0.9200445  0.0315473 -29.1640 &lt; 2.2e-16 ***\nsar1  -0.3471718  0.0684348  -5.0730 3.915e-07 ***\nsar2  -0.3588121  0.0653484  -5.4908 4.002e-08 ***\nAO50  -0.0168670  0.0055266  -3.0520  0.002273 ** \nAO88  -0.0524776  0.0056220  -9.3343 &lt; 2.2e-16 ***\nTC124 -0.0251921  0.0046302  -5.4408 5.304e-08 ***\nTC33   0.0142141  0.0046955   3.0272  0.002468 ** \nTC152 -0.0131171  0.0046450  -2.8239  0.004744 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nresiO= residuals(ARPURO_ref_prueba)\ncoefO= coefs2poly(ARPURO_ref_prueba)\noutliersSARIMA= locate.outliers(resiO,coefO,cval=4.5)\noutliersSARIMA\n\n  type ind    coefhat     tstat\n1   AO  88 -0.6796103 -8.871384\n3   AO 150  0.3644965  4.758003\n4   TC 124 -0.2226187 -4.580497\n5   TC 147  0.2434923  5.009983\n6   TC 148  0.2795131  5.751130\n\nn=length(df_train4)\nxregAR = outliers.effects(outliersSARIMA,n )\nxregAR\n\n       AO88 AO150        TC124        TC147        TC148\n  [1,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [2,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [3,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [4,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [5,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [6,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [7,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [8,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n  [9,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [10,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [11,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [12,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [13,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [14,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [15,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [16,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [17,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [18,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [19,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [20,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [21,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [22,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [23,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [24,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [25,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [26,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [27,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [28,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [29,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [30,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [31,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [32,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [33,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [34,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [35,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [36,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [37,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [38,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [39,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [40,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [41,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [42,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [43,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [44,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [45,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [46,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [47,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [48,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [49,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [50,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [51,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [52,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [53,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [54,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [55,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [56,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [57,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [58,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [59,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [60,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [61,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [62,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [63,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [64,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [65,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [66,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [67,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [68,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [69,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [70,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [71,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [72,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [73,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [74,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [75,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [76,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [77,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [78,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [79,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [80,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [81,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [82,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [83,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [84,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [85,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [86,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [87,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [88,]    1     0 0.000000e+00 0.000000e+00 0.000000e+00\n [89,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [90,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [91,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [92,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [93,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [94,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [95,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [96,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [97,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [98,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n [99,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[100,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[101,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[102,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[103,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[104,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[105,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[106,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[107,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[108,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[109,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[110,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[111,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[112,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[113,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[114,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[115,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[116,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[117,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[118,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[119,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[120,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[121,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[122,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[123,]    0     0 0.000000e+00 0.000000e+00 0.000000e+00\n[124,]    0     0 1.000000e+00 0.000000e+00 0.000000e+00\n[125,]    0     0 7.000000e-01 0.000000e+00 0.000000e+00\n[126,]    0     0 4.900000e-01 0.000000e+00 0.000000e+00\n[127,]    0     0 3.430000e-01 0.000000e+00 0.000000e+00\n[128,]    0     0 2.401000e-01 0.000000e+00 0.000000e+00\n[129,]    0     0 1.680700e-01 0.000000e+00 0.000000e+00\n[130,]    0     0 1.176490e-01 0.000000e+00 0.000000e+00\n[131,]    0     0 8.235430e-02 0.000000e+00 0.000000e+00\n[132,]    0     0 5.764801e-02 0.000000e+00 0.000000e+00\n[133,]    0     0 4.035361e-02 0.000000e+00 0.000000e+00\n[134,]    0     0 2.824752e-02 0.000000e+00 0.000000e+00\n[135,]    0     0 1.977327e-02 0.000000e+00 0.000000e+00\n[136,]    0     0 1.384129e-02 0.000000e+00 0.000000e+00\n[137,]    0     0 9.688901e-03 0.000000e+00 0.000000e+00\n[138,]    0     0 6.782231e-03 0.000000e+00 0.000000e+00\n[139,]    0     0 4.747562e-03 0.000000e+00 0.000000e+00\n[140,]    0     0 3.323293e-03 0.000000e+00 0.000000e+00\n[141,]    0     0 2.326305e-03 0.000000e+00 0.000000e+00\n[142,]    0     0 1.628414e-03 0.000000e+00 0.000000e+00\n[143,]    0     0 1.139890e-03 0.000000e+00 0.000000e+00\n[144,]    0     0 7.979227e-04 0.000000e+00 0.000000e+00\n[145,]    0     0 5.585459e-04 0.000000e+00 0.000000e+00\n[146,]    0     0 3.909821e-04 0.000000e+00 0.000000e+00\n[147,]    0     0 2.736875e-04 1.000000e+00 0.000000e+00\n[148,]    0     0 1.915812e-04 7.000000e-01 1.000000e+00\n[149,]    0     0 1.341069e-04 4.900000e-01 7.000000e-01\n[150,]    0     1 9.387480e-05 3.430000e-01 4.900000e-01\n[151,]    0     0 6.571236e-05 2.401000e-01 3.430000e-01\n[152,]    0     0 4.599865e-05 1.680700e-01 2.401000e-01\n[153,]    0     0 3.219906e-05 1.176490e-01 1.680700e-01\n[154,]    0     0 2.253934e-05 8.235430e-02 1.176490e-01\n[155,]    0     0 1.577754e-05 5.764801e-02 8.235430e-02\n[156,]    0     0 1.104428e-05 4.035361e-02 5.764801e-02\n[157,]    0     0 7.730994e-06 2.824752e-02 4.035361e-02\n[158,]    0     0 5.411696e-06 1.977327e-02 2.824752e-02\n[159,]    0     0 3.788187e-06 1.384129e-02 1.977327e-02\n[160,]    0     0 2.651731e-06 9.688901e-03 1.384129e-02\n[161,]    0     0 1.856212e-06 6.782231e-03 9.688901e-03\n[162,]    0     0 1.299348e-06 4.747562e-03 6.782231e-03\n[163,]    0     0 9.095437e-07 3.323293e-03 4.747562e-03\n[164,]    0     0 6.366806e-07 2.326305e-03 3.323293e-03\n[165,]    0     0 4.456764e-07 1.628414e-03 2.326305e-03\n[166,]    0     0 3.119735e-07 1.139890e-03 1.628414e-03\n[167,]    0     0 2.183814e-07 7.979227e-04 1.139890e-03\n[168,]    0     0 1.528670e-07 5.585459e-04 7.979227e-04\n[169,]    0     0 1.070069e-07 3.909821e-04 5.585459e-04\n[170,]    0     0 7.490483e-08 2.736875e-04 3.909821e-04\n[171,]    0     0 5.243338e-08 1.915812e-04 2.736875e-04\n[172,]    0     0 3.670337e-08 1.341069e-04 1.915812e-04\n[173,]    0     0 2.569236e-08 9.387480e-05 1.341069e-04\n[174,]    0     0 1.798465e-08 6.571236e-05 9.387480e-05\n[175,]    0     0 1.258926e-08 4.599865e-05 6.571236e-05\n[176,]    0     0 8.812479e-09 3.219906e-05 4.599865e-05\n[177,]    0     0 6.168735e-09 2.253934e-05 3.219906e-05\n[178,]    0     0 4.318115e-09 1.577754e-05 2.253934e-05\n[179,]    0     0 3.022680e-09 1.104428e-05 1.577754e-05\n[180,]    0     0 2.115876e-09 7.730994e-06 1.104428e-05\n[181,]    0     0 1.481113e-09 5.411696e-06 7.730994e-06\n[182,]    0     0 1.036779e-09 3.788187e-06 5.411696e-06\n[183,]    0     0 7.257455e-10 2.651731e-06 3.788187e-06\n[184,]    0     0 5.080219e-10 1.856212e-06 2.651731e-06\n[185,]    0     0 3.556153e-10 1.299348e-06 1.856212e-06\n[186,]    0     0 2.489307e-10 9.095437e-07 1.299348e-06\n[187,]    0     0 1.742515e-10 6.366806e-07 9.095437e-07\n[188,]    0     0 1.219760e-10 4.456764e-07 6.366806e-07\n[189,]    0     0 8.538323e-11 3.119735e-07 4.456764e-07\n[190,]    0     0 5.976826e-11 2.183814e-07 3.119735e-07\n[191,]    0     0 4.183778e-11 1.528670e-07 2.183814e-07\n[192,]    0     0 2.928645e-11 1.070069e-07 1.528670e-07\n[193,]    0     0 2.050051e-11 7.490483e-08 1.070069e-07\n[194,]    0     0 1.435036e-11 5.243338e-08 7.490483e-08\n[195,]    0     0 1.004525e-11 3.670337e-08 5.243338e-08\n[196,]    0     0 7.031676e-12 2.569236e-08 3.670337e-08\n[197,]    0     0 4.922174e-12 1.798465e-08 2.569236e-08\n[198,]    0     0 3.445521e-12 1.258926e-08 1.798465e-08\n[199,]    0     0 2.411865e-12 8.812479e-09 1.258926e-08\n[200,]    0     0 1.688306e-12 6.168735e-09 8.812479e-09\n[201,]    0     0 1.181814e-12 4.318115e-09 6.168735e-09\n[202,]    0     0 8.272697e-13 3.022680e-09 4.318115e-09\n[203,]    0     0 5.790888e-13 2.115876e-09 3.022680e-09\n[204,]    0     0 4.053622e-13 1.481113e-09 2.115876e-09\n[205,]    0     0 2.837535e-13 1.036779e-09 1.481113e-09\n[206,]    0     0 1.986275e-13 7.257455e-10 1.036779e-09\n[207,]    0     0 1.390392e-13 5.080219e-10 7.257455e-10\n[208,]    0     0 9.732745e-14 3.556153e-10 5.080219e-10\n[209,]    0     0 6.812922e-14 2.489307e-10 3.556153e-10\n[210,]    0     0 4.769045e-14 1.742515e-10 2.489307e-10\n[211,]    0     0 3.338332e-14 1.219760e-10 1.742515e-10\n[212,]    0     0 2.336832e-14 8.538323e-11 1.219760e-10\n[213,]    0     0 1.635783e-14 5.976826e-11 8.538323e-11\n[214,]    0     0 1.145048e-14 4.183778e-11 5.976826e-11\n[215,]    0     0 8.015334e-15 2.928645e-11 4.183778e-11\n[216,]    0     0 5.610734e-15 2.050051e-11 2.928645e-11\n[217,]    0     0 3.927514e-15 1.435036e-11 2.050051e-11\n[218,]    0     0 2.749260e-15 1.004525e-11 1.435036e-11\n[219,]    0     0 1.924482e-15 7.031676e-12 1.004525e-11\n[220,]    0     0 1.347137e-15 4.922174e-12 7.031676e-12\n[221,]    0     0 9.429961e-16 3.445521e-12 4.922174e-12\n[222,]    0     0 6.600972e-16 2.411865e-12 3.445521e-12\n[223,]    0     0 4.620681e-16 1.688306e-12 2.411865e-12\n[224,]    0     0 3.234477e-16 1.181814e-12 1.688306e-12\n[225,]    0     0 2.264134e-16 8.272697e-13 1.181814e-12\nARPURO_ref_prueba_outliers=Arima(ElimiTenddian_STL_dummy,order=c(1,0,0),seasonal = c(1,0,0),include.mean = F,xreg=xregAR,fixed=c(NA,NA,NA,NA,NA,NA,0))\n#coeftest(modelo_AR_ref_outliers)\nresiO= residuals( ARPURO_ref_prueba_outliers)\ncoefO= coefs2poly( ARPURO_ref_prueba_outliers)\noutliersSARIMA= locate.outliers(resiO,coefO,cval=4.5)\noutliersSARIMA\n\n  type ind   coefhat    tstat\n1   AO  89 0.4699829 7.420944\n\nn=length(df_train4)\nxregAR2 = outliers.effects(outliersSARIMA,n )\nxregAR2\n\n       AO89\n  [1,]    0\n  [2,]    0\n  [3,]    0\n  [4,]    0\n  [5,]    0\n  [6,]    0\n  [7,]    0\n  [8,]    0\n  [9,]    0\n [10,]    0\n [11,]    0\n [12,]    0\n [13,]    0\n [14,]    0\n [15,]    0\n [16,]    0\n [17,]    0\n [18,]    0\n [19,]    0\n [20,]    0\n [21,]    0\n [22,]    0\n [23,]    0\n [24,]    0\n [25,]    0\n [26,]    0\n [27,]    0\n [28,]    0\n [29,]    0\n [30,]    0\n [31,]    0\n [32,]    0\n [33,]    0\n [34,]    0\n [35,]    0\n [36,]    0\n [37,]    0\n [38,]    0\n [39,]    0\n [40,]    0\n [41,]    0\n [42,]    0\n [43,]    0\n [44,]    0\n [45,]    0\n [46,]    0\n [47,]    0\n [48,]    0\n [49,]    0\n [50,]    0\n [51,]    0\n [52,]    0\n [53,]    0\n [54,]    0\n [55,]    0\n [56,]    0\n [57,]    0\n [58,]    0\n [59,]    0\n [60,]    0\n [61,]    0\n [62,]    0\n [63,]    0\n [64,]    0\n [65,]    0\n [66,]    0\n [67,]    0\n [68,]    0\n [69,]    0\n [70,]    0\n [71,]    0\n [72,]    0\n [73,]    0\n [74,]    0\n [75,]    0\n [76,]    0\n [77,]    0\n [78,]    0\n [79,]    0\n [80,]    0\n [81,]    0\n [82,]    0\n [83,]    0\n [84,]    0\n [85,]    0\n [86,]    0\n [87,]    0\n [88,]    0\n [89,]    1\n [90,]    0\n [91,]    0\n [92,]    0\n [93,]    0\n [94,]    0\n [95,]    0\n [96,]    0\n [97,]    0\n [98,]    0\n [99,]    0\n[100,]    0\n[101,]    0\n[102,]    0\n[103,]    0\n[104,]    0\n[105,]    0\n[106,]    0\n[107,]    0\n[108,]    0\n[109,]    0\n[110,]    0\n[111,]    0\n[112,]    0\n[113,]    0\n[114,]    0\n[115,]    0\n[116,]    0\n[117,]    0\n[118,]    0\n[119,]    0\n[120,]    0\n[121,]    0\n[122,]    0\n[123,]    0\n[124,]    0\n[125,]    0\n[126,]    0\n[127,]    0\n[128,]    0\n[129,]    0\n[130,]    0\n[131,]    0\n[132,]    0\n[133,]    0\n[134,]    0\n[135,]    0\n[136,]    0\n[137,]    0\n[138,]    0\n[139,]    0\n[140,]    0\n[141,]    0\n[142,]    0\n[143,]    0\n[144,]    0\n[145,]    0\n[146,]    0\n[147,]    0\n[148,]    0\n[149,]    0\n[150,]    0\n[151,]    0\n[152,]    0\n[153,]    0\n[154,]    0\n[155,]    0\n[156,]    0\n[157,]    0\n[158,]    0\n[159,]    0\n[160,]    0\n[161,]    0\n[162,]    0\n[163,]    0\n[164,]    0\n[165,]    0\n[166,]    0\n[167,]    0\n[168,]    0\n[169,]    0\n[170,]    0\n[171,]    0\n[172,]    0\n[173,]    0\n[174,]    0\n[175,]    0\n[176,]    0\n[177,]    0\n[178,]    0\n[179,]    0\n[180,]    0\n[181,]    0\n[182,]    0\n[183,]    0\n[184,]    0\n[185,]    0\n[186,]    0\n[187,]    0\n[188,]    0\n[189,]    0\n[190,]    0\n[191,]    0\n[192,]    0\n[193,]    0\n[194,]    0\n[195,]    0\n[196,]    0\n[197,]    0\n[198,]    0\n[199,]    0\n[200,]    0\n[201,]    0\n[202,]    0\n[203,]    0\n[204,]    0\n[205,]    0\n[206,]    0\n[207,]    0\n[208,]    0\n[209,]    0\n[210,]    0\n[211,]    0\n[212,]    0\n[213,]    0\n[214,]    0\n[215,]    0\n[216,]    0\n[217,]    0\n[218,]    0\n[219,]    0\n[220,]    0\n[221,]    0\n[222,]    0\n[223,]    0\n[224,]    0\n[225,]    0\ntotal_outliers=cbind(xregAR,xregAR2)\n ARPURO_ref_prueba_outliers2=Arima(ElimiTenddian_STL_dummy,order=c(1,0,0),seasonal = c(1,0,0),include.mean = F,xreg=total_outliers,fixed=c(NA,NA,NA,NA,NA,NA,0,NA))\ncoeftest( ARPURO_ref_prueba_outliers2)\n\n\nz test of coefficients:\n\n       Estimate Std. Error z value  Pr(&gt;|z|)    \nar1   -0.160468   0.066645 -2.4078 0.0160491 *  \nsar1   0.628647   0.051024 12.3206 &lt; 2.2e-16 ***\nAO88  -0.778931   0.078773 -9.8883 &lt; 2.2e-16 ***\nAO150  0.252164   0.082155  3.0694 0.0021452 ** \nTC124 -0.236201   0.050844 -4.6456 3.391e-06 ***\nTC147  0.178754   0.053271  3.3556 0.0007921 ***\nAO89   0.494072   0.079127  6.2441 4.263e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Descriptivo Dian (2).html#eliminación-de-la-estacionalidad-utilizando-variables-dummy",
    "href": "Descriptivo Dian (2).html#eliminación-de-la-estacionalidad-utilizando-variables-dummy",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "1.5 Eliminación de la estacionalidad utilizando variables dummy",
    "text": "1.5 Eliminación de la estacionalidad utilizando variables dummy\n\n## Prueba de crear las variables dummy \nlibrary(stats)\nlibrary(forecast)\nlibrary(ggplot2)\n\n# Carga tus datos o crea una serie de tiempo similar\n# Puedes cargar tus datos desde un archivo o crearlos manualmente\n# Aquí se asume que tienes una serie de tiempo en un objeto llamado 'dian_detrend_STL'\n\n# Crea un objeto de variable dummy estacional\ndummy &lt;- seasonaldummy(ElimiTenddian_STL)\n\n# Ajusta un modelo de regresión lineal\nmodelo &lt;- lm(ElimiTenddian_STL ~ dummy)\n\n# Obtiene un resumen del modelo\nsummary(modelo)\n\n\nCall:\nlm(formula = ElimiTenddian_STL ~ dummy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73461 -0.07125 -0.00763  0.07523  0.54817 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.43056    0.03010 -14.304  &lt; 2e-16 ***\ndummyJan     0.77121    0.04212  18.308  &lt; 2e-16 ***\ndummyFeb     0.21961    0.04212   5.213 3.69e-07 ***\ndummyMar     0.40270    0.04212   9.560  &lt; 2e-16 ***\ndummyApr     0.68980    0.04212  16.375  &lt; 2e-16 ***\ndummyMay     0.63959    0.04212  15.183  &lt; 2e-16 ***\ndummyJun     0.64048    0.04212  15.205  &lt; 2e-16 ***\ndummyJul     0.47789    0.04257  11.226  &lt; 2e-16 ***\ndummyAug     0.21469    0.04257   5.043 8.40e-07 ***\ndummySep     0.68424    0.04257  16.073  &lt; 2e-16 ***\ndummyOct     0.05631    0.04257   1.323    0.187    \ndummyNov     0.50827    0.04257  11.940  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1444 on 270 degrees of freedom\nMultiple R-squared:  0.7594,    Adjusted R-squared:  0.7496 \nF-statistic: 77.49 on 11 and 270 DF,  p-value: &lt; 2.2e-16\n\n# Realiza predicciones con el modelo\nnuevas_obs &lt;- length(ElimiTenddian_STL)  # Número de observaciones en la serie de tiempo\npredicciones &lt;- predict(modelo, newdata = data.frame(dummy = dummy))\n\n# Crea un nuevo objeto de serie de tiempo con las predicciones\ndian_pred &lt;- ts(predicciones, start = start(ElimiTenddian_STL), frequency = frequency(ElimiTenddian_STL))\n\n# Grafica los resultados\nplot(ElimiTenddian_STL, col = \"blue\", type = \"l\", xlab = \"Fecha\", ylab = \"Valor Original\")\nlines(dian_pred, col = \"red\")\nlegend(\"topleft\", legend = c(\"Impuestos\", \"Predicciones\"), col = c(\"blue\", \"red\"))\n\n\n\n\n\nplot(ElimiTenddian_STL-dian_pred, col = \"blue\", type = \"l\", xlab = \"Fecha\", ylab = \"Impuestos\",main=\"Series sin tendencia (STL) y sin estacionalidad (dummy)\")\n\n\n\n\n\nacf(as.numeric(ElimiTenddian_STL-dian_pred))"
  },
  {
    "objectID": "Descriptivo Dian (2).html#division-de-los-datos-en-entrenamiento-y-prueba.",
    "href": "Descriptivo Dian (2).html#division-de-los-datos-en-entrenamiento-y-prueba.",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "2.1 Division de los datos en entrenamiento y prueba.",
    "text": "2.1 Division de los datos en entrenamiento y prueba.\nEn primer lugar se realizará la división de los datos en el conjunto de entrenamiento y de prueba.\n\nlserie=length(ldian2)\nntrain=trunc(length(ldian2)*0.80) ##% del datos en el conjunto de entrenamiento es del 85%.\nntrain\n\n[1] 225\n\ntime(ldian2)\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2000 2000.000 2000.083 2000.167 2000.250 2000.333 2000.417 2000.500 2000.583\n2001 2001.000 2001.083 2001.167 2001.250 2001.333 2001.417 2001.500 2001.583\n2002 2002.000 2002.083 2002.167 2002.250 2002.333 2002.417 2002.500 2002.583\n2003 2003.000 2003.083 2003.167 2003.250 2003.333 2003.417 2003.500 2003.583\n2004 2004.000 2004.083 2004.167 2004.250 2004.333 2004.417 2004.500 2004.583\n2005 2005.000 2005.083 2005.167 2005.250 2005.333 2005.417 2005.500 2005.583\n2006 2006.000 2006.083 2006.167 2006.250 2006.333 2006.417 2006.500 2006.583\n2007 2007.000 2007.083 2007.167 2007.250 2007.333 2007.417 2007.500 2007.583\n2008 2008.000 2008.083 2008.167 2008.250 2008.333 2008.417 2008.500 2008.583\n2009 2009.000 2009.083 2009.167 2009.250 2009.333 2009.417 2009.500 2009.583\n2010 2010.000 2010.083 2010.167 2010.250 2010.333 2010.417 2010.500 2010.583\n2011 2011.000 2011.083 2011.167 2011.250 2011.333 2011.417 2011.500 2011.583\n2012 2012.000 2012.083 2012.167 2012.250 2012.333 2012.417 2012.500 2012.583\n2013 2013.000 2013.083 2013.167 2013.250 2013.333 2013.417 2013.500 2013.583\n2014 2014.000 2014.083 2014.167 2014.250 2014.333 2014.417 2014.500 2014.583\n2015 2015.000 2015.083 2015.167 2015.250 2015.333 2015.417 2015.500 2015.583\n2016 2016.000 2016.083 2016.167 2016.250 2016.333 2016.417 2016.500 2016.583\n2017 2017.000 2017.083 2017.167 2017.250 2017.333 2017.417 2017.500 2017.583\n2018 2018.000 2018.083 2018.167 2018.250 2018.333 2018.417 2018.500 2018.583\n2019 2019.000 2019.083 2019.167 2019.250 2019.333 2019.417 2019.500 2019.583\n2020 2020.000 2020.083 2020.167 2020.250 2020.333 2020.417 2020.500 2020.583\n2021 2021.000 2021.083 2021.167 2021.250 2021.333 2021.417 2021.500 2021.583\n2022 2022.000 2022.083 2022.167 2022.250 2022.333 2022.417 2022.500 2022.583\n2023 2023.000 2023.083 2023.167 2023.250 2023.333 2023.417                  \n          Sep      Oct      Nov      Dec\n2000 2000.667 2000.750 2000.833 2000.917\n2001 2001.667 2001.750 2001.833 2001.917\n2002 2002.667 2002.750 2002.833 2002.917\n2003 2003.667 2003.750 2003.833 2003.917\n2004 2004.667 2004.750 2004.833 2004.917\n2005 2005.667 2005.750 2005.833 2005.917\n2006 2006.667 2006.750 2006.833 2006.917\n2007 2007.667 2007.750 2007.833 2007.917\n2008 2008.667 2008.750 2008.833 2008.917\n2009 2009.667 2009.750 2009.833 2009.917\n2010 2010.667 2010.750 2010.833 2010.917\n2011 2011.667 2011.750 2011.833 2011.917\n2012 2012.667 2012.750 2012.833 2012.917\n2013 2013.667 2013.750 2013.833 2013.917\n2014 2014.667 2014.750 2014.833 2014.917\n2015 2015.667 2015.750 2015.833 2015.917\n2016 2016.667 2016.750 2016.833 2016.917\n2017 2017.667 2017.750 2017.833 2017.917\n2018 2018.667 2018.750 2018.833 2018.917\n2019 2019.667 2019.750 2019.833 2019.917\n2020 2020.667 2020.750 2020.833 2020.917\n2021 2021.667 2021.750 2021.833 2021.917\n2022 2022.667 2022.750 2022.833 2022.917\n2023                                    \n\ntime(ldian2)[ntrain]###Me entrega la ultima fecha de la posición ntrain\n\n[1] 2018.667\n\ntrain=window(ldian2,end=time(ldian2)[ntrain])\ntest=window(ldian2,start=time(ldian2)[ntrain]+1/12)##1/12 porque es la fracción que corresponde a un mes\nlength(train)\n\n[1] 225\n\nntest=length(test)\nntest ##Me define el valor de origins, o de ventanas de rolling.\n\n[1] 57\n\nlserie ### Comparar los valores\n\n[1] 282\n\n\nLuego de dividir los datos procedemos a crear algunos vectores necesarios para realizar el rolling y posteriormente realizaremos el modelo sobre los datos de entrenamiento. ## Modelamiento SE.\n\nh=1\nfchstepahe=matrix(0,nrow=ntest,ncol=h) ##Crea una Columna para los h-pasos adelante\n### verval contiene los verdaderos valores de la serie en el conjunto de prueba con los que se compararán los pronósticos.\nverval=cbind(test[1:ntest])\nfor(j in 2:h){\n  verval=cbind(verval,c(test[j:ntest],rep(NA,j-1)))\n}\n\nverval=cbind(test[1:ntest],c(test[2:ntest],NA),c(test[3:ntest],NA,NA))\n####Ajuste del modelo con los datos de entrenamiento\nHWAP_train=stats::HoltWinters(train,seasonal=\"additive\")\nHWAP_train$alpha\n\n     alpha \n0.09446078 \n\nHWAP_train$beta\n\n      beta \n0.06537417 \n\nHWAP_train$gamma\n\n    gamma \n0.4091491 \n\n\n\n2.1.1 Análisis de reisulaes\nEl análisis de los residuales se hace sobre el conjunto de entrenamiento del siguiente modo.\n\nlibrary(stats)\n# Obtener residuales del modelo Holt-Winters\nresiduales_hw &lt;- residuals(HWAP_train)\n# Visualizar los residuales en un gráfico\nplot(residuales_hw, type = \"l\", col = \"blue\", ylab = \"Residuales\", main = \"Residuales del modelo Holt-Winters\")\n\n\n\nacf(as.numeric(residuales_hw))\n\n\n\npacf(as.numeric(residuales_hw))\n\n\n\n#Test de Ljung-Box\n# Longitud de los residuos dividida por 4\nlongitud_dividida &lt;- length(residuales_hw) / 4\ncat(\"Longitud de los residuos dividida por 4:\", longitud_dividida, \"\\n\")\n\nLongitud de los residuos dividida por 4: 53.25 \n\n# Raíz cuadrada de la longitud de los residuos\nraiz_cuadrada &lt;- sqrt(length(residuales_hw))\ncat(\"Raíz cuadrada de la longitud de los residuos:\", raiz_cuadrada, \"\\n\")\n\nRaíz cuadrada de la longitud de los residuos: 14.59452 \n\n# Test de Ljung-Box para autocorrelación\nlibrary(stats)\n# Lags a considerar (24 en este caso)\nlags &lt;- 24\n# Realizar el test de Ljung-Box\nljung_box_test &lt;- Box.test(residuales_hw, lag = lags, type = \"Ljung-Box\", fitdf = 0)\ncat(\"Estadística de Ljung-Box:\", ljung_box_test$statistic, \"\\n\")\n\nEstadística de Ljung-Box: 28.16708 \n\ncat(\"P-valor:\", ljung_box_test$p.value, \"\\n\")\n\nP-valor: 0.2530558 \n\n\nEn este caso es posible observar que al revisar las graficas del ACf y PACF no se evidencian que se tengan cosas por explicar, además la prueba de Ljung-Box, nos da un p valor de 0.2530558 el cual es más grande que un valor \\(\\alpha=0.05\\), es decir que existe evidencia estadisticamente significativa para rechazar la hipotesis de correlación, es decir que los residuales no parecen estar correlacionados. Luego de esto procederemos a realizar Rolling para conocer la capacidad predictiva del modelo. ### Rolling SE\n\n##Rolling\nfor(i in 1:(ntest))\n{\n  x=window(ldian2,end=time(ldian2)[ntrain]+(i-1)/12)\n  print(length(x))\n  refit=stats::HoltWinters(x,alpha=0.09446078,beta=0.06537417 ,gamma=0.4091491 ,seasonal=\"additive\")\n    fchstepahe[i,]=as.numeric(forecast::forecast(refit,h=h)$mean)\n}\n\n[1] 225\n[1] 226\n[1] 227\n[1] 228\n[1] 229\n[1] 230\n[1] 231\n[1] 232\n[1] 233\n[1] 234\n[1] 235\n[1] 236\n[1] 237\n[1] 238\n[1] 239\n[1] 240\n[1] 241\n[1] 242\n[1] 243\n[1] 244\n[1] 245\n[1] 246\n[1] 247\n[1] 248\n[1] 249\n[1] 250\n[1] 251\n[1] 252\n[1] 253\n[1] 254\n[1] 255\n[1] 256\n[1] 257\n[1] 258\n[1] 259\n[1] 260\n[1] 261\n[1] 262\n[1] 263\n[1] 264\n[1] 265\n[1] 266\n[1] 267\n[1] 268\n[1] 269\n[1] 270\n[1] 271\n[1] 272\n[1] 273\n[1] 274\n[1] 275\n[1] 276\n[1] 277\n[1] 278\n[1] 279\n[1] 280\n[1] 281\n\nfchstepahe\n\n          [,1]\n [1,] 15.59525\n [2,] 16.14393\n [3,] 15.60399\n [4,] 16.55152\n [5,] 15.75755\n [6,] 16.11598\n [7,] 16.46955\n [8,] 16.44372\n [9,] 16.41609\n[10,] 16.21793\n[11,] 15.77656\n[12,] 16.53835\n[13,] 15.75931\n[14,] 16.27512\n[15,] 15.88808\n[16,] 16.61020\n[17,] 15.82943\n[18,] 16.19280\n[19,] 16.46533\n[20,] 16.46223\n[21,] 16.38716\n[22,] 16.24423\n[23,] 15.75905\n[24,] 16.49863\n[25,] 15.71893\n[26,] 16.21610\n[27,] 15.79271\n[28,] 16.56674\n[29,] 15.77398\n[30,] 16.10388\n[31,] 16.20957\n[32,] 16.35066\n[33,] 16.31956\n[34,] 16.18904\n[35,] 15.78278\n[36,] 16.53065\n[37,] 15.83829\n[38,] 16.36082\n[39,] 15.89946\n[40,] 16.70520\n[41,] 15.91163\n[42,] 16.29069\n[43,] 16.37797\n[44,] 16.57237\n[45,] 16.47571\n[46,] 16.45082\n[47,] 16.07613\n[48,] 16.80589\n[49,] 16.12452\n[50,] 16.67698\n[51,] 16.21919\n[52,] 16.98076\n[53,] 16.19589\n[54,] 16.59437\n[55,] 16.73014\n[56,] 16.86376\n[57,] 16.74383\n\nerrores_pred=exp(verval[,1]) -exp(fchstepahe) ##Observación: debo devolver los pronósticos y los verdaderos valores a la escala original si es necesario.\nECM=apply(errores_pred^2,MARGIN = 2,mean,na.rm=TRUE) ##Acá se computa la medida de precisión del pronóstico(en este caso ECM).\nRECM=sqrt(ECM) ##Se le saca raíz \nRECM ##se lee: Primera fila RECM 1-paso adelante y así sucesivamente.\n\n[1] 2143759\n\n\nDe este modo se obtuvo un valor ECM de 4.595703e+12 (Billones) y un valor RECM de 2143759 (Millones) ### Predicción usando el modelo SE.\n\nverval_ts&lt;-ts(exp(verval[,1]),start=time(ldian2)[ntrain]+1/12,frequency=12)\nfchstepahe_ts&lt;-ts(exp(fchstepahe),start=time(ldian2)[ntrain]+1/12,frequency=12)\n\nplot(verval_ts, col = \"blue\", ylab = \"Impuestos\", xlab = \"Tiempo\")\nlines(fchstepahe_ts, col = \"red\")\nlegend(\"topright\", legend = c(\"Reales\", \"Predicciones\"), col = c(\"blue\", \"red\"), lty = 1)"
  },
  {
    "objectID": "Descriptivo Dian (2).html#modelamiento-arma-con-componentes-de-fourier.",
    "href": "Descriptivo Dian (2).html#modelamiento-arma-con-componentes-de-fourier.",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "3.1 Modelamiento ARMA con componentes de fourier.",
    "text": "3.1 Modelamiento ARMA con componentes de fourier.\nEn primer lugar iniciamos utilizando la serie sin tendencia (via STL), con varianza estable y con estacionalidad eliminada (via componentes de fourier), se observaran su gráficos acf y su gráfico pacf para encontrar los posibles ordens p y q, para realizar los modelamientos.\n\nElimiTenddian_STL_fourier&lt;-ElimiTenddian_STL-results_ciclo_ts\n\nacf(as.numeric(ElimiTenddian_STL_fourier),main=\"Series estacionaria (STL-Fourier)\",lag.max = length(dian2)/4)\n\n\n\nacf(as.numeric(ElimiTenddian_STL_fourier),main=\"Series estacionaria (STL-Fourier)\",lag.max = length(dian2)/4,ci.type='ma')## Para el MA\n\n\n\npacf(as.numeric(ElimiTenddian_STL_fourier),main=\"Series estacionaria (STL-Fourier)\",lag.max = length(dian2)/4)\n\n\n\n\n\nrequire(feasts)\ndian2_tsbl_notend=as_tsibble(ElimiTenddian_STL_fourier)\ndian2_tsbl_notend%&gt;%gg_subseries(value)\n\n\n\n\nEste gráfico nos indica que la series sin tendencia via STL y sin estacionalidad via componentes de fourier no hace que nuestra series de tiempo sea estacionaria, esto puede deberse a que las componentes de fourier no ayudaron a estimar de modo correcto la estacionalidad de este proceso. ## Modelamiento ARMA con dummy. Teniendo en cuenta que el metodo que mejor estimó la estacionalidad fue el metodo de variables dummy, se procederá a realizar el modelado de 3 modelos, AR,MA y ARMA, luego procederemos a compararlos en terminos de su capacidad predictiva (ECM). ## División de la base de datos en entrenamiento, validación y prueba.\n\ntrain_weight2 &lt;- 0.8\nsplit2 &lt;- as.integer(length(ElimiTenddian_STL) * train_weight2)\n#window(ldian2,end=time(ldian2)[ntrain])\ndf_train2 &lt;- window(ElimiTenddian_STL, end = time(ElimiTenddian_STL)[split2])#80%\ndf_test2 &lt;- window(ElimiTenddian_STL, start = time(ElimiTenddian_STL)[split2] + 1/12,)#20%"
  },
  {
    "objectID": "Descriptivo Dian (2).html#modelo-ar",
    "href": "Descriptivo Dian (2).html#modelo-ar",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "3.2 Modelo AR",
    "text": "3.2 Modelo AR\nEn primer lugar es necesario realizar la estimación de las variables dummy sobre el conjunto de prueba lo cual se va a hacer a continuación. ### Variables dummy\n\n## Prueba de crear las variables dummy \nlibrary(stats)\nlibrary(forecast)\nlibrary(ggplot2)\n\n# Carga tus datos o crea una serie de tiempo similar\n# Puedes cargar tus datos desde un archivo o crearlos manualmente\n# Aquí se asume que tienes una serie de tiempo en un objeto llamado 'dian_detrend_STL'\n# Crea un objeto de variable dummy estacional\ndummy &lt;- seasonaldummy(df_train2)\n# Ajusta un modelo de regresión lineal\nmodelo &lt;- lm(df_train2 ~ dummy)\n# Obtiene un resumen del modelo\nsummary(modelo)\n\n\nCall:\nlm(formula = df_train2 ~ dummy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.76802 -0.05968 -0.00249  0.08298  0.51336 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.46507    0.03431 -13.556  &lt; 2e-16 ***\ndummyJan     0.78175    0.04788  16.329  &lt; 2e-16 ***\ndummyFeb     0.29694    0.04788   6.202 2.85e-09 ***\ndummyMar     0.43254    0.04788   9.035  &lt; 2e-16 ***\ndummyApr     0.75773    0.04788  15.827  &lt; 2e-16 ***\ndummyMay     0.68003    0.04788  14.204  &lt; 2e-16 ***\ndummyJun     0.70980    0.04788  14.826  &lt; 2e-16 ***\ndummyJul     0.50649    0.04788  10.579  &lt; 2e-16 ***\ndummyAug     0.28110    0.04788   5.872 1.64e-08 ***\ndummySep     0.71053    0.04788  14.841  &lt; 2e-16 ***\ndummyOct     0.09436    0.04852   1.945   0.0531 .  \ndummyNov     0.53051    0.04852  10.934  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1456 on 213 degrees of freedom\nMultiple R-squared:  0.7598,    Adjusted R-squared:  0.7474 \nF-statistic: 61.26 on 11 and 213 DF,  p-value: &lt; 2.2e-16\n\n# Realiza predicciones con el modelo\nnuevas_obs &lt;- length(df_train2)  # Número de observaciones en la serie de tiempo\npredicciones &lt;- predict(modelo, newdata = data.frame(dummy = dummy))\n# Crea un nuevo objeto de serie de tiempo con las predicciones\ndian_pred_train &lt;- ts(predicciones, start = start(df_train2), frequency = frequency(df_train2))\n\n# Grafica los resultados\nplot(df_train2, col = \"blue\", type = \"l\", xlab = \"Fecha\", ylab = \"Valor Original\")\nlines(dian_pred_train, col = \"red\")\nlegend(\"topleft\", legend = c(\"Impuestos\", \"Predicciones\"), col = c(\"blue\", \"red\"))\n\n\n\n\n\n#Eliminando la estacionalidad.\nElimiTenddian_STL_dummy&lt;-df_train2-dian_pred_train\nplot(ElimiTenddian_STL_dummy,main=\"Datos estacionarios\")\n\n\n\nacf(as.numeric(ElimiTenddian_STL_dummy),main=\"ACF datos estacionarios\",lag.max = length(ElimiTenddian_STL_dummy)/4)\n\n\n\nacf(as.numeric(ElimiTenddian_STL_dummy),main=\"ACF datos estacionarios\",lag.max = length(ElimiTenddian_STL_dummy)/4,ci.type='ma')\n\n\n\npacf(as.numeric(ElimiTenddian_STL_dummy),main=\"PACF datos estacionarios\",lag.max = length(ElimiTenddian_STL_dummy)/4)\n\n\n\nrequire(feasts)\ndian2_tsbl_notend=as_tsibble(ElimiTenddian_STL_dummy)\ndian2_tsbl_notend%&gt;%gg_subseries(value)\n\n\n\n\n\n3.2.1 Ajustando el modelo AR(12)\nAl observar los graficos del acf y el pacf es posible observar que el ACF desciende lentamente hacia 0, mientras que en el PACF se observa que el rezago 12 es significativamente diferente de 0 y rezagos más grandes no parecen ser significativamente distintos de 0, por esta razon nos decantamos por construir un modelo de tipo AR(12)\n\nlibrary(lmtest)\nARPURO=Arima(ElimiTenddian_STL_dummy,order=c(12,0,0),include.mean = TRUE)\ncoeftest(ARPURO)\n\n\nz test of coefficients:\n\n             Estimate  Std. Error z value  Pr(&gt;|z|)    \nar1       -0.17658703  0.05979452 -2.9532  0.003145 ** \nar2        0.01322007  0.06117696  0.2161  0.828913    \nar3       -0.09495073  0.06104447 -1.5554  0.119842    \nar4       -0.10611084  0.06137399 -1.7289  0.083823 .  \nar5       -0.06870130  0.06198604 -1.1083  0.267717    \nar6       -0.04956787  0.06153850 -0.8055  0.420544    \nar7       -0.06398874  0.06218240 -1.0290  0.303457    \nar8       -0.02486263  0.06232279 -0.3989  0.689942    \nar9       -0.05688534  0.06197595 -0.9179  0.358691    \nar10      -0.02991973  0.06168740 -0.4850  0.627661    \nar11      -0.00779959  0.06162111 -0.1266  0.899278    \nar12       0.42095115  0.05995852  7.0207 2.208e-12 ***\nintercept -0.00056137  0.00645641 -0.0869  0.930713    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n##Refinamiento del modelo \nARPURO_ref=Arima(ElimiTenddian_STL_dummy,order=c(12,0,0),include.mean = TRUE,\n                 fixed=c(NA,0,0,0,0,0,0,0,0,0,0,NA,0))\n### Modelo más parsimonioso que el anterior.\nARPURO_ref_prueba=Arima(ElimiTenddian_STL_dummy,order=c(1,0,0),seasonal = c(1,0,0),include.mean = F)\ncoeftest(ARPURO_ref)\n\n\nz test of coefficients:\n\n      Estimate Std. Error z value  Pr(&gt;|z|)    \nar1  -0.162738   0.057542 -2.8282  0.004681 ** \nar12  0.446109   0.057620  7.7423 9.764e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(ARPURO_ref_prueba)\n\n\nz test of coefficients:\n\n      Estimate Std. Error z value Pr(&gt;|z|)    \nar1  -0.210490   0.065031 -3.2368 0.001209 ** \nsar1  0.463263   0.058270  7.9503 1.86e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAl realizar el refinamiento es posible observar que unicamente se encontraron como significativos los parametros asociados con los retardos 1 y 12. Posteriormente se va a realizar el análisis de residuales del modelo AR(12). ### Verificación de supusetos modelo AR\n\n# Análisis de residuales\nresiduales=ARPURO_ref_prueba$residuals\nplot(residuales)\n\n\n\nacf(residuales)\n\n\n\nacf(residuales^2)\n\n\n\npacf(residuales)\n\n\n\n\nEs posible observar con los graficos para el ACf y el PACF que no parece que quede algo por explicar dentro de los residuales de este modelo ajustado, lo cual es un buen indicio.\n\n#Test de normalidad\ntseries::jarque.bera.test(residuales)\n\n\n    Jarque Bera Test\n\ndata:  residuales\nX-squared = 973.46, df = 2, p-value &lt; 2.2e-16\n\n#Test de autocorrelación\nlength(residuales)/4\n\n[1] 56.25\n\nsqrt(length(residuales))\n\n[1] 15\n\nBox.test(residuales, lag =17 , type = \"Ljung-Box\", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!\n\n\n    Box-Ljung test\n\ndata:  residuales\nX-squared = 9.6551, df = 15, p-value = 0.8409\n\n\nEn este caso se tiene que el test de normalidad dado por el test de jarque bera, tiene un p valor de 2.2e-16, el cual es menor que un valor alpha del 0.05, por lo tanto existe suficiente evidencia estadistica para rechazar la hipotesis de normalidad. En el caso de la prueba de autocorrelación se obtiene un p valor de 0.8409, lo cual indica que no hay suficiente evidencia para afirmar que hay autocorrelación significativa en los residuos.\n\n###Estad?sticas CUSUM\nres=residuales\ncum=cumsum(res)/sd(res)\nN=length(res)\ncumq=cumsum(res^2)/sum(res^2)\nAf=0.948 ###Cuantil del 95% para la estad?stica cusum\nco=0.10997####Valor del cuantil aproximado para cusumsq para n/2\nLS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)\nLI=-LS\nLQS=co+(1:length(res))/N\nLQI=-co+(1:length(res))/N\nplot(cum,type=\"l\",ylim=c(min(LI),max(LS)),xlab=\"t\",ylab=\"\",main=\"CUSUM\")\nlines(LS,type=\"S\",col=\"red\")\nlines(LI,type=\"S\",col=\"red\")\n\n\n\n#CUSUMSQ\nplot(cumq,type=\"l\",xlab=\"t\",ylab=\"\",main=\"CUSUMSQ\")                      \nlines(LQS,type=\"S\",col=\"red\")                                                                           \nlines(LQI,type=\"S\",col=\"red\")\n\n\n\n\nEste gráfico mide la variabilidad de los residuales, en este caso se tiene que se sale un poco la variabilidad de las bandas de confianza, pero esto no ocurre por un largo periodo de tiempo.\n\n\n3.2.2 Rolling AR()\n\n# rolling \n#ElimiTenddian_STL-dian_pred\nh=1\nfcmat=matrix(0,nrow=ntest,ncol=h)\nfor(i in 1:ntest){\n  x=window((ElimiTenddian_STL-dian_pred),end=time(ElimiTenddian_STL-dian_pred)[ntrain]+(i-1)/12)\n  #print(length(x))\n  refit=Arima(x,model=ARPURO_ref_prueba)\n  fcmat[i,]=as.numeric(forecast::forecast(refit,h=h)$mean)\n}\n\n# para volver a la escala original\nestacionalidad&lt;-as.vector(dian_pred)\ntendencia&lt;-as.vector(modelo_stl$trend)\nfchstepahe&lt;-(fcmat+estacionalidad[226:282])+tendencia[226:282] # primero sumamos la estacionalidad y luego la tendencia\n\nerrores_pred=exp(verval[,1]) -exp(fchstepahe) ##Observación: debo devolver los pronósticos y los verdaderos valores a la escala original si es necesario.\nECM=apply(errores_pred^2,MARGIN = 2,mean,na.rm=TRUE) ##Acá se computa la medida de precisión del pronóstico(en este caso ECM).\nRECM=sqrt(ECM) ##Se le saca raíz \nRECM # 1308387\n\n[1] 1308387\n\n\nCon el Rolling sobre el modelo AR(12) se obutov un valor ECM de 1.711875e+12 (Billones) y un valor RECM de 1308387 (Millones) ### Predicción usando el modelo AR(12)\n\nverval_ts&lt;-ts(exp(verval[,1]),start=time(ldian2)[ntrain]+1/12,frequency=12)\nfchstepahe_ts&lt;-ts(exp(fchstepahe),start=time(ldian2)[ntrain]+1/12,frequency=12)\n\nplot(verval_ts, col = \"blue\", ylab = \"Impuestos\", xlab = \"Tiempo\")\nlines(fchstepahe_ts, col = \"red\")\nlegend(\"topright\", legend = c(\"Reales\", \"Predicciones\"), col = c(\"blue\", \"red\"), lty = 1)"
  },
  {
    "objectID": "Descriptivo Dian (2).html#ajustando-el-modelo-arima",
    "href": "Descriptivo Dian (2).html#ajustando-el-modelo-arima",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "5.1 Ajustando el modelo ARIMA",
    "text": "5.1 Ajustando el modelo ARIMA\nEn primer lugar es necesario diviir los datos en conjunto de entrenamiento y de prueba.\n\n## Divison de los datos Utilizando los datos con varianza estable por boxcox.\n\ntrain_weight2 &lt;- 0.8\nsplit2 &lt;- as.integer(length(dldian2) * train_weight2)\n#window(ldian2,end=time(ldian2)[ntrain])\ndf_train3 &lt;- window(ldian2, end = time(ldian2)[split2])#80%\ndf_test3 &lt;- window(ldian2, start = time(ldian2)[split2] + 1/12)#20%\n\nPor lo observado en los graficos de acf, tiene snetido realizar un ajuste de un modelo ARIMA(12,1,0), puesto que el grafico del ACF baja lentamente y luego del rezago 12, la autocorrelación parcial se hace significativamente igual a 0 (observando el grafico PACF).\n\ndummy_ARIMA&lt;-forecast::seasonaldummy(df_train3)\n###Variables Dummy\nARIMA&lt;-Arima(df_train3,order=c(12,1,0),include.mean = TRUE,xreg = dummy_ARIMA)\ncoeftest(ARIMA)\n\n\nz test of coefficients:\n\n      Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1  -0.836525   0.066167 -12.6426 &lt; 2.2e-16 ***\nar2  -0.540680   0.085515  -6.3226 2.571e-10 ***\nar3  -0.425229   0.092021  -4.6210 3.819e-06 ***\nar4  -0.349554   0.095551  -3.6583 0.0002539 ***\nar5  -0.261111   0.098094  -2.6619 0.0077712 ** \nar6  -0.194296   0.099366  -1.9554 0.0505413 .  \nar7  -0.178789   0.099580  -1.7954 0.0725864 .  \nar8  -0.143400   0.098785  -1.4516 0.1466018    \nar9  -0.176200   0.096527  -1.8254 0.0679407 .  \nar10 -0.191734   0.093073  -2.0600 0.0393940 *  \nar11 -0.215715   0.086273  -2.5004 0.0124063 *  \nar12  0.151694   0.066951   2.2657 0.0234669 *  \nJan   0.782990   0.082581   9.4814 &lt; 2.2e-16 ***\nFeb   0.286359   0.060197   4.7570 1.965e-06 ***\nMar   0.443788   0.077788   5.7051 1.163e-08 ***\nApr   0.747220   0.066226  11.2829 &lt; 2.2e-16 ***\nMay   0.688016   0.073975   9.3006 &lt; 2.2e-16 ***\nJun   0.703536   0.067092  10.4861 &lt; 2.2e-16 ***\nJul   0.517541   0.073977   6.9960 2.634e-12 ***\nAug   0.283447   0.066218   4.2805 1.864e-05 ***\nSep   0.716101   0.078597   9.1111 &lt; 2.2e-16 ***\nOct   0.085955   0.060702   1.4160 0.1567695    \nNov   0.534292   0.083475   6.4006 1.548e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Refinamiento \nARIMA_ref&lt;-Arima(df_train3,order=c(12,1,0),include.mean = TRUE,xreg = dummy_ARIMA,fixed=c(NA,NA,NA,NA,0,0,0,0,0,0,0,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,0,NA))\ncoeftest(ARIMA_ref)\n\n\nz test of coefficients:\n\n      Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1  -0.763976   0.064788 -11.7919 &lt; 2.2e-16 ***\nar2  -0.435002   0.081169  -5.3592 8.359e-08 ***\nar3  -0.283567   0.081087  -3.4971 0.0004704 ***\nar4  -0.143521   0.062208  -2.3071 0.0210484 *  \nar12  0.282710   0.048061   5.8823 4.045e-09 ***\nJan   0.722194   0.065612  11.0071 &lt; 2.2e-16 ***\nFeb   0.255665   0.053402   4.7876 1.688e-06 ***\nMar   0.390215   0.058617   6.6570 2.795e-11 ***\nApr   0.716330   0.059918  11.9552 &lt; 2.2e-16 ***\nMay   0.644563   0.055073  11.7039 &lt; 2.2e-16 ***\nJun   0.653079   0.059922  10.8988 &lt; 2.2e-16 ***\nJul   0.478100   0.058291   8.2020 2.364e-16 ***\nAug   0.228513   0.053555   4.2669 1.983e-05 ***\nSep   0.685207   0.066132  10.3613 &lt; 2.2e-16 ***\nNov   0.489457   0.070777   6.9155 4.663e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAhora procedemos a observar si se cumplen los supuestos de este modelo. ## Verificación de los supuestos.\n\n## Validación de los supuestos.\n# Análisis de residuales\nresiduales &lt;-ARIMA_ref$residuals \n\nplot(residuales)\n\n\n\nacf(as.numeric(residuales))\n\n\n\nacf(as.numeric(residuales^2))\n\n\n\npacf(as.numeric(residuales))\n\n\n\n\n\n#Test de normalidad\ntseries::jarque.bera.test(residuales)\n\n\n    Jarque Bera Test\n\ndata:  residuales\nX-squared = 275.72, df = 2, p-value &lt; 2.2e-16\n\n#Test de autocorrelación\nlength(residuales)/4\n\n[1] 56\n\nsqrt(length(residuales))\n\n[1] 14.96663\n\nBox.test(residuales, lag =17 , type = \"Ljung-Box\", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!\n\n\n    Box-Ljung test\n\ndata:  residuales\nX-squared = 34.765, df = 15, p-value = 0.002655\n\n\nEn este caso se tiene que se rechaza el supuesto de normalidad ya que 2.2e-16 (p-value) es menor que un valor alpha de 0.05,en este caso el p-valor de la prueba Box-Ljung es 0.002655, que es menor que 0.05, por lo tanto hay suficiente evidencia para no rechazar que hay autocorrelación significativa en los residuos.\n\n###Estad?sticas CUSUM\nres=residuales\ncum=cumsum(res)/sd(res)\nN=length(res)\ncumq=cumsum(res^2)/sum(res^2)\nAf=0.948 ###Cuantil del 95% para la estad?stica cusum\nco=0.10997####Valor del cuantil aproximado para cusumsq para n/2\nLS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)\nLI=-LS\nLQS=co+(1:length(res))/N\nLQI=-co+(1:length(res))/N\nplot(cum,type=\"l\",ylim=c(min(LI),max(LS)),xlab=\"t\",ylab=\"\",main=\"CUSUM\")\nlines(LS,type=\"S\",col=\"red\")\nlines(LI,type=\"S\",col=\"red\")\n\n\n\n#CUSUMSQ\nplot(cumq,type=\"l\",xlab=\"t\",ylab=\"\",main=\"CUSUMSQ\")                      \nlines(LQS,type=\"S\",col=\"red\")                                                                           \nlines(LQI,type=\"S\",col=\"red\")"
  },
  {
    "objectID": "Descriptivo Dian (2).html#división-de-los-datos",
    "href": "Descriptivo Dian (2).html#división-de-los-datos",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "6.1 División de los datos",
    "text": "6.1 División de los datos\n\ntrain_weight2 &lt;- 0.8\nsplit2 &lt;- as.integer(length(ldian2) * train_weight2)\n#window(ldian2,end=time(ldian2)[ntrain])\ndf_train4 &lt;- window(ldian2, end = time(ldian2)[split2])#80%\ndf_test4 &lt;- window(ldian2, start = time(ldian2)[split2] + 1/12)#20%\n\nTeniendo en cuenta la prueba de raices unitarias realizada anteriormente se conoce que hay presencia de una raíz unitaria, por lo cual se deberia realizar una diferenciación ordinaria, por otro lado se sabe de la sección del análisis descriptivo que se tiene estacionalidad de periodo 12, por dicha razón procederemos a realizar diferenciacion estacional, posteriormente observaremos los posibles ordenes para el modelamiento SARIMA.\n\n##Diferencia estacional.\ndldian2&lt;-diff(df_train4)\nmonthplot(dldian2)\n\n\n\nacf(as.numeric(dldian2),lag.max = length(dian2)/4)\n\n\n\nspectrum(dldian2)\n\n\n\nnsdiffs(dldian2)\n\n[1] 1\n\nnsdiffs(ldian2)\n\n[1] 1\n\n\nLa función “nsdiffs”, no indica la cantidad de diferencias estacionales que se deben hacer, en este caso se nos indica que es 1, a su ves se conovia gracias a la sección descriptiva que se tenia presencia de una componente estacional de periodo \\(s=12\\).\n\n##Diferenciacion estacional\nDdldian2=diff(dldian2,lag=12)###lag=s\n\n\nplot(dldian2)\n\n\n\nplot(Ddldian2)\n\n\n\nmonthplot(Ddldian2)\n\n\n\nacf(as.numeric(Ddldian2),lag.max = length(dian2)/4)\n\n\n\nspectrum(Ddldian2)\n\n\n\nnsdiffs(Ddldian2)\n\n[1] 0\n\n\nNotese que ya es posible observar una serie estacionaria luego de realizar la diferenciación ordinaria y la diferenciación estacionaria, además no parece ser necesario realizar más diferenciaciones. Ahora si procedemos a encontrar los ordenes para los modelos de la familia SARIMA.\n\nacf(as.numeric(Ddldian2))\n\n\n\nacf(as.numeric(Ddldian2),lag.max = length(dian2)/4, ci.type='ma')# q=0,1,2 Q=0,1\n\n\n\npacf(as.numeric(Ddldian2),lag.max = length(dian2)/4)#p=0,1,2,...,5, P=0,1,2,3\n\n\n\n\nLuego de analizar los gráficos del ACF y PACF se procede a ajustar el modelo SARIMA. ## Ajuste del modelo\n\n##Ajuste del modelo SARIMA\nmodelo_SARIMA = Arima(df_train4, c(5, 1, 1),seasonal = list(order = c(3, 1, 2), period = 12),lambda = 0)\ncoeftest(modelo_SARIMA)\n\n\nz test of coefficients:\n\n      Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1  -0.068782   0.079788  -0.8621  0.388655    \nar2   0.215094   0.077028   2.7924  0.005232 ** \nar3   0.013880   0.074776   0.1856  0.852740    \nar4  -0.077461   0.074281  -1.0428  0.297038    \nar5   0.035963   0.073834   0.4871  0.626202    \nma1  -0.922809   0.037890 -24.3550 &lt; 2.2e-16 ***\nsar1 -0.816776   0.456869  -1.7878  0.073813 .  \nsar2 -0.460331   0.292090  -1.5760  0.115028    \nsar3 -0.249335   0.126857  -1.9655  0.049359 *  \nsma1  0.355469   0.458807   0.7748  0.438476    \nsma2 -0.099372   0.400739  -0.2480  0.804156    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n##Refinando el modelo SARIMA.\nmodelo_SARIMA_ref = Arima(df_train4, c(5, 1, 1),seasonal = list(order = c(3, 1, 2), period = 12),lambda = 0,fixed=c(0,NA,0,0,0,NA,NA,NA,NA,0,0))\ncoeftest(modelo_SARIMA_ref)\n\n\nz test of coefficients:\n\n      Estimate Std. Error  z value  Pr(&gt;|z|)    \nar2   0.211420   0.071878   2.9414  0.003267 ** \nma1  -0.935944   0.024303 -38.5119 &lt; 2.2e-16 ***\nsar1 -0.459411   0.070200  -6.5443 5.978e-11 ***\nsar2 -0.391257   0.069138  -5.6591 1.522e-08 ***\nsar3 -0.131092   0.068258  -1.9205  0.054791 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Descriptivo Dian (2).html#verificación-de-los-supusetos.",
    "href": "Descriptivo Dian (2).html#verificación-de-los-supusetos.",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "6.2 Verificación de los supusetos.",
    "text": "6.2 Verificación de los supusetos.\nAhora procederemos a revisar los supusetos de este modelo.\n\n# Análisis de residuales\nresiduales=modelo_SARIMA_ref$residuals\nplot(residuales)\n\n\n\nacf(as.numeric(residuales))\n\n\n\nacf(as.numeric(residuales^2))\n\n\n\npacf(as.numeric(residuales))\n\n\n\n\nNotese que los acf y pacf parecen indicar que no queda nada por ser explicado en los residuales del modelo, lo cual es un buen indicio.\n\n#Test de normalidad\ntseries::jarque.bera.test(residuales)\n\n\n    Jarque Bera Test\n\ndata:  residuales\nX-squared = 1057, df = 2, p-value &lt; 2.2e-16\n\n#Test de autocorrelación\nlength(residuales)/4\n\n[1] 56.25\n\nsqrt(length(residuales))\n\n[1] 15\n\nBox.test(residuales, lag =17 , type = \"Ljung-Box\", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!\n\n\n    Box-Ljung test\n\ndata:  residuales\nX-squared = 6.9188, df = 15, p-value = 0.9599\n\n\nEn este caso se tiene que se rechaza el supuesto de normalidad ya que 2.2e-16 (p-value) es menor que un valor alpha de 0.05,en este caso el p-valor de la prueba Box-Ljung es 0.9599, que es mayor que 0.05, por lo tanto hay suficiente evidencia para rechazar que hay autocorrelación significativa en los residuos.\n\n###Estad?sticas CUSUM\nres=residuales\ncum=cumsum(res)/sd(res)\nN=length(res)\ncumq=cumsum(res^2)/sum(res^2)\nAf=0.948 ###Cuantil del 95% para la estad?stica cusum\nco=0.10997####Valor del cuantil aproximado para cusumsq para n/2\nLS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)\nLI=-LS\nLQS=co+(1:length(res))/N\nLQI=-co+(1:length(res))/N\nplot(cum,type=\"l\",ylim=c(min(LI),max(LS)),xlab=\"t\",ylab=\"\",main=\"CUSUM\")\nlines(LS,type=\"S\",col=\"red\")\nlines(LI,type=\"S\",col=\"red\")\n\n\n\n#CUSUMSQ\nplot(cumq,type=\"l\",xlab=\"t\",ylab=\"\",main=\"CUSUMSQ\")                      \nlines(LQS,type=\"S\",col=\"red\")                                                                           \nlines(LQI,type=\"S\",col=\"red\")"
  },
  {
    "objectID": "Descriptivo Dian (2).html#rolling-modelo-sarima",
    "href": "Descriptivo Dian (2).html#rolling-modelo-sarima",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "6.3 Rolling modelo SARIMA",
    "text": "6.3 Rolling modelo SARIMA\nLuesto de observar los supuestos sobre los residuales del modelo SARIMA ajustado procedemos a observar la capacidad predictiva del modelo haciendo rolling sobre el conjunto de prueba y obteniendo su ECM.\n\nh=1\nlserie=length(df_train4)##datos de entrenamiento\nntrain=length(df_train4)\nntrain\n\n[1] 225\n\ntime(df_train4)\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2000 2000.000 2000.083 2000.167 2000.250 2000.333 2000.417 2000.500 2000.583\n2001 2001.000 2001.083 2001.167 2001.250 2001.333 2001.417 2001.500 2001.583\n2002 2002.000 2002.083 2002.167 2002.250 2002.333 2002.417 2002.500 2002.583\n2003 2003.000 2003.083 2003.167 2003.250 2003.333 2003.417 2003.500 2003.583\n2004 2004.000 2004.083 2004.167 2004.250 2004.333 2004.417 2004.500 2004.583\n2005 2005.000 2005.083 2005.167 2005.250 2005.333 2005.417 2005.500 2005.583\n2006 2006.000 2006.083 2006.167 2006.250 2006.333 2006.417 2006.500 2006.583\n2007 2007.000 2007.083 2007.167 2007.250 2007.333 2007.417 2007.500 2007.583\n2008 2008.000 2008.083 2008.167 2008.250 2008.333 2008.417 2008.500 2008.583\n2009 2009.000 2009.083 2009.167 2009.250 2009.333 2009.417 2009.500 2009.583\n2010 2010.000 2010.083 2010.167 2010.250 2010.333 2010.417 2010.500 2010.583\n2011 2011.000 2011.083 2011.167 2011.250 2011.333 2011.417 2011.500 2011.583\n2012 2012.000 2012.083 2012.167 2012.250 2012.333 2012.417 2012.500 2012.583\n2013 2013.000 2013.083 2013.167 2013.250 2013.333 2013.417 2013.500 2013.583\n2014 2014.000 2014.083 2014.167 2014.250 2014.333 2014.417 2014.500 2014.583\n2015 2015.000 2015.083 2015.167 2015.250 2015.333 2015.417 2015.500 2015.583\n2016 2016.000 2016.083 2016.167 2016.250 2016.333 2016.417 2016.500 2016.583\n2017 2017.000 2017.083 2017.167 2017.250 2017.333 2017.417 2017.500 2017.583\n2018 2018.000 2018.083 2018.167 2018.250 2018.333 2018.417 2018.500 2018.583\n          Sep      Oct      Nov      Dec\n2000 2000.667 2000.750 2000.833 2000.917\n2001 2001.667 2001.750 2001.833 2001.917\n2002 2002.667 2002.750 2002.833 2002.917\n2003 2003.667 2003.750 2003.833 2003.917\n2004 2004.667 2004.750 2004.833 2004.917\n2005 2005.667 2005.750 2005.833 2005.917\n2006 2006.667 2006.750 2006.833 2006.917\n2007 2007.667 2007.750 2007.833 2007.917\n2008 2008.667 2008.750 2008.833 2008.917\n2009 2009.667 2009.750 2009.833 2009.917\n2010 2010.667 2010.750 2010.833 2010.917\n2011 2011.667 2011.750 2011.833 2011.917\n2012 2012.667 2012.750 2012.833 2012.917\n2013 2013.667 2013.750 2013.833 2013.917\n2014 2014.667 2014.750 2014.833 2014.917\n2015 2015.667 2015.750 2015.833 2015.917\n2016 2016.667 2016.750 2016.833 2016.917\n2017 2017.667 2017.750 2017.833 2017.917\n2018 2018.667                           \n\ntime(df_train4)[ntrain]###Me entrega la ultima fecha de la posición ntrain\n\n[1] 2018.667\n\ntrain=window(ldian2,end=c(2018,8))\ntest=window(ldian2, start = time(df_train4)[split2] + 1/12)\n#length(train)\nntest=length(df_test4)\nntest\n\n[1] 57\n\nfcmat=matrix(0,nrow=ntest,ncol=h)\nfor(i in 1:ntest)\n{\n  x=window(ldian2,end=time(ldian2)[ntrain]+(i-1)/12)\n  #print(length(x))\n  refit=Arima(x, model=modelo_SARIMA_ref)\n  fcmat[i,]=as.numeric(forecast::forecast(refit,h=h)$mean)\n}\nfcmat_menos_reales&lt;-exp(verval[,1])-exp(fcmat)\nECM=apply(fcmat_menos_reales^2,MARGIN = 2,mean,na.rm=TRUE)\nRECM=sqrt(ECM) \nRECM # 4953741\n\n[1] 2197225\n\n\nSe obtuvo un valor de ECM de 4.827798e+12 (Billones) y un RECM de 2197225 (Millones). ## Predicción usando el modelo SARIMA.\n\nverval_ts&lt;-ts(exp(test),start=time(ldian2)[ntrain]+1/12,frequency=12)\nfchstepahe_ts&lt;-ts(exp(fcmat),start=time(ldian2)[ntrain]+1/12,frequency=12)\n\nplot(verval_ts, col = \"blue\", ylab = \"Impuestos\", xlab = \"Tiempo\")\nlines(fchstepahe_ts, col = \"red\")\nlegend(\"topright\", legend = c(\"Reales\", \"Predicciones\"), col = c(\"blue\", \"red\"), lty = 1)"
  },
  {
    "objectID": "Descriptivo Dian (2).html#verificación-de-los-supusetos.-1",
    "href": "Descriptivo Dian (2).html#verificación-de-los-supusetos.-1",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "7.1 Verificación de los supusetos.",
    "text": "7.1 Verificación de los supusetos.\nAhora procederemos a revisar los supusetos de este modelo.\n\n# Análisis de residuales\nresiduales=modelo_SARIMA_ref_outliers2$residuals\nplot(residuales)\n\n\n\nacf(as.numeric(residuales))\n\n\n\nacf(as.numeric(residuales^2))\n\n\n\npacf(as.numeric(residuales))\n\n\n\n\n\n#Test de normalidad\ntseries::jarque.bera.test(residuales)\n\n\n    Jarque Bera Test\n\ndata:  residuales\nX-squared = 233.76, df = 2, p-value &lt; 2.2e-16\n\n#Test de autocorrelación\nlength(residuales)/4\n\n[1] 56.25\n\nsqrt(length(residuales))\n\n[1] 15\n\nBox.test(residuales, lag =17 , type = \"Ljung-Box\", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!\n\n\n    Box-Ljung test\n\ndata:  residuales\nX-squared = 8.0528, df = 15, p-value = 0.9216\n\n\nPor el p value encontrado en el test de Jarque bera (1.059e-08&lt;0.05) se tiene suficiente evidencia estadistica para rechazar la hipotesis de normalidad, es decir que lso residuales no siguen una distribución normal, por otro lado se tiene un p valor de 0.9216 en la prueba de Box-Ljung, lo cual nos indica que hay suficiente evidencia estadistica para rechazar la hipótesis de Autocorrelación dentro de los residuales.\n\n### Estadisticas CUSUM\nres=residuales\ncum=cumsum(res)/sd(res)\nN=length(res)\ncumq=cumsum(res^2)/sum(res^2)\nAf=0.948 ###Cuantil del 95% para la estad?stica cusum\nco=0.10997####Valor del cuantil aproximado para cusumsq para n/2\nLS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)\nLI=-LS\nLQS=co+(1:length(res))/N\nLQI=-co+(1:length(res))/N\nplot(cum,type=\"l\",ylim=c(min(LI),max(LS)),xlab=\"t\",ylab=\"\",main=\"CUSUM\")\nlines(LS,type=\"S\",col=\"red\")\nlines(LI,type=\"S\",col=\"red\")\n\n\n\n#CUSUMSQ\nplot(cumq,type=\"l\",xlab=\"t\",ylab=\"\",main=\"CUSUMSQ\")                      \nlines(LQS,type=\"S\",col=\"red\")                                                                           \nlines(LQI,type=\"S\",col=\"red\")"
  },
  {
    "objectID": "Descriptivo Dian (2).html#rolling-sobre-el-modelo-sarima-con-outliers.",
    "href": "Descriptivo Dian (2).html#rolling-sobre-el-modelo-sarima-con-outliers.",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "7.2 Rolling sobre el modelo SARIMA con outliers.",
    "text": "7.2 Rolling sobre el modelo SARIMA con outliers.\n\nh=1\nnum_outliers=dim(total_outliers2)[2]\nregresoras_aditivos1=matrix(c(rep(0,h*(num_outliers-7))),h,num_outliers-7)\n#regresoras_LS=matrix(c(rep(1,h)),h,1)\n#regresoras_aditivos2=matrix(c(rep(0,h)),h,1)\nregresoras_TC=matrix(c(rep(0,h)),h,7)\nregresoras=cbind(regresoras_aditivos1,regresoras_TC)\ncolnames(regresoras)=colnames(total_outliers2)\n\n\nprediccSARIMAo &lt;- matrix(0, nrow=ntest, ncol=h) \nverval &lt;- cbind(test[1:ntest])\nfor(i in 1:(ntest)){\n  x&lt;-window(ldian2,end=time(ldian2)[ntrain]+(i-1)/12)\n  refit &lt;- Arima(x, model=modelo_SARIMA_ref_outliers2,xreg = total_outliers2)\n  prediccSARIMAo[i,] &lt;- forecast::forecast(refit, xreg=regresoras,h=h)$mean\n  total_outliers2&lt;-rbind(total_outliers2,regresoras)\n}\nerrores_predSARIMAo &lt;- exp(verval) -exp(prediccSARIMAo)\nECM_SARIMAo &lt;- mean(errores_predSARIMAo^2) # Medida de precisión del pronóstico (ECM).\nRECM_SARIMAo &lt;- sqrt(ECM_SARIMAo) # Se le saca raíz \nRECM_SARIMAo\n\n[1] 2129751\n\n\nSe obtuvo un valor ECM de 4.827798e+12 y un valor RECM de 2129751. ## Prediccion utilizando el modelo SARIMA con outliers.\n\nverval_ts&lt;-ts(exp(verval),start=time(ldian2)[ntrain]+1/12,frequency=12)\nfchstepahe_ts&lt;-ts(exp(prediccSARIMAo),start=time(ldian2)[ntrain]+1/12,frequency=12)\n\nplot(verval_ts, col = \"blue\", ylab = \"Impuestos\", xlab = \"Tiempo\")\nlines(fchstepahe_ts, col = \"red\")\nlegend(\"topright\", legend = c(\"Reales\", \"Predicciones\"), col = c(\"blue\", \"red\"), lty = 1)"
  },
  {
    "objectID": "Descriptivo Dian (2).html#verificación-de-los-supuestos-modelo-ar12-con-outliers.",
    "href": "Descriptivo Dian (2).html#verificación-de-los-supuestos-modelo-ar12-con-outliers.",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "8.1 Verificación de los supuestos modelo AR(12) con Outliers.",
    "text": "8.1 Verificación de los supuestos modelo AR(12) con Outliers.\n\n# Análisis de residuales\nresiduales= ARPURO_ref_prueba_outliers2$residuals\nplot(residuales)\n\n\n\nacf(as.numeric(residuales))\n\n\n\nacf(as.numeric(residuales^2))\n\n\n\npacf(as.numeric(residuales))\n\n\n\n\n\n#Test de normalidad\ntseries::jarque.bera.test(residuales)\n\n\n    Jarque Bera Test\n\ndata:  residuales\nX-squared = 44.24, df = 2, p-value = 2.474e-10\n\n#Test de autocorrelación\nlength(residuales)/4\n\n[1] 56.25\n\nsqrt(length(residuales))\n\n[1] 15\n\nBox.test(residuales, lag =17 , type = \"Ljung-Box\", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!\n\n\n    Box-Ljung test\n\ndata:  residuales\nX-squared = 11.715, df = 15, p-value = 0.7005\n\n\n\n###Estad?sticas CUSUM\nres=residuales\ncum=cumsum(res)/sd(res)\nN=length(res)\ncumq=cumsum(res^2)/sum(res^2)\nAf=0.948 ###Cuantil del 95% para la estad?stica cusum\nco=0.10997####Valor del cuantil aproximado para cusumsq para n/2\nLS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)\nLI=-LS\nLQS=co+(1:length(res))/N\nLQI=-co+(1:length(res))/N\nplot(cum,type=\"l\",ylim=c(min(LI),max(LS)),xlab=\"t\",ylab=\"\",main=\"CUSUM\")\nlines(LS,type=\"S\",col=\"red\")\nlines(LI,type=\"S\",col=\"red\")\n\n\n\n#CUSUMSQ\nplot(cumq,type=\"l\",xlab=\"t\",ylab=\"\",main=\"CUSUMSQ\")                      \nlines(LQS,type=\"S\",col=\"red\")                                                                           \nlines(LQI,type=\"S\",col=\"red\")"
  },
  {
    "objectID": "Descriptivo Dian (2).html#rolling-sobre-el-modelo-ar-con-outliers.",
    "href": "Descriptivo Dian (2).html#rolling-sobre-el-modelo-ar-con-outliers.",
    "title": "1  Análisis descriptivo del recaudo de impuestos internos por la DIAN",
    "section": "8.2 Rolling sobre el modelo AR con outliers.",
    "text": "8.2 Rolling sobre el modelo AR con outliers.\n\nh=1\nnum_outliers=dim(total_outliers)[2]\nregresoras_aditivos1=matrix(c(rep(0,h*(num_outliers-4))),h,num_outliers-4)\n#regresoras_LS=matrix(c(rep(1,h)),h,1)\nregresoras_aditivos2=matrix(c(rep(0,h)),h,1)\nregresoras_TC=matrix(c(rep(0,h)),h,3)\nregresoras=cbind(regresoras_aditivos1,regresoras_TC,regresoras_aditivos2)\ncolnames(regresoras)=colnames(total_outliers)\n\n\nprediccARo &lt;- matrix(0, nrow=ntest, ncol=h) \nverval &lt;- cbind(test[1:ntest])\nfor(i in 1:(ntest)){\n  x &lt;- window((ElimiTenddian_STL-dian_pred), end = time(ElimiTenddian_STL-dian_pred)[ntrain]+(i-1)/12)\n  refit &lt;- Arima(x, model=ARPURO_ref_prueba_outliers2,xreg = total_outliers)\n  prediccARo[i,] &lt;- forecast::forecast(refit, xreg=regresoras,h=h)$mean\n  total_outliers&lt;-rbind(total_outliers,regresoras)\n}\nprediccARo1&lt;-(prediccARo+estacionalidad[226:282])+tendencia[226:282]\nerrores_predARo &lt;- exp(verval) -exp(prediccARo1)\nECM_ARo &lt;- mean(errores_predARo^2) # Medida de precisión del pronóstico (ECM).\nRECM_ARo &lt;- sqrt(ECM_ARo) # Se le saca raíz \nRECM_ARo\n\n[1] 1302753\n\n\nNotese que se obtuvo un valor ECM de 1.697165e+12 y un valor de RECM de 1302753. ## Prediccion utilizando el modelo AR con outliers\n\nverval_ts&lt;-ts(exp(verval[,1]),start=time(ldian2)[ntrain]+1/12,frequency=12)\nfchstepahe_ts&lt;-ts(exp(prediccARo1),start=time(ldian2)[ntrain]+1/12,frequency=12)\n\nplot(verval_ts, col = \"blue\", ylab = \"Impuestos\", xlab = \"Tiempo\")\nlines(fchstepahe_ts, col = \"red\")\nlegend(\"topright\", legend = c(\"Reales\", \"Predicciones\"), col = c(\"blue\", \"red\"), lty = 1)"
  }
]